{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yannik/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from TestHelper import TestHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  Review\n",
      "The file does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yannik/ba/TestHelper.py:64: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  input_data = self.decide_which_input(input_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509411\n",
      "Tensor(\"lstm_net/dense/BiasAdd:0\", shape=(?, 3), dtype=float32)\n",
      "Tensor(\"lstm_net_1/dense/BiasAdd:0\", shape=(?, 3), dtype=float32)\n",
      "---init ready   LSTM ---\n",
      "\n",
      "Saving...\n",
      "saved to models/LSTM/pretrained_model.ckpt-0\n",
      "0 : loss:  2.3879201 \t acc:  0.41\n",
      "40 : loss:  1.1112999 \t acc:  0.38\n",
      "80 : loss:  1.067989 \t acc:  0.44\n",
      "120 : loss:  0.98896223 \t acc:  0.57\n",
      "160 : loss:  1.0579009 \t acc:  0.43\n",
      "200 : loss:  1.0366187 \t acc:  0.46\n",
      "240 : loss:  0.97282 \t acc:  0.55\n",
      "280 : loss:  0.88116586 \t acc:  0.57\n",
      "320 : loss:  1.0063351 \t acc:  0.54\n",
      "360 : loss:  1.0470605 \t acc:  0.45\n",
      "400 : loss:  0.92028254 \t acc:  0.54\n",
      "440 : loss:  1.0193422 \t acc:  0.44\n",
      "480 : loss:  0.9291122 \t acc:  0.58\n",
      "520 : loss:  1.0686318 \t acc:  0.47\n",
      "560 : loss:  1.0775262 \t acc:  0.49\n",
      "600 : loss:  0.9634448 \t acc:  0.53\n",
      "640 : loss:  0.9683103 \t acc:  0.52\n",
      "680 : loss:  0.8198275 \t acc:  0.63\n",
      "720 : loss:  0.83760023 \t acc:  0.65\n",
      "760 : loss:  0.8762352 \t acc:  0.61\n",
      "800 : loss:  0.9557656 \t acc:  0.54\n",
      "840 : loss:  0.92637 \t acc:  0.53\n",
      "880 : loss:  1.0241374 \t acc:  0.45\n",
      "920 : loss:  0.9651421 \t acc:  0.53\n",
      "960 : loss:  0.9028717 \t acc:  0.6\n",
      "1000 : loss:  0.95158494 \t acc:  0.54\n",
      "1040 : loss:  0.9890363 \t acc:  0.51\n",
      "1080 : loss:  0.91042167 \t acc:  0.58\n",
      "1120 : loss:  0.9983445 \t acc:  0.55\n",
      "1160 : loss:  0.8930449 \t acc:  0.57\n",
      "1200 : loss:  0.8328342 \t acc:  0.63\n",
      "1240 : loss:  0.9892657 \t acc:  0.51\n",
      "1280 : loss:  0.8515049 \t acc:  0.61\n",
      "1320 : loss:  0.91728854 \t acc:  0.61\n",
      "1360 : loss:  0.8330854 \t acc:  0.64\n",
      "1400 : loss:  0.9410131 \t acc:  0.58\n",
      "1440 : loss:  0.9540713 \t acc:  0.61\n",
      "1480 : loss:  0.8831671 \t acc:  0.58\n",
      "1520 : loss:  0.8406075 \t acc:  0.62\n",
      "1560 : loss:  0.85499537 \t acc:  0.59\n",
      "1600 : loss:  0.9953637 \t acc:  0.47\n",
      "1640 : loss:  0.87077194 \t acc:  0.6\n",
      "1680 : loss:  0.85170317 \t acc:  0.65\n",
      "1720 : loss:  0.8154461 \t acc:  0.67\n",
      "1760 : loss:  0.83092964 \t acc:  0.63\n",
      "1800 : loss:  0.87913823 \t acc:  0.58\n",
      "1840 : loss:  0.86339587 \t acc:  0.56\n",
      "1880 : loss:  0.839605 \t acc:  0.58\n",
      "1920 : loss:  0.80256677 \t acc:  0.64\n",
      "1960 : loss:  0.9083865 \t acc:  0.6\n",
      "2000 : loss:  0.8771624 \t acc:  0.58\n",
      "2040 : loss:  0.8765202 \t acc:  0.56\n",
      "2080 : loss:  0.83340764 \t acc:  0.63\n",
      "2120 : loss:  0.8415373 \t acc:  0.61\n",
      "2160 : loss:  0.81848174 \t acc:  0.6\n",
      "2200 : loss:  0.9030244 \t acc:  0.61\n",
      "2240 : loss:  0.8753635 \t acc:  0.6\n",
      "2280 : loss:  0.8203685 \t acc:  0.63\n",
      "2320 : loss:  0.7582885 \t acc:  0.63\n",
      "2360 : loss:  0.9079935 \t acc:  0.57\n",
      "2400 : loss:  0.7851954 \t acc:  0.66\n",
      "2440 : loss:  0.7797704 \t acc:  0.67\n",
      "2480 : loss:  0.91038346 \t acc:  0.57\n",
      "2520 : loss:  1.0152452 \t acc:  0.56\n",
      "2560 : loss:  0.77641386 \t acc:  0.69\n",
      "2600 : loss:  0.71706223 \t acc:  0.64\n",
      "2640 : loss:  0.85853124 \t acc:  0.61\n",
      "2680 : loss:  0.8815944 \t acc:  0.57\n",
      "2720 : loss:  0.7947242 \t acc:  0.69\n",
      "2760 : loss:  0.8217969 \t acc:  0.58\n",
      "2800 : loss:  0.8036159 \t acc:  0.68\n",
      "2840 : loss:  0.7881075 \t acc:  0.67\n",
      "2880 : loss:  0.91195834 \t acc:  0.57\n",
      "2920 : loss:  0.7282884 \t acc:  0.69\n",
      "2960 : loss:  0.8589873 \t acc:  0.59\n",
      "3000 : loss:  0.68459994 \t acc:  0.69\n",
      "3040 : loss:  0.77510226 \t acc:  0.59\n",
      "3080 : loss:  0.6768673 \t acc:  0.69\n",
      "3120 : loss:  0.89574134 \t acc:  0.56\n",
      "3160 : loss:  0.74436975 \t acc:  0.66\n",
      "3200 : loss:  0.80221295 \t acc:  0.65\n",
      "3240 : loss:  0.7865269 \t acc:  0.65\n",
      "3280 : loss:  0.9817593 \t acc:  0.54\n",
      "3320 : loss:  0.8481923 \t acc:  0.62\n",
      "3360 : loss:  0.7922937 \t acc:  0.61\n",
      "3400 : loss:  0.79646546 \t acc:  0.57\n",
      "3440 : loss:  0.76503634 \t acc:  0.69\n",
      "3480 : loss:  0.72577393 \t acc:  0.67\n",
      "3520 : loss:  0.81462026 \t acc:  0.64\n",
      "3560 : loss:  0.81479454 \t acc:  0.62\n",
      "3600 : loss:  0.7650337 \t acc:  0.63\n",
      "3640 : loss:  0.8785484 \t acc:  0.6\n",
      "3680 : loss:  0.8479377 \t acc:  0.64\n",
      "3720 : loss:  0.8335343 \t acc:  0.56\n",
      "3760 : loss:  0.8288817 \t acc:  0.59\n",
      "3800 : loss:  0.9127271 \t acc:  0.6\n",
      "3840 : loss:  0.7641436 \t acc:  0.66\n",
      "3880 : loss:  0.71958 \t acc:  0.68\n",
      "3920 : loss:  0.70564044 \t acc:  0.7\n",
      "3960 : loss:  0.7994115 \t acc:  0.65\n",
      "4000 : loss:  0.84768575 \t acc:  0.59\n",
      "4040 : loss:  0.88393193 \t acc:  0.58\n",
      "4080 : loss:  0.7881992 \t acc:  0.69\n",
      "4120 : loss:  0.8003613 \t acc:  0.64\n",
      "4160 : loss:  0.8127011 \t acc:  0.64\n",
      "4200 : loss:  0.8211998 \t acc:  0.63\n",
      "4240 : loss:  0.7878353 \t acc:  0.61\n",
      "4280 : loss:  0.8045204 \t acc:  0.64\n",
      "4320 : loss:  0.7315065 \t acc:  0.73\n",
      "4360 : loss:  0.7801863 \t acc:  0.63\n",
      "4400 : loss:  0.6724183 \t acc:  0.66\n",
      "4440 : loss:  0.75652754 \t acc:  0.64\n",
      "4480 : loss:  0.88018113 \t acc:  0.58\n",
      "\n",
      "Saving...\n",
      "saved to models/LSTM/pretrained_model.ckpt-4482\n",
      "\n",
      "500 \t [348.84323815 218.91088334 357.94932868]\n",
      "0 \tval accuracy:  0.64312 \t f_! score:  [0.69768648 0.43782177 0.71589866]\n",
      "\n",
      "4520 : loss:  0.7936763 \t acc:  0.67\n",
      "4560 : loss:  0.7951338 \t acc:  0.62\n",
      "4600 : loss:  0.87481695 \t acc:  0.61\n",
      "4640 : loss:  0.714759 \t acc:  0.66\n",
      "4680 : loss:  0.8342462 \t acc:  0.63\n",
      "4720 : loss:  0.8023948 \t acc:  0.61\n",
      "4760 : loss:  0.8154515 \t acc:  0.59\n",
      "4800 : loss:  0.7410777 \t acc:  0.68\n",
      "4840 : loss:  0.6839482 \t acc:  0.67\n",
      "4880 : loss:  0.75481296 \t acc:  0.64\n",
      "4920 : loss:  0.84657425 \t acc:  0.53\n",
      "4960 : loss:  0.74390775 \t acc:  0.66\n",
      "5000 : loss:  0.74597335 \t acc:  0.69\n",
      "5040 : loss:  0.78723836 \t acc:  0.64\n",
      "5080 : loss:  0.82214546 \t acc:  0.59\n",
      "5120 : loss:  0.76103354 \t acc:  0.67\n",
      "5160 : loss:  0.72247976 \t acc:  0.67\n",
      "5200 : loss:  0.71315295 \t acc:  0.72\n",
      "5240 : loss:  0.79872394 \t acc:  0.59\n",
      "5280 : loss:  0.8040572 \t acc:  0.65\n",
      "5320 : loss:  0.7260481 \t acc:  0.67\n",
      "5360 : loss:  0.7742034 \t acc:  0.64\n",
      "5400 : loss:  0.84236205 \t acc:  0.64\n",
      "5440 : loss:  0.76593053 \t acc:  0.66\n",
      "5480 : loss:  0.9994624 \t acc:  0.62\n",
      "5520 : loss:  0.7060163 \t acc:  0.68\n",
      "5560 : loss:  0.87948936 \t acc:  0.6\n",
      "5600 : loss:  0.7692816 \t acc:  0.61\n",
      "5640 : loss:  0.80052733 \t acc:  0.57\n",
      "5680 : loss:  0.71172976 \t acc:  0.7\n",
      "5720 : loss:  0.752403 \t acc:  0.69\n",
      "5760 : loss:  0.7250073 \t acc:  0.69\n",
      "5800 : loss:  0.75412714 \t acc:  0.69\n",
      "5840 : loss:  0.8318012 \t acc:  0.6\n",
      "5880 : loss:  0.76420856 \t acc:  0.63\n",
      "5920 : loss:  0.8583117 \t acc:  0.6\n",
      "5960 : loss:  0.73629487 \t acc:  0.71\n",
      "6000 : loss:  0.88795316 \t acc:  0.54\n",
      "6040 : loss:  0.7567334 \t acc:  0.69\n",
      "6080 : loss:  0.7079445 \t acc:  0.72\n",
      "6120 : loss:  0.73582673 \t acc:  0.74\n",
      "6160 : loss:  0.9867202 \t acc:  0.61\n",
      "6200 : loss:  0.71304786 \t acc:  0.66\n",
      "6240 : loss:  0.860826 \t acc:  0.6\n",
      "6280 : loss:  0.6323516 \t acc:  0.74\n",
      "6320 : loss:  0.66807884 \t acc:  0.71\n",
      "6360 : loss:  0.8030342 \t acc:  0.56\n",
      "6400 : loss:  0.7594465 \t acc:  0.67\n",
      "6440 : loss:  0.7756133 \t acc:  0.64\n",
      "6480 : loss:  0.72782874 \t acc:  0.64\n",
      "6520 : loss:  0.7737073 \t acc:  0.64\n",
      "6560 : loss:  0.8189235 \t acc:  0.71\n",
      "6600 : loss:  0.72399974 \t acc:  0.66\n",
      "6640 : loss:  0.83097684 \t acc:  0.67\n",
      "6680 : loss:  0.88694733 \t acc:  0.59\n",
      "6720 : loss:  0.8066339 \t acc:  0.61\n",
      "6760 : loss:  0.80190897 \t acc:  0.68\n",
      "6800 : loss:  0.7417805 \t acc:  0.64\n",
      "6840 : loss:  0.7418006 \t acc:  0.63\n",
      "6880 : loss:  0.75722414 \t acc:  0.65\n",
      "6920 : loss:  0.7102652 \t acc:  0.68\n",
      "6960 : loss:  0.7593398 \t acc:  0.65\n",
      "7000 : loss:  0.81920016 \t acc:  0.62\n",
      "7040 : loss:  0.71206605 \t acc:  0.71\n",
      "7080 : loss:  0.7650275 \t acc:  0.66\n",
      "7120 : loss:  0.8250233 \t acc:  0.64\n",
      "7160 : loss:  0.6742502 \t acc:  0.69\n",
      "7200 : loss:  0.6160969 \t acc:  0.68\n",
      "7240 : loss:  0.7115364 \t acc:  0.66\n",
      "7280 : loss:  0.7360099 \t acc:  0.7\n",
      "7320 : loss:  0.6830066 \t acc:  0.69\n",
      "7360 : loss:  0.74731743 \t acc:  0.63\n",
      "7400 : loss:  0.7760274 \t acc:  0.66\n",
      "7440 : loss:  0.7116189 \t acc:  0.66\n",
      "7480 : loss:  0.74171656 \t acc:  0.69\n",
      "7520 : loss:  0.8251973 \t acc:  0.67\n",
      "7560 : loss:  0.8422396 \t acc:  0.61\n",
      "7600 : loss:  0.7207219 \t acc:  0.66\n",
      "7640 : loss:  0.8120818 \t acc:  0.68\n",
      "7680 : loss:  0.7726487 \t acc:  0.66\n",
      "7720 : loss:  0.80596864 \t acc:  0.61\n",
      "7760 : loss:  0.76198983 \t acc:  0.7\n",
      "7800 : loss:  0.8051654 \t acc:  0.6\n",
      "7840 : loss:  0.7525025 \t acc:  0.63\n",
      "7880 : loss:  0.6909471 \t acc:  0.68\n",
      "7920 : loss:  0.76876205 \t acc:  0.65\n",
      "7960 : loss:  0.7851156 \t acc:  0.64\n",
      "8000 : loss:  0.7651395 \t acc:  0.69\n",
      "8040 : loss:  0.7811096 \t acc:  0.63\n",
      "8080 : loss:  0.8725881 \t acc:  0.59\n",
      "8120 : loss:  0.6647477 \t acc:  0.68\n",
      "8160 : loss:  0.82632565 \t acc:  0.56\n",
      "8200 : loss:  0.8380342 \t acc:  0.64\n",
      "8240 : loss:  0.7647465 \t acc:  0.57\n",
      "8280 : loss:  0.8873111 \t acc:  0.6\n",
      "8320 : loss:  0.8373986 \t acc:  0.59\n",
      "8360 : loss:  0.65694547 \t acc:  0.7\n",
      "8400 : loss:  0.94447225 \t acc:  0.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8440 : loss:  0.9176275 \t acc:  0.58\n",
      "8480 : loss:  0.61608607 \t acc:  0.72\n",
      "8520 : loss:  0.80876696 \t acc:  0.61\n",
      "8560 : loss:  0.7677362 \t acc:  0.66\n",
      "8600 : loss:  0.8072121 \t acc:  0.59\n",
      "8640 : loss:  0.7631189 \t acc:  0.64\n",
      "8680 : loss:  0.8398084 \t acc:  0.59\n",
      "8720 : loss:  0.81023353 \t acc:  0.64\n",
      "8760 : loss:  0.70961905 \t acc:  0.7\n",
      "8800 : loss:  0.70940435 \t acc:  0.69\n",
      "8840 : loss:  0.76329607 \t acc:  0.63\n",
      "8880 : loss:  0.74762726 \t acc:  0.71\n",
      "8920 : loss:  0.77655685 \t acc:  0.63\n",
      "8960 : loss:  0.7017312 \t acc:  0.69\n",
      "\n",
      "Saving...\n",
      "saved to models/LSTM/pretrained_model.ckpt-8965\n",
      "\n",
      "500 \t [354.97072784 240.87100583 371.45546525]\n",
      "1 \tval accuracy:  0.66440004 \t f_! score:  [0.70994146 0.48174201 0.74291093]\n",
      "\n",
      "9000 : loss:  0.66578925 \t acc:  0.74\n",
      "9040 : loss:  0.68626875 \t acc:  0.65\n",
      "9080 : loss:  0.8344801 \t acc:  0.68\n",
      "9120 : loss:  0.78615564 \t acc:  0.61\n",
      "9160 : loss:  0.6226625 \t acc:  0.75\n",
      "9200 : loss:  0.68581307 \t acc:  0.67\n",
      "9240 : loss:  0.77502495 \t acc:  0.65\n",
      "9280 : loss:  0.7241143 \t acc:  0.69\n",
      "9320 : loss:  0.7081259 \t acc:  0.61\n",
      "9360 : loss:  0.6867578 \t acc:  0.72\n",
      "9400 : loss:  0.6790764 \t acc:  0.76\n",
      "9440 : loss:  0.77716535 \t acc:  0.63\n",
      "9480 : loss:  0.6630027 \t acc:  0.73\n",
      "9520 : loss:  0.64964414 \t acc:  0.7\n",
      "9560 : loss:  0.65793073 \t acc:  0.72\n",
      "9600 : loss:  0.83661485 \t acc:  0.62\n",
      "9640 : loss:  0.78080976 \t acc:  0.63\n",
      "9680 : loss:  0.7020046 \t acc:  0.68\n",
      "9720 : loss:  0.75026125 \t acc:  0.67\n",
      "9760 : loss:  0.842219 \t acc:  0.59\n",
      "9800 : loss:  0.72528243 \t acc:  0.65\n",
      "9840 : loss:  0.85772127 \t acc:  0.63\n",
      "9880 : loss:  0.5765735 \t acc:  0.76\n",
      "9920 : loss:  0.67418015 \t acc:  0.7\n",
      "9960 : loss:  0.6586323 \t acc:  0.74\n",
      "10000 : loss:  0.6947717 \t acc:  0.67\n",
      "10040 : loss:  0.82838875 \t acc:  0.65\n",
      "10080 : loss:  0.813318 \t acc:  0.65\n",
      "10120 : loss:  0.86610335 \t acc:  0.62\n",
      "10160 : loss:  0.713696 \t acc:  0.67\n",
      "10200 : loss:  0.7602959 \t acc:  0.64\n",
      "10240 : loss:  0.7139825 \t acc:  0.68\n",
      "10280 : loss:  0.63499147 \t acc:  0.75\n",
      "10320 : loss:  0.71703017 \t acc:  0.69\n",
      "10360 : loss:  0.6829262 \t acc:  0.71\n",
      "10400 : loss:  0.809182 \t acc:  0.66\n",
      "10440 : loss:  0.7483836 \t acc:  0.67\n",
      "10480 : loss:  0.86508423 \t acc:  0.63\n",
      "10520 : loss:  0.7617022 \t acc:  0.65\n",
      "10560 : loss:  0.7766297 \t acc:  0.62\n",
      "10600 : loss:  0.6533054 \t acc:  0.69\n",
      "10640 : loss:  0.78158575 \t acc:  0.61\n",
      "10680 : loss:  0.7315804 \t acc:  0.71\n",
      "10720 : loss:  0.69613403 \t acc:  0.69\n",
      "10760 : loss:  0.6366152 \t acc:  0.77\n",
      "10800 : loss:  0.704577 \t acc:  0.67\n",
      "10840 : loss:  0.841546 \t acc:  0.67\n",
      "10880 : loss:  0.670448 \t acc:  0.71\n",
      "10920 : loss:  0.78096205 \t acc:  0.65\n",
      "10960 : loss:  0.7919393 \t acc:  0.66\n",
      "11000 : loss:  0.68439984 \t acc:  0.72\n",
      "11040 : loss:  0.71953464 \t acc:  0.68\n",
      "11080 : loss:  0.7980699 \t acc:  0.67\n",
      "11120 : loss:  0.79380816 \t acc:  0.65\n",
      "11160 : loss:  0.8832521 \t acc:  0.61\n",
      "11200 : loss:  0.76070714 \t acc:  0.66\n",
      "11240 : loss:  0.7292699 \t acc:  0.63\n",
      "11280 : loss:  0.7805991 \t acc:  0.63\n",
      "11320 : loss:  0.7599133 \t acc:  0.68\n",
      "11360 : loss:  0.80408007 \t acc:  0.62\n",
      "11400 : loss:  0.6213104 \t acc:  0.74\n",
      "11440 : loss:  0.81645554 \t acc:  0.68\n",
      "11480 : loss:  0.8177537 \t acc:  0.6\n",
      "11520 : loss:  1.0339975 \t acc:  0.5\n",
      "11560 : loss:  0.76335835 \t acc:  0.67\n",
      "11600 : loss:  0.6470324 \t acc:  0.78\n",
      "11640 : loss:  0.75927335 \t acc:  0.64\n",
      "11680 : loss:  0.7283652 \t acc:  0.67\n",
      "11720 : loss:  0.83567315 \t acc:  0.61\n",
      "11760 : loss:  0.8005177 \t acc:  0.59\n",
      "11800 : loss:  0.7018857 \t acc:  0.66\n",
      "11840 : loss:  0.8229435 \t acc:  0.6\n",
      "11880 : loss:  0.7385368 \t acc:  0.7\n",
      "11920 : loss:  0.7232579 \t acc:  0.64\n",
      "11960 : loss:  0.7330545 \t acc:  0.66\n",
      "12000 : loss:  0.59723336 \t acc:  0.77\n",
      "12040 : loss:  0.72337306 \t acc:  0.67\n",
      "12080 : loss:  0.791723 \t acc:  0.67\n",
      "12120 : loss:  0.76764596 \t acc:  0.69\n",
      "12160 : loss:  0.714153 \t acc:  0.65\n",
      "12200 : loss:  0.7916503 \t acc:  0.63\n",
      "12240 : loss:  0.68057305 \t acc:  0.7\n",
      "12280 : loss:  0.67440915 \t acc:  0.69\n",
      "12320 : loss:  0.63392836 \t acc:  0.72\n",
      "12360 : loss:  0.7635518 \t acc:  0.64\n",
      "12400 : loss:  0.6823381 \t acc:  0.72\n",
      "12440 : loss:  0.8093771 \t acc:  0.62\n",
      "12480 : loss:  0.79951215 \t acc:  0.58\n",
      "12520 : loss:  0.8078292 \t acc:  0.62\n",
      "12560 : loss:  0.7358572 \t acc:  0.66\n",
      "12600 : loss:  0.814388 \t acc:  0.64\n",
      "12640 : loss:  0.54150724 \t acc:  0.79\n",
      "12680 : loss:  0.75647944 \t acc:  0.61\n",
      "12720 : loss:  0.61947274 \t acc:  0.73\n",
      "12760 : loss:  0.8018693 \t acc:  0.62\n",
      "12800 : loss:  0.7243416 \t acc:  0.63\n",
      "12840 : loss:  0.80465543 \t acc:  0.6\n",
      "12880 : loss:  0.7042828 \t acc:  0.7\n",
      "12920 : loss:  0.773618 \t acc:  0.66\n",
      "12960 : loss:  0.6750798 \t acc:  0.72\n",
      "13000 : loss:  0.8032583 \t acc:  0.62\n",
      "13040 : loss:  0.68347555 \t acc:  0.71\n",
      "13080 : loss:  0.7179205 \t acc:  0.67\n",
      "13120 : loss:  0.89712805 \t acc:  0.67\n",
      "13160 : loss:  0.63912493 \t acc:  0.74\n",
      "13200 : loss:  0.6295723 \t acc:  0.66\n",
      "13240 : loss:  0.8898681 \t acc:  0.62\n",
      "13280 : loss:  0.6328724 \t acc:  0.7\n",
      "13320 : loss:  0.677199 \t acc:  0.73\n",
      "13360 : loss:  0.79913545 \t acc:  0.65\n",
      "13400 : loss:  0.62279207 \t acc:  0.75\n",
      "13440 : loss:  0.7427123 \t acc:  0.66\n",
      "\n",
      "Saving...\n",
      "saved to models/LSTM/pretrained_model.ckpt-13448\n",
      "\n",
      "500 \t [363.99764219 239.99179797 380.01295281]\n",
      "2 \tval accuracy:  0.67738 \t f_! score:  [0.72799528 0.4799836  0.76002591]\n",
      "\n",
      "13480 : loss:  0.5915907 \t acc:  0.73\n",
      "13520 : loss:  0.77749735 \t acc:  0.65\n",
      "13560 : loss:  0.6782726 \t acc:  0.75\n",
      "13600 : loss:  0.7071599 \t acc:  0.68\n",
      "13640 : loss:  0.624903 \t acc:  0.75\n",
      "13680 : loss:  0.59927917 \t acc:  0.75\n",
      "13720 : loss:  0.60118943 \t acc:  0.67\n",
      "13760 : loss:  0.62195665 \t acc:  0.76\n",
      "13800 : loss:  0.75990784 \t acc:  0.66\n",
      "13840 : loss:  0.79904664 \t acc:  0.63\n",
      "13880 : loss:  0.5558524 \t acc:  0.75\n",
      "13920 : loss:  0.7413531 \t acc:  0.69\n",
      "13960 : loss:  0.83262753 \t acc:  0.68\n",
      "14000 : loss:  0.77278566 \t acc:  0.63\n",
      "14040 : loss:  0.69544935 \t acc:  0.68\n",
      "14080 : loss:  0.68381643 \t acc:  0.69\n",
      "14120 : loss:  0.7506206 \t acc:  0.71\n",
      "14160 : loss:  0.7346132 \t acc:  0.64\n",
      "14200 : loss:  0.7639193 \t acc:  0.67\n",
      "14240 : loss:  0.6229423 \t acc:  0.73\n",
      "14280 : loss:  0.6274669 \t acc:  0.71\n",
      "14320 : loss:  0.8489562 \t acc:  0.58\n",
      "14360 : loss:  0.6937417 \t acc:  0.69\n",
      "14400 : loss:  0.6517748 \t acc:  0.77\n",
      "14440 : loss:  0.7529671 \t acc:  0.63\n",
      "14480 : loss:  0.7593985 \t acc:  0.65\n",
      "14520 : loss:  0.74003863 \t acc:  0.72\n",
      "14560 : loss:  0.56410426 \t acc:  0.77\n",
      "14600 : loss:  0.7056544 \t acc:  0.71\n",
      "14640 : loss:  0.6385349 \t acc:  0.69\n",
      "14680 : loss:  0.71613175 \t acc:  0.68\n",
      "14720 : loss:  0.67064065 \t acc:  0.75\n",
      "14760 : loss:  0.732324 \t acc:  0.69\n",
      "14800 : loss:  0.69569415 \t acc:  0.74\n",
      "14840 : loss:  0.7286507 \t acc:  0.63\n",
      "14880 : loss:  0.744121 \t acc:  0.67\n",
      "14920 : loss:  0.6789401 \t acc:  0.71\n",
      "14960 : loss:  0.7087906 \t acc:  0.67\n",
      "15000 : loss:  0.7666266 \t acc:  0.69\n",
      "15040 : loss:  0.75648993 \t acc:  0.73\n",
      "15080 : loss:  0.6882553 \t acc:  0.69\n",
      "15120 : loss:  0.91227865 \t acc:  0.54\n",
      "15160 : loss:  0.70277494 \t acc:  0.66\n",
      "15200 : loss:  0.7885622 \t acc:  0.68\n",
      "15240 : loss:  0.68546945 \t acc:  0.7\n",
      "15280 : loss:  0.6658337 \t acc:  0.71\n",
      "15320 : loss:  0.57945645 \t acc:  0.76\n",
      "15360 : loss:  0.74935836 \t acc:  0.64\n",
      "15400 : loss:  0.8076945 \t acc:  0.68\n",
      "15440 : loss:  0.6968243 \t acc:  0.64\n",
      "15480 : loss:  0.5901354 \t acc:  0.72\n",
      "15520 : loss:  0.7171629 \t acc:  0.66\n",
      "15560 : loss:  0.63697577 \t acc:  0.69\n",
      "15600 : loss:  0.77772284 \t acc:  0.64\n",
      "15640 : loss:  0.809338 \t acc:  0.61\n",
      "15680 : loss:  0.82134145 \t acc:  0.63\n",
      "15720 : loss:  0.6424536 \t acc:  0.67\n",
      "15760 : loss:  0.7151963 \t acc:  0.69\n",
      "15800 : loss:  0.62865424 \t acc:  0.74\n",
      "15840 : loss:  0.65250283 \t acc:  0.66\n",
      "15880 : loss:  0.6590688 \t acc:  0.69\n",
      "15920 : loss:  0.6138193 \t acc:  0.71\n",
      "15960 : loss:  0.8866865 \t acc:  0.6\n",
      "16000 : loss:  0.84146035 \t acc:  0.59\n",
      "16040 : loss:  0.7377714 \t acc:  0.63\n",
      "16080 : loss:  0.701644 \t acc:  0.67\n",
      "16120 : loss:  0.9286367 \t acc:  0.59\n",
      "16160 : loss:  0.755974 \t acc:  0.66\n",
      "16200 : loss:  0.60232294 \t acc:  0.73\n",
      "16240 : loss:  0.69876456 \t acc:  0.66\n",
      "16280 : loss:  0.59808314 \t acc:  0.78\n",
      "16320 : loss:  0.62020564 \t acc:  0.73\n",
      "16360 : loss:  0.6479563 \t acc:  0.69\n",
      "16400 : loss:  0.69074875 \t acc:  0.72\n",
      "16440 : loss:  0.59914774 \t acc:  0.76\n",
      "16480 : loss:  0.7436932 \t acc:  0.62\n",
      "16520 : loss:  0.6512737 \t acc:  0.71\n",
      "16560 : loss:  0.6065669 \t acc:  0.69\n",
      "16600 : loss:  0.7050942 \t acc:  0.71\n",
      "16640 : loss:  0.7362584 \t acc:  0.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16680 : loss:  0.6984044 \t acc:  0.66\n",
      "16720 : loss:  0.7478543 \t acc:  0.66\n",
      "16760 : loss:  0.57930547 \t acc:  0.74\n",
      "16800 : loss:  0.67661494 \t acc:  0.67\n",
      "16840 : loss:  0.7720737 \t acc:  0.62\n",
      "16880 : loss:  0.7711816 \t acc:  0.65\n",
      "16920 : loss:  0.737326 \t acc:  0.7\n",
      "16960 : loss:  0.6592871 \t acc:  0.71\n",
      "17000 : loss:  0.7671675 \t acc:  0.65\n",
      "17040 : loss:  0.6398089 \t acc:  0.7\n",
      "17080 : loss:  0.63823664 \t acc:  0.68\n",
      "17120 : loss:  0.6085842 \t acc:  0.77\n",
      "17160 : loss:  0.7487683 \t acc:  0.69\n",
      "17200 : loss:  0.8294992 \t acc:  0.65\n",
      "17240 : loss:  0.66622466 \t acc:  0.68\n",
      "17280 : loss:  0.61708355 \t acc:  0.69\n",
      "17320 : loss:  0.69027907 \t acc:  0.72\n",
      "17360 : loss:  0.779277 \t acc:  0.63\n",
      "17400 : loss:  0.7785798 \t acc:  0.67\n",
      "17440 : loss:  0.5665935 \t acc:  0.74\n",
      "17480 : loss:  0.71074677 \t acc:  0.59\n",
      "17520 : loss:  0.73306066 \t acc:  0.68\n",
      "17560 : loss:  0.688298 \t acc:  0.66\n",
      "17600 : loss:  0.7517902 \t acc:  0.67\n",
      "17640 : loss:  0.64465094 \t acc:  0.76\n",
      "17680 : loss:  0.7156392 \t acc:  0.63\n",
      "17720 : loss:  0.63004035 \t acc:  0.71\n",
      "17760 : loss:  0.63404727 \t acc:  0.72\n",
      "17800 : loss:  0.5736805 \t acc:  0.78\n",
      "17840 : loss:  0.7079596 \t acc:  0.67\n",
      "17880 : loss:  0.6879919 \t acc:  0.64\n",
      "17920 : loss:  0.84009445 \t acc:  0.61\n",
      "\n",
      "Saving...\n",
      "saved to models/LSTM/pretrained_model.ckpt-17931\n",
      "\n",
      "500 \t [365.49844288 238.95598148 383.7185376 ]\n",
      "3 \tval accuracy:  0.68338 \t f_! score:  [0.73099689 0.47791196 0.76743708]\n",
      "\n",
      "17960 : loss:  0.7611412 \t acc:  0.64\n",
      "18000 : loss:  0.58311546 \t acc:  0.77\n",
      "18040 : loss:  0.54364234 \t acc:  0.78\n",
      "18080 : loss:  0.7626535 \t acc:  0.65\n",
      "18120 : loss:  0.7147515 \t acc:  0.68\n",
      "18160 : loss:  0.6264973 \t acc:  0.71\n",
      "18200 : loss:  0.68387955 \t acc:  0.71\n",
      "18240 : loss:  0.6155293 \t acc:  0.73\n",
      "18280 : loss:  0.6367329 \t acc:  0.76\n",
      "18320 : loss:  0.6739542 \t acc:  0.7\n",
      "18360 : loss:  0.8320907 \t acc:  0.58\n",
      "18400 : loss:  0.7061697 \t acc:  0.64\n",
      "18440 : loss:  0.6044752 \t acc:  0.68\n",
      "18480 : loss:  0.7384979 \t acc:  0.63\n",
      "18520 : loss:  0.7356534 \t acc:  0.65\n",
      "18560 : loss:  0.60775805 \t acc:  0.73\n",
      "18600 : loss:  0.65199035 \t acc:  0.71\n",
      "18640 : loss:  0.59919584 \t acc:  0.77\n",
      "18680 : loss:  0.70468384 \t acc:  0.67\n",
      "18720 : loss:  0.6281443 \t acc:  0.75\n",
      "18760 : loss:  0.6943647 \t acc:  0.69\n",
      "18800 : loss:  0.5779023 \t acc:  0.72\n",
      "18840 : loss:  0.64017224 \t acc:  0.76\n",
      "18880 : loss:  0.77377313 \t acc:  0.65\n",
      "18920 : loss:  0.6660733 \t acc:  0.73\n",
      "18960 : loss:  0.66379446 \t acc:  0.7\n",
      "19000 : loss:  0.6431225 \t acc:  0.69\n",
      "19040 : loss:  0.76881707 \t acc:  0.61\n",
      "19080 : loss:  0.80393255 \t acc:  0.62\n",
      "19120 : loss:  0.7472322 \t acc:  0.63\n",
      "19160 : loss:  0.6571144 \t acc:  0.76\n",
      "19200 : loss:  0.69231665 \t acc:  0.73\n",
      "19240 : loss:  0.6377466 \t acc:  0.71\n",
      "19280 : loss:  0.7037905 \t acc:  0.66\n",
      "19320 : loss:  0.64205694 \t acc:  0.78\n",
      "19360 : loss:  0.62653756 \t acc:  0.74\n",
      "19400 : loss:  0.72494733 \t acc:  0.69\n",
      "19440 : loss:  0.53599894 \t acc:  0.78\n",
      "19480 : loss:  0.63757783 \t acc:  0.72\n",
      "19520 : loss:  0.63592154 \t acc:  0.7\n",
      "19560 : loss:  0.6622789 \t acc:  0.67\n",
      "19600 : loss:  0.5376246 \t acc:  0.73\n",
      "19640 : loss:  0.5705102 \t acc:  0.74\n",
      "19680 : loss:  0.6690047 \t acc:  0.74\n",
      "19720 : loss:  0.7514498 \t acc:  0.65\n",
      "19760 : loss:  0.66179883 \t acc:  0.68\n",
      "19800 : loss:  0.5908311 \t acc:  0.75\n",
      "19840 : loss:  0.64840645 \t acc:  0.73\n",
      "19880 : loss:  0.6702129 \t acc:  0.69\n",
      "19920 : loss:  0.5895018 \t acc:  0.78\n",
      "19960 : loss:  0.6128226 \t acc:  0.69\n",
      "20000 : loss:  0.7643358 \t acc:  0.69\n",
      "20040 : loss:  0.7425847 \t acc:  0.66\n",
      "20080 : loss:  0.712305 \t acc:  0.7\n",
      "20120 : loss:  0.5858639 \t acc:  0.75\n",
      "20160 : loss:  0.6953086 \t acc:  0.73\n",
      "20200 : loss:  0.7399768 \t acc:  0.72\n",
      "20240 : loss:  0.76652646 \t acc:  0.68\n",
      "20280 : loss:  0.6771139 \t acc:  0.66\n",
      "20320 : loss:  0.7240445 \t acc:  0.67\n",
      "20360 : loss:  0.79884374 \t acc:  0.63\n",
      "20400 : loss:  0.81288666 \t acc:  0.6\n",
      "20440 : loss:  0.638198 \t acc:  0.72\n",
      "20480 : loss:  0.6468176 \t acc:  0.67\n",
      "20520 : loss:  0.6789082 \t acc:  0.67\n",
      "20560 : loss:  0.66990644 \t acc:  0.72\n",
      "20600 : loss:  0.56194687 \t acc:  0.7\n",
      "20640 : loss:  0.70824134 \t acc:  0.65\n",
      "20680 : loss:  0.7518498 \t acc:  0.63\n",
      "20720 : loss:  0.6620773 \t acc:  0.69\n",
      "20760 : loss:  0.72153074 \t acc:  0.67\n",
      "20800 : loss:  0.54002684 \t acc:  0.78\n",
      "20840 : loss:  0.7625517 \t acc:  0.62\n",
      "20880 : loss:  0.59090215 \t acc:  0.73\n",
      "20920 : loss:  0.57575154 \t acc:  0.74\n",
      "20960 : loss:  0.6868116 \t acc:  0.69\n",
      "21000 : loss:  0.657735 \t acc:  0.68\n",
      "21040 : loss:  0.7141216 \t acc:  0.71\n",
      "21080 : loss:  0.7567134 \t acc:  0.61\n",
      "21120 : loss:  0.6154031 \t acc:  0.73\n",
      "21160 : loss:  0.6808695 \t acc:  0.67\n",
      "21200 : loss:  0.5599002 \t acc:  0.7\n",
      "21240 : loss:  0.72538984 \t acc:  0.67\n",
      "21280 : loss:  0.58710843 \t acc:  0.73\n",
      "21320 : loss:  0.6444277 \t acc:  0.73\n",
      "21360 : loss:  0.8689339 \t acc:  0.61\n",
      "21400 : loss:  0.567339 \t acc:  0.79\n",
      "21440 : loss:  0.79054433 \t acc:  0.68\n",
      "21480 : loss:  0.61214477 \t acc:  0.78\n",
      "21520 : loss:  0.623558 \t acc:  0.76\n",
      "21560 : loss:  0.6341891 \t acc:  0.74\n",
      "21600 : loss:  0.6696993 \t acc:  0.73\n",
      "21640 : loss:  0.79009575 \t acc:  0.68\n",
      "21680 : loss:  0.6955978 \t acc:  0.7\n",
      "21720 : loss:  0.6347654 \t acc:  0.73\n",
      "21760 : loss:  0.77794844 \t acc:  0.66\n",
      "21800 : loss:  0.6564338 \t acc:  0.7\n",
      "21840 : loss:  0.69158775 \t acc:  0.7\n",
      "21880 : loss:  0.6413575 \t acc:  0.72\n",
      "21920 : loss:  0.6220637 \t acc:  0.69\n",
      "21960 : loss:  0.60036105 \t acc:  0.72\n",
      "22000 : loss:  0.61929244 \t acc:  0.69\n",
      "22040 : loss:  0.7494731 \t acc:  0.68\n",
      "22080 : loss:  0.72414917 \t acc:  0.68\n",
      "22120 : loss:  0.6358502 \t acc:  0.75\n",
      "22160 : loss:  0.6967489 \t acc:  0.65\n",
      "22200 : loss:  0.8425217 \t acc:  0.66\n",
      "22240 : loss:  0.6534598 \t acc:  0.73\n",
      "22280 : loss:  0.7695209 \t acc:  0.71\n",
      "22320 : loss:  0.63835365 \t acc:  0.74\n",
      "22360 : loss:  0.6969659 \t acc:  0.64\n",
      "22400 : loss:  0.7678405 \t acc:  0.67\n",
      "\n",
      "Saving...\n",
      "saved to models/LSTM/pretrained_model.ckpt-22414\n",
      "\n",
      "500 \t [353.9820463  252.25308636 383.323206  ]\n",
      "4 \tval accuracy:  0.67595994 \t f_! score:  [0.70796409 0.50450617 0.76664641]\n",
      "\n",
      "22440 : loss:  0.72189796 \t acc:  0.69\n",
      "22480 : loss:  0.72406924 \t acc:  0.68\n",
      "22520 : loss:  0.63665706 \t acc:  0.74\n",
      "22560 : loss:  0.72634727 \t acc:  0.67\n",
      "22600 : loss:  0.75925475 \t acc:  0.69\n",
      "22640 : loss:  0.6367266 \t acc:  0.71\n",
      "22680 : loss:  0.7773117 \t acc:  0.61\n",
      "22720 : loss:  0.68715173 \t acc:  0.68\n",
      "22760 : loss:  0.6822676 \t acc:  0.71\n",
      "22800 : loss:  0.58919215 \t acc:  0.79\n",
      "22840 : loss:  0.5954084 \t acc:  0.76\n",
      "22880 : loss:  0.64263445 \t acc:  0.74\n",
      "22920 : loss:  0.6917443 \t acc:  0.67\n",
      "22960 : loss:  0.699956 \t acc:  0.65\n",
      "23000 : loss:  0.6472396 \t acc:  0.74\n",
      "23040 : loss:  0.5707702 \t acc:  0.69\n",
      "23080 : loss:  0.6578182 \t acc:  0.71\n",
      "23120 : loss:  0.67425424 \t acc:  0.73\n",
      "23160 : loss:  0.7014158 \t acc:  0.72\n",
      "23200 : loss:  0.6693748 \t acc:  0.69\n",
      "23240 : loss:  0.59712255 \t acc:  0.77\n",
      "23280 : loss:  0.85174155 \t acc:  0.64\n",
      "23320 : loss:  0.73793674 \t acc:  0.64\n",
      "23360 : loss:  0.69610894 \t acc:  0.69\n",
      "23400 : loss:  0.674948 \t acc:  0.71\n",
      "23440 : loss:  0.5476944 \t acc:  0.75\n",
      "23480 : loss:  0.90695685 \t acc:  0.68\n",
      "23520 : loss:  0.6122058 \t acc:  0.74\n",
      "23560 : loss:  0.7794549 \t acc:  0.64\n",
      "23600 : loss:  0.7441047 \t acc:  0.7\n",
      "23640 : loss:  0.6769156 \t acc:  0.71\n",
      "23680 : loss:  0.6415261 \t acc:  0.71\n",
      "23720 : loss:  0.69074243 \t acc:  0.67\n",
      "23760 : loss:  0.567481 \t acc:  0.78\n",
      "23800 : loss:  0.4928626 \t acc:  0.8\n",
      "23840 : loss:  0.64510787 \t acc:  0.69\n",
      "23880 : loss:  0.7348319 \t acc:  0.65\n",
      "23920 : loss:  0.6830311 \t acc:  0.73\n",
      "23960 : loss:  0.7200833 \t acc:  0.71\n",
      "24000 : loss:  0.5998451 \t acc:  0.77\n",
      "24040 : loss:  0.7448613 \t acc:  0.65\n",
      "24080 : loss:  0.7288428 \t acc:  0.67\n",
      "24120 : loss:  0.6741345 \t acc:  0.71\n",
      "24160 : loss:  0.66978675 \t acc:  0.77\n",
      "24200 : loss:  0.5588272 \t acc:  0.79\n",
      "24240 : loss:  0.6693688 \t acc:  0.73\n",
      "24280 : loss:  0.6729227 \t acc:  0.71\n",
      "24320 : loss:  0.72698724 \t acc:  0.69\n",
      "24360 : loss:  0.5879393 \t acc:  0.7\n",
      "24400 : loss:  0.70464057 \t acc:  0.69\n",
      "24440 : loss:  0.7168001 \t acc:  0.69\n",
      "24480 : loss:  0.741182 \t acc:  0.69\n",
      "24520 : loss:  0.5884234 \t acc:  0.75\n",
      "24560 : loss:  0.6375643 \t acc:  0.76\n",
      "24600 : loss:  0.55727047 \t acc:  0.76\n",
      "24640 : loss:  0.6394927 \t acc:  0.72\n",
      "24680 : loss:  0.64241636 \t acc:  0.69\n",
      "24720 : loss:  0.74810296 \t acc:  0.66\n",
      "24760 : loss:  0.6136906 \t acc:  0.76\n",
      "24800 : loss:  0.7092473 \t acc:  0.7\n",
      "24840 : loss:  0.6511764 \t acc:  0.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24880 : loss:  0.5833708 \t acc:  0.75\n",
      "24920 : loss:  0.55481833 \t acc:  0.79\n",
      "24960 : loss:  0.7002739 \t acc:  0.69\n",
      "25000 : loss:  0.68557435 \t acc:  0.68\n",
      "25040 : loss:  0.63331485 \t acc:  0.74\n",
      "25080 : loss:  0.8085101 \t acc:  0.64\n",
      "25120 : loss:  0.66985404 \t acc:  0.74\n",
      "25160 : loss:  0.8246105 \t acc:  0.62\n",
      "25200 : loss:  0.74307275 \t acc:  0.69\n",
      "25240 : loss:  0.73168916 \t acc:  0.7\n",
      "25280 : loss:  0.66317177 \t acc:  0.7\n",
      "25320 : loss:  0.53515536 \t acc:  0.79\n",
      "25360 : loss:  0.5965144 \t acc:  0.78\n",
      "25400 : loss:  0.5093135 \t acc:  0.79\n",
      "25440 : loss:  0.60184497 \t acc:  0.74\n",
      "25480 : loss:  0.55488944 \t acc:  0.82\n",
      "25520 : loss:  0.691371 \t acc:  0.75\n",
      "25560 : loss:  0.68249583 \t acc:  0.71\n",
      "25600 : loss:  0.71184266 \t acc:  0.71\n",
      "25640 : loss:  0.7126464 \t acc:  0.7\n",
      "25680 : loss:  0.56934327 \t acc:  0.71\n",
      "25720 : loss:  0.72563285 \t acc:  0.64\n",
      "25760 : loss:  0.6188799 \t acc:  0.73\n",
      "25800 : loss:  0.64059824 \t acc:  0.72\n",
      "25840 : loss:  0.65165526 \t acc:  0.72\n",
      "25880 : loss:  0.5264546 \t acc:  0.78\n",
      "25920 : loss:  0.67959565 \t acc:  0.64\n",
      "25960 : loss:  0.7250807 \t acc:  0.66\n",
      "26000 : loss:  0.8308513 \t acc:  0.55\n",
      "26040 : loss:  0.79564065 \t acc:  0.56\n",
      "26080 : loss:  0.7403155 \t acc:  0.63\n",
      "26120 : loss:  0.60736954 \t acc:  0.72\n",
      "26160 : loss:  0.6365428 \t acc:  0.77\n",
      "26200 : loss:  0.7731866 \t acc:  0.7\n",
      "26240 : loss:  0.93575346 \t acc:  0.62\n",
      "26280 : loss:  0.56685984 \t acc:  0.77\n",
      "26320 : loss:  0.74705523 \t acc:  0.66\n",
      "26360 : loss:  0.58033967 \t acc:  0.74\n",
      "26400 : loss:  0.65687925 \t acc:  0.7\n",
      "26440 : loss:  0.70412695 \t acc:  0.65\n",
      "26480 : loss:  0.6343175 \t acc:  0.7\n",
      "26520 : loss:  0.5665427 \t acc:  0.77\n",
      "26560 : loss:  0.7884129 \t acc:  0.56\n",
      "26600 : loss:  0.6046112 \t acc:  0.69\n",
      "26640 : loss:  0.7508046 \t acc:  0.7\n",
      "26680 : loss:  0.540933 \t acc:  0.78\n",
      "26720 : loss:  0.6245998 \t acc:  0.72\n",
      "26760 : loss:  0.6976646 \t acc:  0.63\n",
      "26800 : loss:  0.6345739 \t acc:  0.75\n",
      "26840 : loss:  0.64295685 \t acc:  0.71\n",
      "26880 : loss:  0.6760399 \t acc:  0.69\n",
      "\n",
      "Saving...\n",
      "saved to models/LSTM/pretrained_model.ckpt-26897\n",
      "\n",
      "500 \t [365.69032342 252.65446281 387.04123624]\n",
      "5 \tval accuracy:  0.68955994 \t f_! score:  [0.73138065 0.50530893 0.77408247]\n",
      "\n",
      "26920 : loss:  0.6826106 \t acc:  0.7\n",
      "26960 : loss:  0.60637605 \t acc:  0.72\n",
      "27000 : loss:  0.5677445 \t acc:  0.73\n",
      "27040 : loss:  0.57367015 \t acc:  0.77\n",
      "27080 : loss:  0.6543896 \t acc:  0.69\n",
      "27120 : loss:  0.6876306 \t acc:  0.74\n",
      "27160 : loss:  0.6723969 \t acc:  0.66\n",
      "27200 : loss:  0.612332 \t acc:  0.69\n",
      "27240 : loss:  0.5927874 \t acc:  0.72\n",
      "27280 : loss:  0.6137184 \t acc:  0.77\n",
      "27320 : loss:  0.61223197 \t acc:  0.7\n",
      "27360 : loss:  0.7126938 \t acc:  0.68\n",
      "27400 : loss:  0.72286505 \t acc:  0.71\n",
      "27440 : loss:  0.65319705 \t acc:  0.7\n",
      "27480 : loss:  0.60192144 \t acc:  0.72\n",
      "27520 : loss:  0.62405336 \t acc:  0.73\n",
      "27560 : loss:  0.70637625 \t acc:  0.67\n",
      "27600 : loss:  0.68286616 \t acc:  0.68\n",
      "27640 : loss:  0.5860839 \t acc:  0.8\n",
      "27680 : loss:  0.708838 \t acc:  0.68\n",
      "27720 : loss:  0.5987301 \t acc:  0.73\n",
      "27760 : loss:  0.72979146 \t acc:  0.67\n",
      "27800 : loss:  0.6048971 \t acc:  0.72\n",
      "27840 : loss:  0.6245924 \t acc:  0.76\n",
      "27880 : loss:  0.66005826 \t acc:  0.71\n",
      "27920 : loss:  0.75866205 \t acc:  0.64\n",
      "27960 : loss:  0.5845076 \t acc:  0.75\n",
      "28000 : loss:  0.70154786 \t acc:  0.67\n",
      "28040 : loss:  0.661889 \t acc:  0.72\n",
      "28080 : loss:  0.61895376 \t acc:  0.74\n",
      "28120 : loss:  0.597766 \t acc:  0.73\n",
      "28160 : loss:  0.5916216 \t acc:  0.73\n",
      "28200 : loss:  0.5931848 \t acc:  0.73\n",
      "28240 : loss:  0.6195647 \t acc:  0.74\n",
      "28280 : loss:  0.79578644 \t acc:  0.67\n",
      "28320 : loss:  0.7422579 \t acc:  0.63\n",
      "28360 : loss:  0.70270073 \t acc:  0.72\n",
      "28400 : loss:  0.7587406 \t acc:  0.73\n",
      "28440 : loss:  0.6589873 \t acc:  0.68\n",
      "28480 : loss:  0.6493728 \t acc:  0.68\n",
      "28520 : loss:  0.6188094 \t acc:  0.71\n",
      "28560 : loss:  0.57453614 \t acc:  0.74\n",
      "28600 : loss:  0.69146883 \t acc:  0.7\n",
      "28640 : loss:  0.75778955 \t acc:  0.69\n",
      "28680 : loss:  0.6879412 \t acc:  0.69\n",
      "28720 : loss:  0.7681984 \t acc:  0.73\n",
      "28760 : loss:  0.61667544 \t acc:  0.71\n",
      "28800 : loss:  0.79309195 \t acc:  0.61\n",
      "28840 : loss:  0.74497795 \t acc:  0.69\n",
      "28880 : loss:  0.6830142 \t acc:  0.69\n",
      "28920 : loss:  0.6931717 \t acc:  0.68\n",
      "28960 : loss:  0.65280896 \t acc:  0.68\n",
      "29000 : loss:  0.656801 \t acc:  0.68\n",
      "29040 : loss:  0.5729294 \t acc:  0.79\n",
      "29080 : loss:  0.6053747 \t acc:  0.69\n",
      "29120 : loss:  0.6980192 \t acc:  0.69\n",
      "29160 : loss:  0.75634825 \t acc:  0.65\n",
      "29200 : loss:  0.7056441 \t acc:  0.69\n",
      "29240 : loss:  0.70843506 \t acc:  0.64\n",
      "29280 : loss:  0.610725 \t acc:  0.75\n",
      "29320 : loss:  0.7050291 \t acc:  0.7\n",
      "29360 : loss:  0.7130892 \t acc:  0.67\n",
      "29400 : loss:  0.6191997 \t acc:  0.67\n",
      "29440 : loss:  0.70878464 \t acc:  0.66\n",
      "29480 : loss:  0.63162714 \t acc:  0.71\n",
      "29520 : loss:  0.71599597 \t acc:  0.67\n",
      "29560 : loss:  0.6024743 \t acc:  0.77\n",
      "29600 : loss:  0.7019322 \t acc:  0.7\n",
      "29640 : loss:  0.67234033 \t acc:  0.75\n",
      "29680 : loss:  0.6924702 \t acc:  0.72\n",
      "29720 : loss:  0.7053715 \t acc:  0.72\n",
      "29760 : loss:  0.5999379 \t acc:  0.74\n",
      "29800 : loss:  0.5682651 \t acc:  0.73\n",
      "29840 : loss:  0.6461102 \t acc:  0.68\n",
      "29880 : loss:  0.74414474 \t acc:  0.69\n",
      "29920 : loss:  0.5221342 \t acc:  0.81\n",
      "29960 : loss:  0.6836488 \t acc:  0.65\n",
      "30000 : loss:  0.6595288 \t acc:  0.69\n",
      "30040 : loss:  0.69142884 \t acc:  0.68\n",
      "30080 : loss:  0.6679489 \t acc:  0.73\n",
      "30120 : loss:  0.5314321 \t acc:  0.77\n",
      "30160 : loss:  0.6396091 \t acc:  0.72\n",
      "30200 : loss:  0.6347199 \t acc:  0.67\n",
      "30240 : loss:  0.7117865 \t acc:  0.68\n",
      "30280 : loss:  0.57966924 \t acc:  0.73\n",
      "30320 : loss:  0.67004377 \t acc:  0.68\n",
      "30360 : loss:  0.66929597 \t acc:  0.65\n",
      "30400 : loss:  0.7301012 \t acc:  0.72\n",
      "30440 : loss:  0.7341807 \t acc:  0.66\n",
      "30480 : loss:  0.7026299 \t acc:  0.66\n",
      "30520 : loss:  0.627301 \t acc:  0.77\n",
      "30560 : loss:  0.6749037 \t acc:  0.7\n",
      "30600 : loss:  0.74160737 \t acc:  0.68\n",
      "30640 : loss:  0.6618113 \t acc:  0.72\n",
      "30680 : loss:  0.7220325 \t acc:  0.68\n",
      "30720 : loss:  0.6315407 \t acc:  0.71\n",
      "30760 : loss:  0.62825316 \t acc:  0.77\n",
      "30800 : loss:  0.7327335 \t acc:  0.71\n",
      "30840 : loss:  0.690429 \t acc:  0.65\n",
      "30880 : loss:  0.3681762 \t acc:  1.0\n",
      "30920 : loss:  0.576003 \t acc:  0.78\n",
      "30960 : loss:  0.54506695 \t acc:  0.78\n",
      "31000 : loss:  0.721207 \t acc:  0.72\n",
      "31040 : loss:  0.66691136 \t acc:  0.68\n",
      "31080 : loss:  0.8061837 \t acc:  0.59\n",
      "31120 : loss:  0.73655856 \t acc:  0.67\n",
      "31160 : loss:  0.66397995 \t acc:  0.72\n",
      "31200 : loss:  0.6630456 \t acc:  0.7\n",
      "31240 : loss:  0.52636695 \t acc:  0.77\n",
      "31280 : loss:  0.59095335 \t acc:  0.72\n",
      "31320 : loss:  0.57920784 \t acc:  0.78\n",
      "31360 : loss:  0.63132167 \t acc:  0.72\n",
      "\n",
      "Saving...\n",
      "saved to models/LSTM/pretrained_model.ckpt-31380\n",
      "\n",
      "500 \t [356.99704215 265.80930843 388.28077532]\n",
      "6 \tval accuracy:  0.68513995 \t f_! score:  [0.71399408 0.53161862 0.77656155]\n",
      "\n",
      "500 \t [356.99704215 265.80930843 388.28077532]\n",
      "500 \t [382.89657505 252.50247545 384.82695282]\n",
      "500 \t [337.08046757 285.6745346  394.45884759]\n",
      "500 \t [18794. 13823. 17383.]\n",
      "--- Test   Review ---\n",
      "0.68513995\n",
      "f1:  [0.71399408 0.53161862 0.77656155]\n",
      "65730\n",
      "Vorsicht! es muss vorher ein  LSTM  Netz abgespeichert worden sein\n",
      "\n",
      "Restoring...\n",
      "models/LSTM/pretrained_model.ckpt-31380\n",
      "INFO:tensorflow:Restoring parameters from models/LSTM/pretrained_model.ckpt-31380\n",
      "acc:  0.3\n",
      "acc:  0.32\n",
      "acc:  0.33\n",
      "acc:  0.33\n",
      "acc:  0.36\n",
      "acc:  0.31\n",
      "acc:  0.31\n",
      "acc:  0.3\n",
      "acc:  0.23\n",
      "acc:  0.34\n",
      "acc:  0.31\n",
      "acc:  0.32\n",
      "acc:  0.34\n",
      "acc:  0.4\n",
      "acc:  0.33\n",
      "acc:  0.4\n",
      "acc:  0.39\n",
      "acc:  0.38\n",
      "acc:  0.31\n",
      "acc:  0.26\n",
      "acc:  0.34\n",
      "acc:  0.35\n",
      "acc:  0.35\n",
      "acc:  0.31\n",
      "acc:  0.39\n",
      "acc:  0.35\n",
      "acc:  0.31\n",
      "acc:  0.36\n",
      "acc:  0.26\n",
      "acc:  0.37\n",
      "acc:  0.3\n",
      "acc:  0.26\n",
      "acc:  0.38\n",
      "acc:  0.35\n",
      "acc:  0.33\n",
      "acc:  0.36\n",
      "acc:  0.26\n",
      "acc:  0.37\n",
      "acc:  0.25\n",
      "acc:  0.27\n",
      "acc:  0.26\n",
      "acc:  0.37\n",
      "acc:  0.3\n",
      "acc:  0.34\n",
      "acc:  0.33\n",
      "acc:  0.3\n",
      "acc:  0.33\n",
      "acc:  0.31\n",
      "acc:  0.33\n",
      "acc:  0.39\n",
      "acc:  0.41\n",
      "acc:  0.35\n",
      "acc:  0.23\n",
      "acc:  0.38\n",
      "acc:  0.26\n",
      "acc:  0.36\n",
      "acc:  0.32\n",
      "acc:  0.4\n",
      "acc:  0.3\n",
      "acc:  0.25\n",
      "acc:  0.29\n",
      "acc:  0.36\n",
      "acc:  0.3\n",
      "acc:  0.31\n",
      "acc:  0.28\n",
      "acc:  0.33\n",
      "acc:  0.32\n",
      "acc:  0.36\n",
      "acc:  0.32\n",
      "acc:  0.34\n",
      "acc:  0.26\n",
      "acc:  0.29\n",
      "acc:  0.33\n",
      "acc:  0.28\n",
      "acc:  0.4\n",
      "acc:  0.27\n",
      "acc:  0.32\n",
      "acc:  0.35\n",
      "acc:  0.4\n",
      "acc:  0.25\n",
      "acc:  0.31\n",
      "acc:  0.38\n",
      "acc:  0.34\n",
      "acc:  0.38\n",
      "acc:  0.32\n",
      "acc:  0.29\n",
      "acc:  0.38\n",
      "acc:  0.27\n",
      "acc:  0.36\n",
      "acc:  0.34\n",
      "acc:  0.29\n",
      "acc:  0.26\n",
      "acc:  0.33\n",
      "acc:  0.34\n",
      "acc:  0.26\n",
      "acc:  0.3\n",
      "acc:  0.33\n",
      "acc:  0.37\n",
      "acc:  0.3\n",
      "acc:  0.35\n",
      "\n",
      "100 \t [44.256325   26.90926411  7.61092705]\n",
      "val accuracy:  0.32469997 \t f_! score:  [0.44256325 0.26909264 0.07610927]\n",
      "\n",
      "100 \t [44.256325   26.90926411  7.61092705]\n",
      "100 \t [32.89804727 31.1828099  35.90555556]\n",
      "100 \t [68.66303105 24.30918207  4.37590071]\n",
      "100 \t [3339. 3358. 3303.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---just Test  Twitter ---\n",
      "0.32469997\n",
      "f1:  [0.44256325 0.26909264 0.07610927]\n",
      "(7733, 3)\n",
      "7733\n",
      "Vorsicht! es muss vorher ein  LSTM  Netz abgespeichert worden sein\n",
      "\n",
      "Restoring...\n",
      "models/LSTM/pretrained_model.ckpt-31380\n",
      "INFO:tensorflow:Restoring parameters from models/LSTM/pretrained_model.ckpt-31380\n",
      "acc:  0.33\n",
      "acc:  0.27\n",
      "acc:  0.33\n",
      "acc:  0.31\n",
      "acc:  0.35\n",
      "acc:  0.31\n",
      "acc:  0.36\n",
      "acc:  0.34\n",
      "acc:  0.3\n",
      "acc:  0.29\n",
      "acc:  0.41\n",
      "acc:  0.38\n",
      "acc:  0.29\n",
      "acc:  0.33\n",
      "\n",
      "14 \t [1.70921211 2.05936397 6.41436911]\n",
      "val accuracy:  0.3285714 \t f_! score:  [0.12208658 0.14709743 0.45816922]\n",
      "\n",
      "14 \t [1.70921211 2.05936397 6.41436911]\n",
      "14 \t [ 1.23320707  1.24615317 11.78836632]\n",
      "14 \t [3.15934066 7.11113054 4.42774031]\n",
      "14 \t [ 105.  116. 1179.]\n",
      "---just Test  Medical ---\n",
      "0.3285714\n",
      "f1:  [0.12208658 0.14709743 0.45816922]\n",
      "(37126,)\n",
      "(37126,)\n",
      "37126\n",
      "Vorsicht! es muss vorher ein  LSTM  Netz abgespeichert worden sein\n",
      "\n",
      "Restoring...\n",
      "models/LSTM/pretrained_model.ckpt-31380\n",
      "INFO:tensorflow:Restoring parameters from models/LSTM/pretrained_model.ckpt-31380\n",
      "acc:  0.31\n",
      "acc:  0.15\n",
      "acc:  0.24\n",
      "acc:  0.2\n",
      "acc:  0.13\n",
      "acc:  0.24\n",
      "acc:  0.19\n",
      "acc:  0.23\n",
      "acc:  0.14\n",
      "acc:  0.18\n",
      "acc:  0.18\n",
      "acc:  0.3\n",
      "acc:  0.23\n",
      "acc:  0.24\n",
      "acc:  0.27\n",
      "acc:  0.13\n",
      "acc:  0.22\n",
      "acc:  0.27\n",
      "acc:  0.11\n",
      "acc:  0.29\n",
      "acc:  0.2\n",
      "acc:  0.21\n",
      "acc:  0.27\n",
      "acc:  0.31\n",
      "acc:  0.25\n",
      "acc:  0.22\n",
      "acc:  0.15\n",
      "acc:  0.25\n",
      "acc:  0.21\n",
      "acc:  0.17\n",
      "acc:  0.27\n",
      "acc:  0.26\n",
      "acc:  0.35\n",
      "acc:  0.26\n",
      "acc:  0.2\n",
      "acc:  0.24\n",
      "acc:  0.29\n",
      "acc:  0.24\n",
      "acc:  0.18\n",
      "acc:  0.23\n",
      "acc:  0.1\n",
      "acc:  0.09\n",
      "acc:  0.23\n",
      "acc:  0.28\n",
      "acc:  0.14\n",
      "acc:  0.2\n",
      "acc:  0.16\n",
      "acc:  0.13\n",
      "acc:  0.09\n",
      "acc:  0.21\n",
      "acc:  0.26\n",
      "acc:  0.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yannik/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.25\n",
      "acc:  0.22\n",
      "acc:  0.07\n",
      "acc:  0.13\n",
      "acc:  0.16\n",
      "acc:  0.23\n",
      "acc:  0.22\n",
      "acc:  0.24\n",
      "acc:  0.28\n",
      "acc:  0.35\n",
      "acc:  0.15\n",
      "acc:  0.33\n",
      "acc:  0.19\n",
      "acc:  0.23\n",
      "acc:  0.27\n",
      "acc:  0.22\n",
      "acc:  0.27\n",
      "acc:  0.24\n",
      "\n",
      "70 \t [ 4.26224886 10.15519152 20.18378676]\n",
      "val accuracy:  0.21657144 \t f_! score:  [0.06088927 0.14507416 0.28833981]\n",
      "\n",
      "70 \t [ 4.26224886 10.15519152 20.18378676]\n",
      "70 \t [ 5.00199523  5.90688454 56.38060927]\n",
      "70 \t [ 4.92869996 47.72447815 12.65907904]\n",
      "70 \t [ 501.  664. 5835.]\n",
      "---just Test  Prime ---\n",
      "0.21657144\n",
      "f1:  [0.06088927 0.14507416 0.28833981]\n"
     ]
    }
   ],
   "source": [
    "net_name = \"LSTM\"\n",
    "\n",
    "testhelper = TestHelper()\n",
    "review_loss, review_acc = testhelper.train_input(\"Review\", net_name)\n",
    "\n",
    "testhelper.just_test(\"Twitter\", net_name)\n",
    "testhelper.just_test(\"Medical\", net_name)\n",
    "testhelper.just_test(\"Prime\", net_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  Twitter\n",
      "delete old models\n",
      "65730\n",
      "Tensor(\"lstm_net/dense/BiasAdd:0\", shape=(?, 3), dtype=float32)\n",
      "Tensor(\"lstm_net_1/dense/BiasAdd:0\", shape=(?, 3), dtype=float32)\n",
      "---init ready   LSTM ---\n",
      "\n",
      "Saving...\n",
      "saved to models/LSTM/pretrained_model.ckpt-0\n",
      "0 : loss:  3.0008154 \t acc:  0.28\n",
      "2 : loss:  1.1257457 \t acc:  0.38\n",
      "4 : loss:  1.5177373 \t acc:  0.33\n",
      "6 : loss:  1.0809916 \t acc:  0.42\n",
      "8 : loss:  1.278701 \t acc:  0.3\n",
      "10 : loss:  1.0787684 \t acc:  0.46\n",
      "12 : loss:  1.077097 \t acc:  0.34\n",
      "14 : loss:  1.1429827 \t acc:  0.33\n",
      "16 : loss:  1.0476928 \t acc:  0.47\n",
      "18 : loss:  1.0300888 \t acc:  0.44\n",
      "20 : loss:  1.0408623 \t acc:  0.45\n",
      "22 : loss:  1.0710579 \t acc:  0.44\n",
      "24 : loss:  1.0877337 \t acc:  0.45\n",
      "26 : loss:  1.0429258 \t acc:  0.49\n",
      "28 : loss:  1.0283513 \t acc:  0.45\n",
      "30 : loss:  1.0131342 \t acc:  0.5\n",
      "32 : loss:  1.0129985 \t acc:  0.44\n",
      "34 : loss:  1.0229924 \t acc:  0.51\n",
      "36 : loss:  1.0601673 \t acc:  0.39\n",
      "38 : loss:  1.1049548 \t acc:  0.37\n",
      "40 : loss:  1.064651 \t acc:  0.42\n",
      "42 : loss:  1.0413594 \t acc:  0.48\n",
      "44 : loss:  1.0430471 \t acc:  0.37\n",
      "46 : loss:  1.0143386 \t acc:  0.49\n",
      "48 : loss:  1.0119944 \t acc:  0.45\n",
      "50 : loss:  1.045876 \t acc:  0.4\n",
      "52 : loss:  1.0012496 \t acc:  0.47\n",
      "54 : loss:  1.0008767 \t acc:  0.44\n",
      "56 : loss:  0.9426319 \t acc:  0.54\n",
      "58 : loss:  1.0455313 \t acc:  0.49\n",
      "60 : loss:  0.9802364 \t acc:  0.55\n",
      "62 : loss:  0.88500327 \t acc:  0.59\n",
      "64 : loss:  0.9970207 \t acc:  0.43\n",
      "66 : loss:  0.98783064 \t acc:  0.53\n",
      "68 : loss:  0.9919468 \t acc:  0.48\n",
      "70 : loss:  0.97270364 \t acc:  0.56\n",
      "72 : loss:  0.9289476 \t acc:  0.52\n",
      "74 : loss:  0.99626917 \t acc:  0.52\n",
      "76 : loss:  1.0062176 \t acc:  0.55\n",
      "78 : loss:  0.9380137 \t acc:  0.57\n",
      "80 : loss:  1.0354058 \t acc:  0.46\n",
      "82 : loss:  0.96380424 \t acc:  0.6\n",
      "84 : loss:  1.102428 \t acc:  0.44\n",
      "86 : loss:  1.026202 \t acc:  0.57\n",
      "88 : loss:  0.9507557 \t acc:  0.57\n",
      "90 : loss:  0.96785283 \t acc:  0.55\n",
      "92 : loss:  0.87403077 \t acc:  0.59\n",
      "94 : loss:  0.93859136 \t acc:  0.57\n",
      "96 : loss:  0.9664596 \t acc:  0.53\n",
      "98 : loss:  1.0280668 \t acc:  0.46\n",
      "100 : loss:  1.1285045 \t acc:  0.49\n",
      "102 : loss:  0.9455555 \t acc:  0.58\n",
      "104 : loss:  0.97015834 \t acc:  0.53\n",
      "106 : loss:  0.9920769 \t acc:  0.55\n",
      "108 : loss:  0.98059416 \t acc:  0.55\n",
      "110 : loss:  0.93676084 \t acc:  0.53\n",
      "112 : loss:  0.9755417 \t acc:  0.54\n",
      "114 : loss:  1.0099058 \t acc:  0.52\n",
      "116 : loss:  1.0103158 \t acc:  0.52\n",
      "118 : loss:  0.8772861 \t acc:  0.56\n",
      "120 : loss:  0.9604198 \t acc:  0.47\n",
      "122 : loss:  0.9807936 \t acc:  0.59\n",
      "124 : loss:  0.91782534 \t acc:  0.56\n",
      "126 : loss:  0.86981386 \t acc:  0.58\n",
      "128 : loss:  0.9627335 \t acc:  0.53\n",
      "130 : loss:  0.9197153 \t acc:  0.56\n",
      "132 : loss:  0.91009855 \t acc:  0.56\n",
      "134 : loss:  0.9073348 \t acc:  0.55\n",
      "136 : loss:  0.98227227 \t acc:  0.5\n",
      "138 : loss:  0.9323566 \t acc:  0.56\n",
      "140 : loss:  0.92884207 \t acc:  0.56\n",
      "142 : loss:  0.9586227 \t acc:  0.48\n",
      "144 : loss:  0.9771266 \t acc:  0.54\n",
      "146 : loss:  0.94144243 \t acc:  0.52\n",
      "148 : loss:  0.84379876 \t acc:  0.64\n",
      "150 : loss:  0.97909296 \t acc:  0.46\n",
      "152 : loss:  0.90861344 \t acc:  0.61\n",
      "154 : loss:  0.91251457 \t acc:  0.54\n",
      "156 : loss:  1.0037576 \t acc:  0.52\n",
      "158 : loss:  0.94895226 \t acc:  0.58\n",
      "160 : loss:  0.85050166 \t acc:  0.67\n",
      "162 : loss:  0.9908275 \t acc:  0.46\n",
      "164 : loss:  0.89212 \t acc:  0.59\n",
      "166 : loss:  0.8999228 \t acc:  0.56\n",
      "168 : loss:  0.95316124 \t acc:  0.53\n",
      "170 : loss:  0.9125037 \t acc:  0.6\n",
      "172 : loss:  0.8906154 \t acc:  0.57\n",
      "174 : loss:  0.925746 \t acc:  0.61\n",
      "176 : loss:  0.8989555 \t acc:  0.64\n",
      "178 : loss:  0.8748068 \t acc:  0.65\n",
      "180 : loss:  0.9819734 \t acc:  0.45\n",
      "182 : loss:  0.9776265 \t acc:  0.53\n",
      "184 : loss:  1.0475764 \t acc:  0.46\n",
      "186 : loss:  1.0004596 \t acc:  0.47\n",
      "188 : loss:  0.87397885 \t acc:  0.57\n",
      "190 : loss:  0.8622194 \t acc:  0.56\n",
      "192 : loss:  0.894222 \t acc:  0.63\n",
      "194 : loss:  0.9679394 \t acc:  0.54\n",
      "196 : loss:  0.8694899 \t acc:  0.59\n",
      "198 : loss:  0.8846439 \t acc:  0.6\n",
      "200 : loss:  0.8868116 \t acc:  0.56\n",
      "202 : loss:  0.99388415 \t acc:  0.5\n",
      "204 : loss:  1.0391257 \t acc:  0.48\n",
      "206 : loss:  0.84091157 \t acc:  0.6\n",
      "208 : loss:  0.97756606 \t acc:  0.49\n",
      "210 : loss:  0.8802097 \t acc:  0.6\n",
      "212 : loss:  0.9502774 \t acc:  0.51\n",
      "214 : loss:  0.87801635 \t acc:  0.54\n",
      "216 : loss:  0.88936746 \t acc:  0.58\n",
      "218 : loss:  0.9268728 \t acc:  0.51\n",
      "220 : loss:  0.9966182 \t acc:  0.52\n",
      "222 : loss:  0.85098296 \t acc:  0.6\n",
      "224 : loss:  0.8999647 \t acc:  0.63\n",
      "226 : loss:  0.9522779 \t acc:  0.56\n",
      "228 : loss:  1.011998 \t acc:  0.51\n",
      "230 : loss:  0.9890927 \t acc:  0.41\n",
      "232 : loss:  0.87586755 \t acc:  0.55\n",
      "234 : loss:  0.8186771 \t acc:  0.63\n",
      "236 : loss:  0.9926719 \t acc:  0.46\n",
      "238 : loss:  0.84141195 \t acc:  0.6\n",
      "240 : loss:  0.85452944 \t acc:  0.61\n",
      "242 : loss:  0.91178805 \t acc:  0.55\n",
      "244 : loss:  0.90229934 \t acc:  0.58\n",
      "246 : loss:  0.91593665 \t acc:  0.59\n",
      "248 : loss:  0.9136614 \t acc:  0.56\n",
      "250 : loss:  0.9350885 \t acc:  0.48\n",
      "252 : loss:  0.9029158 \t acc:  0.58\n",
      "254 : loss:  0.8710861 \t acc:  0.68\n",
      "256 : loss:  0.83810884 \t acc:  0.66\n",
      "258 : loss:  0.9291656 \t acc:  0.54\n",
      "260 : loss:  0.91839325 \t acc:  0.57\n",
      "262 : loss:  0.9399965 \t acc:  0.5\n",
      "264 : loss:  0.9128515 \t acc:  0.64\n",
      "266 : loss:  0.96505266 \t acc:  0.55\n",
      "268 : loss:  0.87842315 \t acc:  0.57\n",
      "270 : loss:  0.9887716 \t acc:  0.52\n",
      "272 : loss:  0.83343744 \t acc:  0.6\n",
      "274 : loss:  0.9162772 \t acc:  0.58\n",
      "276 : loss:  0.9182513 \t acc:  0.5\n",
      "278 : loss:  0.90061593 \t acc:  0.54\n",
      "280 : loss:  0.970375 \t acc:  0.56\n",
      "282 : loss:  0.94067186 \t acc:  0.49\n",
      "284 : loss:  0.88136125 \t acc:  0.6\n",
      "286 : loss:  0.9557674 \t acc:  0.52\n",
      "288 : loss:  0.94773453 \t acc:  0.56\n",
      "290 : loss:  0.77053535 \t acc:  0.63\n",
      "292 : loss:  0.8517012 \t acc:  0.61\n",
      "294 : loss:  0.912653 \t acc:  0.58\n",
      "296 : loss:  0.97278434 \t acc:  0.53\n",
      "298 : loss:  0.8678192 \t acc:  0.59\n",
      "300 : loss:  0.84670013 \t acc:  0.63\n",
      "302 : loss:  0.8696566 \t acc:  0.51\n",
      "304 : loss:  0.8909778 \t acc:  0.58\n",
      "306 : loss:  0.89208114 \t acc:  0.6\n",
      "308 : loss:  1.0442312 \t acc:  0.51\n",
      "310 : loss:  0.9815391 \t acc:  0.52\n",
      "312 : loss:  0.89810526 \t acc:  0.58\n",
      "314 : loss:  0.96000326 \t acc:  0.6\n",
      "316 : loss:  0.918462 \t acc:  0.53\n",
      "318 : loss:  0.88076156 \t acc:  0.56\n",
      "320 : loss:  0.9177872 \t acc:  0.59\n",
      "322 : loss:  0.94309217 \t acc:  0.51\n",
      "324 : loss:  0.86042404 \t acc:  0.61\n",
      "326 : loss:  0.92295873 \t acc:  0.55\n",
      "328 : loss:  0.88473463 \t acc:  0.56\n",
      "330 : loss:  1.019462 \t acc:  0.53\n",
      "332 : loss:  0.85428613 \t acc:  0.66\n",
      "334 : loss:  0.92171997 \t acc:  0.57\n",
      "336 : loss:  0.86928076 \t acc:  0.55\n",
      "338 : loss:  0.84712046 \t acc:  0.63\n",
      "340 : loss:  0.794735 \t acc:  0.58\n",
      "342 : loss:  0.9504394 \t acc:  0.55\n",
      "344 : loss:  0.96241456 \t acc:  0.5\n",
      "346 : loss:  1.018353 \t acc:  0.46\n",
      "348 : loss:  0.85420173 \t acc:  0.59\n",
      "350 : loss:  0.8584826 \t acc:  0.61\n",
      "352 : loss:  0.8786946 \t acc:  0.57\n",
      "354 : loss:  0.86429614 \t acc:  0.64\n",
      "356 : loss:  0.8384982 \t acc:  0.56\n",
      "358 : loss:  1.0625578 \t acc:  0.52\n",
      "360 : loss:  0.8498001 \t acc:  0.62\n",
      "362 : loss:  0.88422495 \t acc:  0.58\n",
      "364 : loss:  0.9007459 \t acc:  0.55\n",
      "366 : loss:  0.8832937 \t acc:  0.56\n",
      "368 : loss:  0.94685847 \t acc:  0.5\n",
      "370 : loss:  0.88848335 \t acc:  0.58\n",
      "\n",
      "Saving...\n",
      "saved to models/LSTM/pretrained_model.ckpt-372\n",
      "\n",
      "100 \t [63.35112518 58.34855911 51.54486444]\n",
      "0 \tval accuracy:  0.5804 \t f_! score:  [0.63351125 0.58348559 0.51544864]\n",
      "\n",
      "372 : loss:  0.88017434 \t acc:  0.59\n",
      "374 : loss:  0.9291727 \t acc:  0.54\n",
      "376 : loss:  0.8910936 \t acc:  0.55\n",
      "378 : loss:  0.88921195 \t acc:  0.58\n",
      "380 : loss:  0.9231771 \t acc:  0.56\n",
      "382 : loss:  0.84490126 \t acc:  0.65\n",
      "384 : loss:  0.8208251 \t acc:  0.62\n",
      "386 : loss:  0.9216385 \t acc:  0.58\n",
      "388 : loss:  0.8582646 \t acc:  0.61\n",
      "390 : loss:  0.8492492 \t acc:  0.62\n",
      "392 : loss:  0.8934328 \t acc:  0.67\n",
      "394 : loss:  0.9568555 \t acc:  0.53\n",
      "396 : loss:  0.9267674 \t acc:  0.54\n",
      "398 : loss:  0.8780897 \t acc:  0.54\n",
      "400 : loss:  0.9325093 \t acc:  0.49\n",
      "402 : loss:  0.8657553 \t acc:  0.68\n",
      "404 : loss:  0.8608856 \t acc:  0.61\n",
      "406 : loss:  0.7688486 \t acc:  0.65\n",
      "408 : loss:  0.88897955 \t acc:  0.54\n",
      "410 : loss:  0.9092751 \t acc:  0.57\n",
      "412 : loss:  0.88793635 \t acc:  0.54\n",
      "414 : loss:  0.9900538 \t acc:  0.53\n",
      "416 : loss:  0.8619877 \t acc:  0.61\n",
      "418 : loss:  0.85894257 \t acc:  0.59\n",
      "420 : loss:  0.8610718 \t acc:  0.53\n",
      "422 : loss:  0.8327631 \t acc:  0.52\n",
      "424 : loss:  0.88557416 \t acc:  0.65\n",
      "426 : loss:  0.8874724 \t acc:  0.58\n",
      "428 : loss:  0.9136605 \t acc:  0.55\n",
      "430 : loss:  0.83229494 \t acc:  0.61\n",
      "432 : loss:  0.9260502 \t acc:  0.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "434 : loss:  0.9627734 \t acc:  0.49\n",
      "436 : loss:  0.8628241 \t acc:  0.68\n",
      "438 : loss:  0.9598637 \t acc:  0.56\n",
      "440 : loss:  0.8922242 \t acc:  0.55\n",
      "442 : loss:  0.96577096 \t acc:  0.55\n",
      "444 : loss:  0.85689247 \t acc:  0.58\n",
      "446 : loss:  0.9165132 \t acc:  0.54\n",
      "448 : loss:  0.90237534 \t acc:  0.6\n",
      "450 : loss:  0.8598556 \t acc:  0.61\n",
      "452 : loss:  0.84210914 \t acc:  0.66\n",
      "454 : loss:  0.8717475 \t acc:  0.65\n",
      "456 : loss:  0.87128615 \t acc:  0.57\n",
      "458 : loss:  0.8333042 \t acc:  0.63\n",
      "460 : loss:  0.9404206 \t acc:  0.52\n",
      "462 : loss:  0.8107044 \t acc:  0.62\n",
      "464 : loss:  0.8889263 \t acc:  0.52\n",
      "466 : loss:  0.89102876 \t acc:  0.59\n",
      "468 : loss:  0.82160497 \t acc:  0.6\n",
      "470 : loss:  0.944197 \t acc:  0.54\n",
      "472 : loss:  0.9580452 \t acc:  0.52\n",
      "474 : loss:  0.90263647 \t acc:  0.58\n",
      "476 : loss:  0.8164062 \t acc:  0.62\n",
      "478 : loss:  0.89035064 \t acc:  0.6\n",
      "480 : loss:  0.98142713 \t acc:  0.48\n",
      "482 : loss:  0.7756737 \t acc:  0.63\n",
      "484 : loss:  0.89947915 \t acc:  0.59\n",
      "486 : loss:  1.0348171 \t acc:  0.45\n",
      "488 : loss:  0.9322627 \t acc:  0.57\n",
      "490 : loss:  0.8819049 \t acc:  0.54\n",
      "492 : loss:  0.9185743 \t acc:  0.59\n",
      "494 : loss:  0.8909757 \t acc:  0.59\n",
      "496 : loss:  0.9235902 \t acc:  0.57\n",
      "498 : loss:  0.96334517 \t acc:  0.46\n",
      "500 : loss:  0.7289143 \t acc:  0.67\n",
      "502 : loss:  0.8594955 \t acc:  0.6\n",
      "504 : loss:  0.8695817 \t acc:  0.58\n",
      "506 : loss:  0.94336975 \t acc:  0.53\n",
      "508 : loss:  0.92500657 \t acc:  0.56\n",
      "510 : loss:  0.8771145 \t acc:  0.55\n",
      "512 : loss:  0.8740106 \t acc:  0.54\n",
      "514 : loss:  0.89805764 \t acc:  0.64\n",
      "516 : loss:  0.80469805 \t acc:  0.62\n",
      "518 : loss:  0.90543276 \t acc:  0.59\n",
      "520 : loss:  0.7823607 \t acc:  0.64\n",
      "522 : loss:  1.0370086 \t acc:  0.46\n",
      "524 : loss:  0.85963356 \t acc:  0.6\n",
      "526 : loss:  0.84916204 \t acc:  0.57\n",
      "528 : loss:  0.86955214 \t acc:  0.59\n",
      "530 : loss:  1.0206991 \t acc:  0.49\n",
      "532 : loss:  0.870957 \t acc:  0.62\n",
      "534 : loss:  0.8569589 \t acc:  0.61\n",
      "536 : loss:  0.82302666 \t acc:  0.58\n",
      "538 : loss:  0.91179526 \t acc:  0.54\n",
      "540 : loss:  0.8836531 \t acc:  0.57\n",
      "542 : loss:  0.8665855 \t acc:  0.58\n",
      "544 : loss:  0.8376335 \t acc:  0.65\n",
      "546 : loss:  0.8909848 \t acc:  0.55\n",
      "548 : loss:  0.8941792 \t acc:  0.61\n",
      "550 : loss:  0.9537896 \t acc:  0.58\n",
      "552 : loss:  0.9192909 \t acc:  0.55\n",
      "554 : loss:  0.93963915 \t acc:  0.51\n",
      "556 : loss:  1.024786 \t acc:  0.47\n",
      "558 : loss:  0.9450133 \t acc:  0.59\n",
      "560 : loss:  0.77980334 \t acc:  0.61\n",
      "562 : loss:  0.9381683 \t acc:  0.52\n",
      "564 : loss:  0.92543596 \t acc:  0.57\n",
      "566 : loss:  0.8928852 \t acc:  0.53\n",
      "568 : loss:  0.99147344 \t acc:  0.48\n",
      "570 : loss:  0.8829389 \t acc:  0.54\n",
      "572 : loss:  0.836752 \t acc:  0.62\n",
      "574 : loss:  0.8650802 \t acc:  0.59\n",
      "576 : loss:  0.8703298 \t acc:  0.54\n",
      "578 : loss:  0.89617836 \t acc:  0.53\n",
      "580 : loss:  0.816891 \t acc:  0.61\n",
      "582 : loss:  0.87606347 \t acc:  0.61\n",
      "584 : loss:  0.91040206 \t acc:  0.61\n",
      "586 : loss:  0.90175986 \t acc:  0.58\n",
      "588 : loss:  0.8032098 \t acc:  0.6\n",
      "590 : loss:  0.82088464 \t acc:  0.62\n",
      "592 : loss:  0.92159605 \t acc:  0.57\n",
      "594 : loss:  0.87735474 \t acc:  0.56\n",
      "596 : loss:  0.9051746 \t acc:  0.58\n",
      "598 : loss:  0.936655 \t acc:  0.53\n",
      "600 : loss:  0.86933 \t acc:  0.59\n",
      "602 : loss:  0.9305886 \t acc:  0.56\n",
      "604 : loss:  0.8176642 \t acc:  0.62\n",
      "606 : loss:  0.96603936 \t acc:  0.51\n",
      "608 : loss:  0.8092549 \t acc:  0.64\n",
      "610 : loss:  0.8126264 \t acc:  0.64\n",
      "612 : loss:  0.8742276 \t acc:  0.52\n",
      "614 : loss:  0.934463 \t acc:  0.5\n",
      "616 : loss:  0.95128495 \t acc:  0.57\n",
      "618 : loss:  0.8047789 \t acc:  0.6\n",
      "620 : loss:  0.900963 \t acc:  0.55\n",
      "622 : loss:  0.78706545 \t acc:  0.65\n",
      "624 : loss:  0.84166366 \t acc:  0.58\n",
      "626 : loss:  0.88749075 \t acc:  0.52\n",
      "628 : loss:  0.80367315 \t acc:  0.61\n",
      "630 : loss:  0.79229164 \t acc:  0.65\n",
      "632 : loss:  0.88052887 \t acc:  0.54\n",
      "634 : loss:  0.8604047 \t acc:  0.61\n",
      "636 : loss:  0.83257055 \t acc:  0.61\n",
      "638 : loss:  0.9739353 \t acc:  0.52\n",
      "640 : loss:  0.9765036 \t acc:  0.51\n",
      "642 : loss:  0.8956655 \t acc:  0.57\n",
      "644 : loss:  0.9068126 \t acc:  0.57\n",
      "646 : loss:  0.7592387 \t acc:  0.63\n",
      "648 : loss:  0.797129 \t acc:  0.61\n",
      "650 : loss:  0.8435164 \t acc:  0.62\n",
      "652 : loss:  0.8093344 \t acc:  0.61\n",
      "654 : loss:  0.82371444 \t acc:  0.61\n",
      "656 : loss:  0.88849425 \t acc:  0.61\n",
      "658 : loss:  0.995023 \t acc:  0.48\n",
      "660 : loss:  0.83843696 \t acc:  0.61\n",
      "662 : loss:  0.9303129 \t acc:  0.56\n",
      "664 : loss:  0.9178361 \t acc:  0.5\n",
      "666 : loss:  0.8109828 \t acc:  0.68\n",
      "668 : loss:  0.96599215 \t acc:  0.54\n",
      "670 : loss:  0.93473756 \t acc:  0.57\n",
      "672 : loss:  0.82535917 \t acc:  0.71\n",
      "674 : loss:  0.90982145 \t acc:  0.55\n",
      "676 : loss:  0.81963366 \t acc:  0.61\n",
      "678 : loss:  0.9023716 \t acc:  0.54\n",
      "680 : loss:  0.89182526 \t acc:  0.59\n",
      "682 : loss:  0.842605 \t acc:  0.56\n",
      "684 : loss:  0.8669373 \t acc:  0.59\n",
      "686 : loss:  0.90859115 \t acc:  0.55\n",
      "688 : loss:  0.92417026 \t acc:  0.51\n",
      "690 : loss:  0.9220687 \t acc:  0.59\n",
      "692 : loss:  0.8546981 \t acc:  0.58\n",
      "694 : loss:  0.9528937 \t acc:  0.55\n",
      "696 : loss:  0.89114946 \t acc:  0.59\n",
      "698 : loss:  0.87229204 \t acc:  0.6\n",
      "700 : loss:  0.8188243 \t acc:  0.61\n",
      "702 : loss:  0.935937 \t acc:  0.56\n",
      "704 : loss:  0.8756759 \t acc:  0.58\n",
      "706 : loss:  0.9409695 \t acc:  0.58\n",
      "708 : loss:  0.88919854 \t acc:  0.61\n",
      "710 : loss:  0.9599343 \t acc:  0.55\n",
      "712 : loss:  0.9166034 \t acc:  0.5\n",
      "714 : loss:  0.80439276 \t acc:  0.58\n",
      "716 : loss:  0.8920915 \t acc:  0.61\n",
      "718 : loss:  0.83515877 \t acc:  0.63\n",
      "720 : loss:  0.73222995 \t acc:  0.66\n",
      "722 : loss:  0.85874534 \t acc:  0.58\n",
      "724 : loss:  0.84791297 \t acc:  0.58\n",
      "726 : loss:  0.8378118 \t acc:  0.65\n",
      "728 : loss:  0.80405426 \t acc:  0.64\n",
      "730 : loss:  0.9363017 \t acc:  0.56\n",
      "732 : loss:  0.8753621 \t acc:  0.55\n",
      "734 : loss:  0.84151316 \t acc:  0.62\n",
      "736 : loss:  0.83996713 \t acc:  0.64\n",
      "738 : loss:  0.78995895 \t acc:  0.63\n",
      "740 : loss:  0.8291241 \t acc:  0.65\n",
      "742 : loss:  0.9116469 \t acc:  0.58\n",
      "\n",
      "Saving...\n",
      "saved to models/LSTM/pretrained_model.ckpt-744\n",
      "\n",
      "100 \t [60.89503727 62.27893395 50.86057458]\n",
      "1 \tval accuracy:  0.5836 \t f_! score:  [0.60895037 0.62278934 0.50860575]\n",
      "\n",
      "744 : loss:  0.9434596 \t acc:  0.53\n",
      "746 : loss:  0.8290992 \t acc:  0.59\n",
      "748 : loss:  0.8547546 \t acc:  0.57\n",
      "750 : loss:  0.8581976 \t acc:  0.58\n",
      "752 : loss:  0.9350029 \t acc:  0.51\n",
      "754 : loss:  0.8547068 \t acc:  0.61\n",
      "756 : loss:  0.81746703 \t acc:  0.63\n",
      "758 : loss:  0.8929805 \t acc:  0.62\n",
      "760 : loss:  0.8260707 \t acc:  0.67\n",
      "762 : loss:  0.89317614 \t acc:  0.58\n",
      "764 : loss:  0.7721274 \t acc:  0.62\n",
      "766 : loss:  0.85754085 \t acc:  0.6\n",
      "768 : loss:  0.95654 \t acc:  0.56\n",
      "770 : loss:  0.8495107 \t acc:  0.6\n",
      "772 : loss:  0.85756373 \t acc:  0.62\n",
      "774 : loss:  0.7773541 \t acc:  0.65\n",
      "776 : loss:  0.87333417 \t acc:  0.63\n",
      "778 : loss:  0.92386216 \t acc:  0.57\n",
      "780 : loss:  0.77124566 \t acc:  0.68\n",
      "782 : loss:  0.8921196 \t acc:  0.62\n",
      "784 : loss:  0.75846887 \t acc:  0.67\n",
      "786 : loss:  0.81861824 \t acc:  0.63\n",
      "788 : loss:  0.7762031 \t acc:  0.68\n",
      "790 : loss:  0.92697436 \t acc:  0.58\n",
      "792 : loss:  0.85879743 \t acc:  0.58\n",
      "794 : loss:  0.7630853 \t acc:  0.66\n",
      "796 : loss:  0.8369091 \t acc:  0.61\n",
      "798 : loss:  0.86110336 \t acc:  0.62\n",
      "800 : loss:  0.92963934 \t acc:  0.57\n",
      "802 : loss:  0.7925608 \t acc:  0.65\n",
      "804 : loss:  0.97280276 \t acc:  0.5\n",
      "806 : loss:  0.87832373 \t acc:  0.57\n",
      "808 : loss:  0.79467833 \t acc:  0.63\n",
      "810 : loss:  0.7764414 \t acc:  0.63\n",
      "812 : loss:  0.8356937 \t acc:  0.58\n",
      "814 : loss:  0.8292506 \t acc:  0.65\n",
      "816 : loss:  0.84408134 \t acc:  0.61\n",
      "818 : loss:  0.9152394 \t acc:  0.57\n",
      "820 : loss:  0.94456255 \t acc:  0.54\n",
      "822 : loss:  0.8128695 \t acc:  0.62\n",
      "824 : loss:  0.85818326 \t acc:  0.62\n",
      "826 : loss:  0.9344178 \t acc:  0.53\n",
      "828 : loss:  0.70533025 \t acc:  0.74\n",
      "830 : loss:  0.97331274 \t acc:  0.55\n",
      "832 : loss:  0.8296898 \t acc:  0.56\n",
      "834 : loss:  0.8601979 \t acc:  0.65\n",
      "836 : loss:  0.8767276 \t acc:  0.56\n",
      "838 : loss:  0.8506958 \t acc:  0.54\n",
      "840 : loss:  0.85665464 \t acc:  0.58\n",
      "842 : loss:  0.9465668 \t acc:  0.53\n",
      "844 : loss:  0.87086546 \t acc:  0.6\n",
      "846 : loss:  0.8877066 \t acc:  0.61\n",
      "848 : loss:  0.9225312 \t acc:  0.53\n",
      "850 : loss:  0.78449935 \t acc:  0.61\n",
      "852 : loss:  0.89028543 \t acc:  0.57\n",
      "854 : loss:  0.85244834 \t acc:  0.63\n",
      "856 : loss:  0.8353539 \t acc:  0.61\n",
      "858 : loss:  0.91092324 \t acc:  0.59\n",
      "860 : loss:  0.89153236 \t acc:  0.58\n",
      "862 : loss:  0.94891286 \t acc:  0.52\n",
      "864 : loss:  0.97528285 \t acc:  0.51\n",
      "866 : loss:  0.92267764 \t acc:  0.58\n",
      "868 : loss:  0.8630418 \t acc:  0.56\n",
      "870 : loss:  0.86770177 \t acc:  0.55\n",
      "872 : loss:  0.8310757 \t acc:  0.6\n",
      "874 : loss:  0.80718726 \t acc:  0.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "876 : loss:  0.8154875 \t acc:  0.6\n",
      "878 : loss:  0.8572208 \t acc:  0.56\n",
      "880 : loss:  0.8869579 \t acc:  0.6\n",
      "882 : loss:  0.7733843 \t acc:  0.64\n",
      "884 : loss:  0.72962004 \t acc:  0.7\n",
      "886 : loss:  0.93449813 \t acc:  0.55\n",
      "888 : loss:  0.9059029 \t acc:  0.58\n",
      "890 : loss:  0.85994613 \t acc:  0.63\n",
      "892 : loss:  0.8402626 \t acc:  0.59\n",
      "894 : loss:  0.9018622 \t acc:  0.62\n",
      "896 : loss:  0.9609239 \t acc:  0.56\n",
      "898 : loss:  0.7864017 \t acc:  0.65\n",
      "900 : loss:  0.82084334 \t acc:  0.63\n",
      "902 : loss:  0.87777275 \t acc:  0.58\n",
      "904 : loss:  0.92375076 \t acc:  0.53\n",
      "906 : loss:  0.8658017 \t acc:  0.6\n",
      "908 : loss:  0.9124726 \t acc:  0.59\n",
      "910 : loss:  0.8902844 \t acc:  0.61\n",
      "912 : loss:  0.9228584 \t acc:  0.54\n",
      "914 : loss:  0.91028184 \t acc:  0.6\n",
      "916 : loss:  0.83607966 \t acc:  0.63\n",
      "918 : loss:  0.97251016 \t acc:  0.52\n",
      "920 : loss:  0.90751535 \t acc:  0.59\n",
      "922 : loss:  0.89090896 \t acc:  0.58\n",
      "924 : loss:  0.84049994 \t acc:  0.63\n",
      "926 : loss:  0.77269995 \t acc:  0.65\n",
      "928 : loss:  0.9469446 \t acc:  0.58\n",
      "930 : loss:  0.891091 \t acc:  0.61\n",
      "932 : loss:  0.7675474 \t acc:  0.68\n",
      "934 : loss:  0.94314647 \t acc:  0.54\n",
      "936 : loss:  0.8509203 \t acc:  0.6\n",
      "938 : loss:  0.8568649 \t acc:  0.6\n",
      "940 : loss:  0.92303497 \t acc:  0.65\n",
      "942 : loss:  0.8452075 \t acc:  0.64\n",
      "944 : loss:  0.8976034 \t acc:  0.57\n",
      "946 : loss:  0.75056356 \t acc:  0.72\n",
      "948 : loss:  0.84990364 \t acc:  0.65\n",
      "950 : loss:  0.9334339 \t acc:  0.57\n",
      "952 : loss:  0.7615806 \t acc:  0.66\n",
      "954 : loss:  0.8145877 \t acc:  0.63\n",
      "956 : loss:  0.8485501 \t acc:  0.62\n",
      "958 : loss:  0.7748794 \t acc:  0.73\n",
      "960 : loss:  0.8291032 \t acc:  0.67\n",
      "962 : loss:  0.84062177 \t acc:  0.64\n",
      "964 : loss:  0.80055815 \t acc:  0.67\n",
      "966 : loss:  0.8084926 \t acc:  0.63\n",
      "968 : loss:  0.84768724 \t acc:  0.58\n",
      "970 : loss:  0.81258494 \t acc:  0.67\n",
      "972 : loss:  0.9849663 \t acc:  0.55\n",
      "974 : loss:  0.9409991 \t acc:  0.56\n",
      "976 : loss:  0.7638526 \t acc:  0.68\n",
      "978 : loss:  0.9109079 \t acc:  0.51\n",
      "980 : loss:  0.8753689 \t acc:  0.57\n",
      "982 : loss:  0.8249649 \t acc:  0.68\n",
      "984 : loss:  0.75796705 \t acc:  0.69\n",
      "986 : loss:  0.73095477 \t acc:  0.64\n",
      "988 : loss:  0.9211405 \t acc:  0.53\n",
      "990 : loss:  0.94191056 \t acc:  0.48\n",
      "992 : loss:  0.8210458 \t acc:  0.59\n",
      "994 : loss:  0.7160539 \t acc:  0.71\n",
      "996 : loss:  0.949921 \t acc:  0.55\n",
      "998 : loss:  0.8255865 \t acc:  0.6\n",
      "1000 : loss:  0.86374944 \t acc:  0.62\n",
      "1002 : loss:  0.85001016 \t acc:  0.68\n",
      "1004 : loss:  0.8312146 \t acc:  0.56\n",
      "1006 : loss:  0.80138826 \t acc:  0.62\n",
      "1008 : loss:  0.8968176 \t acc:  0.57\n",
      "1010 : loss:  0.8156351 \t acc:  0.63\n",
      "1012 : loss:  0.777304 \t acc:  0.67\n",
      "1014 : loss:  0.77617776 \t acc:  0.62\n",
      "1016 : loss:  0.82975775 \t acc:  0.6\n",
      "1018 : loss:  0.86959666 \t acc:  0.56\n",
      "1020 : loss:  0.7808533 \t acc:  0.64\n",
      "1022 : loss:  0.8449592 \t acc:  0.61\n",
      "1024 : loss:  0.84268796 \t acc:  0.57\n",
      "1026 : loss:  0.7868123 \t acc:  0.64\n",
      "1028 : loss:  0.83641297 \t acc:  0.65\n",
      "1030 : loss:  0.85781753 \t acc:  0.56\n",
      "1032 : loss:  0.8604876 \t acc:  0.56\n",
      "1034 : loss:  0.86912584 \t acc:  0.58\n",
      "1036 : loss:  0.8200959 \t acc:  0.62\n",
      "1038 : loss:  0.76912117 \t acc:  0.67\n",
      "1040 : loss:  0.88746053 \t acc:  0.6\n",
      "1042 : loss:  0.7967524 \t acc:  0.62\n",
      "1044 : loss:  0.8611154 \t acc:  0.57\n",
      "1046 : loss:  0.91262406 \t acc:  0.61\n",
      "1048 : loss:  0.84079415 \t acc:  0.61\n",
      "1050 : loss:  0.7514069 \t acc:  0.69\n",
      "1052 : loss:  0.756172 \t acc:  0.63\n",
      "1054 : loss:  0.84715986 \t acc:  0.63\n",
      "1056 : loss:  0.86937696 \t acc:  0.56\n",
      "1058 : loss:  0.963562 \t acc:  0.59\n",
      "1060 : loss:  0.8437214 \t acc:  0.61\n",
      "1062 : loss:  0.8754427 \t acc:  0.66\n",
      "1064 : loss:  0.9258985 \t acc:  0.58\n",
      "1066 : loss:  0.9241082 \t acc:  0.54\n",
      "1068 : loss:  0.9401603 \t acc:  0.5\n",
      "1070 : loss:  0.7980384 \t acc:  0.62\n",
      "1072 : loss:  0.8887783 \t acc:  0.61\n",
      "1074 : loss:  0.81936777 \t acc:  0.65\n",
      "1076 : loss:  0.81329095 \t acc:  0.63\n",
      "1078 : loss:  0.9975769 \t acc:  0.47\n",
      "1080 : loss:  0.9126336 \t acc:  0.56\n",
      "1082 : loss:  0.9443558 \t acc:  0.55\n",
      "1084 : loss:  0.8497747 \t acc:  0.6\n",
      "1086 : loss:  0.8658072 \t acc:  0.6\n",
      "1088 : loss:  0.82036537 \t acc:  0.63\n",
      "1090 : loss:  0.819664 \t acc:  0.63\n",
      "1092 : loss:  0.82047534 \t acc:  0.61\n",
      "1094 : loss:  0.72838956 \t acc:  0.69\n",
      "1096 : loss:  0.81607485 \t acc:  0.58\n",
      "1098 : loss:  0.9134617 \t acc:  0.59\n",
      "1100 : loss:  0.8730714 \t acc:  0.57\n",
      "1102 : loss:  0.9288295 \t acc:  0.6\n",
      "1104 : loss:  0.9066592 \t acc:  0.54\n",
      "1106 : loss:  0.76024383 \t acc:  0.68\n",
      "1108 : loss:  0.8345024 \t acc:  0.61\n",
      "1110 : loss:  0.83220565 \t acc:  0.58\n",
      "1112 : loss:  0.8203885 \t acc:  0.63\n",
      "1114 : loss:  0.8799656 \t acc:  0.56\n",
      "\n",
      "Saving...\n",
      "saved to models/LSTM/pretrained_model.ckpt-1116\n",
      "\n",
      "100 \t [66.36271574 56.46881485 55.24849317]\n",
      "2 \tval accuracy:  0.5975 \t f_! score:  [0.66362716 0.56468815 0.55248493]\n",
      "\n",
      "1116 : loss:  0.8566959 \t acc:  0.61\n",
      "1118 : loss:  0.89030075 \t acc:  0.57\n",
      "1120 : loss:  0.8209182 \t acc:  0.61\n",
      "1122 : loss:  0.8322928 \t acc:  0.67\n",
      "1124 : loss:  0.7713343 \t acc:  0.63\n",
      "1126 : loss:  0.98489 \t acc:  0.54\n",
      "1128 : loss:  0.7510498 \t acc:  0.71\n",
      "1130 : loss:  0.8079858 \t acc:  0.67\n",
      "1132 : loss:  0.85892403 \t acc:  0.56\n",
      "1134 : loss:  0.7988922 \t acc:  0.67\n",
      "1136 : loss:  0.88201165 \t acc:  0.6\n",
      "1138 : loss:  0.8296943 \t acc:  0.61\n",
      "1140 : loss:  0.86441535 \t acc:  0.56\n",
      "1142 : loss:  0.8706207 \t acc:  0.57\n",
      "1144 : loss:  0.7686351 \t acc:  0.58\n",
      "1146 : loss:  0.8632804 \t acc:  0.63\n",
      "1148 : loss:  0.8412787 \t acc:  0.6\n",
      "1150 : loss:  0.9227453 \t acc:  0.55\n",
      "1152 : loss:  0.8868138 \t acc:  0.57\n",
      "1154 : loss:  0.791009 \t acc:  0.61\n",
      "1156 : loss:  0.82903147 \t acc:  0.61\n",
      "1158 : loss:  0.8179882 \t acc:  0.57\n",
      "1160 : loss:  0.7250233 \t acc:  0.7\n",
      "1162 : loss:  0.84727705 \t acc:  0.6\n",
      "1164 : loss:  0.8506933 \t acc:  0.6\n",
      "1166 : loss:  0.87242484 \t acc:  0.59\n",
      "1168 : loss:  0.86081976 \t acc:  0.64\n",
      "1170 : loss:  0.8647919 \t acc:  0.64\n",
      "1172 : loss:  0.91575676 \t acc:  0.59\n",
      "1174 : loss:  0.8234707 \t acc:  0.57\n",
      "1176 : loss:  0.8834562 \t acc:  0.59\n",
      "1178 : loss:  0.877776 \t acc:  0.61\n",
      "1180 : loss:  0.8754123 \t acc:  0.56\n",
      "1182 : loss:  0.88111687 \t acc:  0.54\n",
      "1184 : loss:  0.83439046 \t acc:  0.58\n",
      "1186 : loss:  0.91002256 \t acc:  0.53\n",
      "1188 : loss:  0.8962348 \t acc:  0.54\n",
      "1190 : loss:  0.7877385 \t acc:  0.68\n",
      "1192 : loss:  0.8550556 \t acc:  0.57\n",
      "1194 : loss:  0.8925305 \t acc:  0.56\n",
      "1196 : loss:  0.899796 \t acc:  0.54\n",
      "1198 : loss:  0.8261854 \t acc:  0.62\n",
      "1200 : loss:  0.8441208 \t acc:  0.61\n",
      "1202 : loss:  0.887464 \t acc:  0.57\n",
      "1204 : loss:  0.82086515 \t acc:  0.6\n",
      "1206 : loss:  0.83347815 \t acc:  0.59\n",
      "1208 : loss:  0.82658523 \t acc:  0.58\n",
      "1210 : loss:  0.861217 \t acc:  0.62\n",
      "1212 : loss:  0.930317 \t acc:  0.56\n",
      "1214 : loss:  0.87230635 \t acc:  0.58\n",
      "1216 : loss:  0.8289042 \t acc:  0.62\n",
      "1218 : loss:  0.8556808 \t acc:  0.57\n",
      "1220 : loss:  0.7765435 \t acc:  0.6\n",
      "1222 : loss:  0.8234465 \t acc:  0.61\n",
      "1224 : loss:  0.84409404 \t acc:  0.61\n",
      "1226 : loss:  0.8971184 \t acc:  0.54\n",
      "1228 : loss:  0.78044355 \t acc:  0.65\n",
      "1230 : loss:  0.9359386 \t acc:  0.61\n",
      "1232 : loss:  0.8517485 \t acc:  0.62\n",
      "1234 : loss:  0.88980585 \t acc:  0.58\n",
      "1236 : loss:  0.8303381 \t acc:  0.65\n",
      "1238 : loss:  0.86994624 \t acc:  0.63\n",
      "1240 : loss:  0.8296913 \t acc:  0.65\n",
      "1242 : loss:  0.82937074 \t acc:  0.58\n",
      "1244 : loss:  0.8189325 \t acc:  0.62\n",
      "1246 : loss:  0.9181862 \t acc:  0.63\n",
      "1248 : loss:  0.80435646 \t acc:  0.6\n",
      "1250 : loss:  0.82301736 \t acc:  0.63\n",
      "1252 : loss:  0.7821901 \t acc:  0.63\n",
      "1254 : loss:  0.8531993 \t acc:  0.55\n",
      "1256 : loss:  0.92629755 \t acc:  0.56\n",
      "1258 : loss:  0.9696337 \t acc:  0.55\n",
      "1260 : loss:  0.89325505 \t acc:  0.58\n",
      "1262 : loss:  0.76729333 \t acc:  0.65\n",
      "1264 : loss:  0.7899128 \t acc:  0.66\n",
      "1266 : loss:  0.8208709 \t acc:  0.64\n",
      "1268 : loss:  0.78811544 \t acc:  0.7\n",
      "1270 : loss:  0.73358107 \t acc:  0.67\n",
      "1272 : loss:  0.9378938 \t acc:  0.51\n",
      "1274 : loss:  0.8521494 \t acc:  0.64\n",
      "1276 : loss:  0.7483327 \t acc:  0.69\n",
      "1278 : loss:  0.8497505 \t acc:  0.58\n",
      "1280 : loss:  0.7229715 \t acc:  0.64\n",
      "1282 : loss:  0.8155291 \t acc:  0.62\n",
      "1284 : loss:  0.88239527 \t acc:  0.54\n",
      "1286 : loss:  0.9205933 \t acc:  0.55\n",
      "1288 : loss:  0.84518903 \t acc:  0.58\n",
      "1290 : loss:  0.78418046 \t acc:  0.72\n",
      "1292 : loss:  0.8101231 \t acc:  0.62\n",
      "1294 : loss:  0.78264594 \t acc:  0.62\n",
      "1296 : loss:  0.8964847 \t acc:  0.57\n",
      "1298 : loss:  0.7049202 \t acc:  0.66\n",
      "1300 : loss:  0.8848672 \t acc:  0.6\n",
      "1302 : loss:  0.8649785 \t acc:  0.56\n",
      "1304 : loss:  0.76806283 \t acc:  0.66\n",
      "1306 : loss:  0.8420836 \t acc:  0.6\n",
      "1308 : loss:  0.7519689 \t acc:  0.64\n",
      "1310 : loss:  0.9275279 \t acc:  0.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1312 : loss:  0.8153672 \t acc:  0.58\n",
      "1314 : loss:  0.72757494 \t acc:  0.67\n",
      "1316 : loss:  0.8947283 \t acc:  0.6\n",
      "1318 : loss:  0.736483 \t acc:  0.69\n",
      "1320 : loss:  0.78972346 \t acc:  0.64\n",
      "1322 : loss:  0.8446495 \t acc:  0.61\n",
      "1324 : loss:  0.6945591 \t acc:  0.71\n",
      "1326 : loss:  0.8874154 \t acc:  0.56\n",
      "1328 : loss:  0.7721274 \t acc:  0.67\n",
      "1330 : loss:  0.7245525 \t acc:  0.72\n",
      "1332 : loss:  0.8654003 \t acc:  0.63\n",
      "1334 : loss:  0.845929 \t acc:  0.6\n",
      "1336 : loss:  0.8344159 \t acc:  0.61\n",
      "1338 : loss:  0.8754379 \t acc:  0.56\n",
      "1340 : loss:  0.85618234 \t acc:  0.62\n",
      "1342 : loss:  0.96973395 \t acc:  0.57\n",
      "1344 : loss:  0.75086 \t acc:  0.63\n",
      "1346 : loss:  0.918012 \t acc:  0.58\n",
      "1348 : loss:  0.8637194 \t acc:  0.54\n",
      "1350 : loss:  0.8878288 \t acc:  0.61\n",
      "1352 : loss:  0.7231538 \t acc:  0.74\n",
      "1354 : loss:  0.7544937 \t acc:  0.64\n",
      "1356 : loss:  0.8644749 \t acc:  0.63\n",
      "1358 : loss:  0.76894903 \t acc:  0.67\n",
      "1360 : loss:  0.86320716 \t acc:  0.61\n",
      "1362 : loss:  0.8192018 \t acc:  0.6\n",
      "1364 : loss:  0.819823 \t acc:  0.66\n",
      "1366 : loss:  0.92120177 \t acc:  0.52\n",
      "1368 : loss:  0.71523446 \t acc:  0.72\n",
      "1370 : loss:  0.8376274 \t acc:  0.57\n",
      "1372 : loss:  0.7869892 \t acc:  0.62\n",
      "1374 : loss:  0.8018528 \t acc:  0.61\n",
      "1376 : loss:  0.8384209 \t acc:  0.59\n",
      "1378 : loss:  0.90847045 \t acc:  0.57\n",
      "1380 : loss:  0.7387272 \t acc:  0.63\n",
      "1382 : loss:  0.8779662 \t acc:  0.55\n",
      "1384 : loss:  0.875469 \t acc:  0.6\n",
      "1386 : loss:  0.84649414 \t acc:  0.66\n",
      "1388 : loss:  0.9393544 \t acc:  0.57\n",
      "1390 : loss:  0.93854797 \t acc:  0.53\n",
      "1392 : loss:  0.83235306 \t acc:  0.61\n",
      "1394 : loss:  0.7604057 \t acc:  0.71\n",
      "1396 : loss:  0.9351494 \t acc:  0.55\n",
      "1398 : loss:  0.86345327 \t acc:  0.7\n",
      "1400 : loss:  0.7631527 \t acc:  0.7\n",
      "1402 : loss:  0.8065663 \t acc:  0.57\n",
      "1404 : loss:  0.84562993 \t acc:  0.6\n",
      "1406 : loss:  0.8241209 \t acc:  0.62\n",
      "1408 : loss:  0.8414047 \t acc:  0.62\n",
      "1410 : loss:  0.9393054 \t acc:  0.53\n",
      "1412 : loss:  0.8680245 \t acc:  0.52\n",
      "1414 : loss:  0.8986876 \t acc:  0.53\n",
      "1416 : loss:  0.84591323 \t acc:  0.6\n",
      "1418 : loss:  0.8453304 \t acc:  0.64\n",
      "1420 : loss:  0.7252625 \t acc:  0.7\n",
      "1422 : loss:  0.83030874 \t acc:  0.6\n",
      "1424 : loss:  0.7467002 \t acc:  0.64\n",
      "1426 : loss:  0.72135663 \t acc:  0.65\n",
      "1428 : loss:  0.86193 \t acc:  0.64\n",
      "1430 : loss:  0.84963816 \t acc:  0.59\n",
      "1432 : loss:  0.7228522 \t acc:  0.67\n",
      "1434 : loss:  0.7506826 \t acc:  0.65\n",
      "1436 : loss:  0.87935656 \t acc:  0.6\n",
      "1438 : loss:  0.7394966 \t acc:  0.65\n",
      "1440 : loss:  0.96641195 \t acc:  0.56\n",
      "1442 : loss:  0.867973 \t acc:  0.63\n",
      "1444 : loss:  0.85212845 \t acc:  0.59\n",
      "1446 : loss:  0.8600513 \t acc:  0.61\n",
      "1448 : loss:  0.8493865 \t acc:  0.61\n",
      "1450 : loss:  0.7658032 \t acc:  0.66\n",
      "1452 : loss:  0.9110467 \t acc:  0.56\n",
      "1454 : loss:  0.86926705 \t acc:  0.6\n",
      "1456 : loss:  0.8099431 \t acc:  0.6\n",
      "1458 : loss:  0.89112616 \t acc:  0.56\n",
      "1460 : loss:  0.84549826 \t acc:  0.59\n",
      "1462 : loss:  0.83736694 \t acc:  0.65\n",
      "1464 : loss:  0.8535576 \t acc:  0.57\n",
      "1466 : loss:  0.7330538 \t acc:  0.65\n",
      "1468 : loss:  0.7438676 \t acc:  0.66\n",
      "1470 : loss:  0.84887743 \t acc:  0.55\n",
      "1472 : loss:  0.89378357 \t acc:  0.56\n",
      "1474 : loss:  0.74810165 \t acc:  0.65\n",
      "1476 : loss:  0.86979616 \t acc:  0.59\n",
      "1478 : loss:  0.86525977 \t acc:  0.61\n",
      "1480 : loss:  0.8957355 \t acc:  0.57\n",
      "1482 : loss:  0.7620951 \t acc:  0.65\n",
      "1484 : loss:  0.8212953 \t acc:  0.61\n",
      "1486 : loss:  0.8762533 \t acc:  0.57\n",
      "\n",
      "Saving...\n",
      "saved to models/LSTM/pretrained_model.ckpt-1488\n",
      "\n",
      "100 \t [67.05346096 61.54613993 50.30518864]\n",
      "3 \tval accuracy:  0.60620004 \t f_! score:  [0.67053461 0.6154614  0.50305189]\n",
      "\n",
      "1488 : loss:  0.81554985 \t acc:  0.58\n",
      "1490 : loss:  0.82700175 \t acc:  0.56\n",
      "1492 : loss:  0.8333549 \t acc:  0.57\n",
      "1494 : loss:  0.83091676 \t acc:  0.63\n",
      "1496 : loss:  0.8184438 \t acc:  0.66\n",
      "1498 : loss:  0.9657048 \t acc:  0.49\n",
      "1500 : loss:  0.8469722 \t acc:  0.58\n",
      "1502 : loss:  0.84336686 \t acc:  0.59\n",
      "1504 : loss:  0.835494 \t acc:  0.62\n",
      "1506 : loss:  0.8293879 \t acc:  0.63\n",
      "1508 : loss:  0.83854157 \t acc:  0.64\n",
      "1510 : loss:  0.7546011 \t acc:  0.63\n",
      "1512 : loss:  0.8114898 \t acc:  0.61\n",
      "1514 : loss:  0.76533854 \t acc:  0.68\n",
      "1516 : loss:  0.89312524 \t acc:  0.56\n",
      "1518 : loss:  0.88199615 \t acc:  0.65\n",
      "1520 : loss:  0.82248086 \t acc:  0.61\n",
      "1522 : loss:  0.7956831 \t acc:  0.63\n",
      "1524 : loss:  0.824219 \t acc:  0.64\n",
      "1526 : loss:  0.80012286 \t acc:  0.62\n",
      "1528 : loss:  0.7261707 \t acc:  0.7\n",
      "1530 : loss:  0.87632406 \t acc:  0.56\n",
      "1532 : loss:  0.8226285 \t acc:  0.66\n",
      "1534 : loss:  0.8084812 \t acc:  0.59\n",
      "1536 : loss:  0.7974115 \t acc:  0.66\n",
      "1538 : loss:  0.78692865 \t acc:  0.64\n",
      "1540 : loss:  0.83181936 \t acc:  0.63\n",
      "1542 : loss:  0.75679773 \t acc:  0.68\n",
      "1544 : loss:  0.81332916 \t acc:  0.62\n",
      "1546 : loss:  0.8481884 \t acc:  0.6\n",
      "1548 : loss:  0.76449263 \t acc:  0.61\n",
      "1550 : loss:  0.90334976 \t acc:  0.61\n",
      "1552 : loss:  0.81220895 \t acc:  0.61\n",
      "1554 : loss:  0.93839204 \t acc:  0.47\n",
      "1556 : loss:  0.7624865 \t acc:  0.62\n",
      "1558 : loss:  0.96522325 \t acc:  0.53\n",
      "1560 : loss:  0.8444152 \t acc:  0.6\n",
      "1562 : loss:  0.7654252 \t acc:  0.65\n",
      "1564 : loss:  0.8626522 \t acc:  0.65\n",
      "1566 : loss:  0.79566824 \t acc:  0.65\n",
      "1568 : loss:  0.8319051 \t acc:  0.56\n",
      "1570 : loss:  0.8355117 \t acc:  0.62\n",
      "1572 : loss:  0.73505586 \t acc:  0.61\n",
      "1574 : loss:  0.7806013 \t acc:  0.65\n",
      "1576 : loss:  0.8276783 \t acc:  0.62\n",
      "1578 : loss:  0.7623436 \t acc:  0.63\n",
      "1580 : loss:  0.72945297 \t acc:  0.72\n",
      "1582 : loss:  0.7856266 \t acc:  0.64\n",
      "1584 : loss:  0.83181196 \t acc:  0.6\n",
      "1586 : loss:  0.8590763 \t acc:  0.59\n",
      "1588 : loss:  0.83134305 \t acc:  0.62\n",
      "1590 : loss:  0.73635405 \t acc:  0.68\n",
      "1592 : loss:  0.88581395 \t acc:  0.61\n",
      "1594 : loss:  0.70866585 \t acc:  0.69\n",
      "1596 : loss:  0.9294303 \t acc:  0.56\n",
      "1598 : loss:  0.8219743 \t acc:  0.6\n",
      "1600 : loss:  0.77173495 \t acc:  0.69\n",
      "1602 : loss:  0.86361045 \t acc:  0.53\n",
      "1604 : loss:  0.8229967 \t acc:  0.57\n",
      "1606 : loss:  0.8534933 \t acc:  0.62\n",
      "1608 : loss:  0.8332977 \t acc:  0.62\n",
      "1610 : loss:  0.8659125 \t acc:  0.62\n",
      "1612 : loss:  0.80741286 \t acc:  0.69\n",
      "1614 : loss:  0.8942587 \t acc:  0.57\n",
      "1616 : loss:  0.8457103 \t acc:  0.64\n",
      "1618 : loss:  0.7656725 \t acc:  0.61\n",
      "1620 : loss:  0.8582262 \t acc:  0.63\n",
      "1622 : loss:  0.96072793 \t acc:  0.53\n",
      "1624 : loss:  0.79617834 \t acc:  0.68\n",
      "1626 : loss:  0.7645867 \t acc:  0.67\n",
      "1628 : loss:  0.786586 \t acc:  0.64\n",
      "1630 : loss:  0.77390707 \t acc:  0.69\n",
      "1632 : loss:  0.88451517 \t acc:  0.55\n",
      "1634 : loss:  0.78850096 \t acc:  0.6\n",
      "1636 : loss:  0.8829926 \t acc:  0.61\n",
      "1638 : loss:  0.7502356 \t acc:  0.62\n",
      "1640 : loss:  0.8923738 \t acc:  0.55\n",
      "1642 : loss:  0.813071 \t acc:  0.59\n",
      "1644 : loss:  0.9243119 \t acc:  0.59\n",
      "1646 : loss:  0.7346963 \t acc:  0.69\n",
      "1648 : loss:  0.79220015 \t acc:  0.59\n",
      "1650 : loss:  0.90424097 \t acc:  0.55\n",
      "1652 : loss:  0.74914545 \t acc:  0.64\n",
      "1654 : loss:  0.8081842 \t acc:  0.64\n",
      "1656 : loss:  0.7817344 \t acc:  0.63\n",
      "1658 : loss:  0.70718443 \t acc:  0.7\n",
      "1660 : loss:  0.83829325 \t acc:  0.63\n",
      "1662 : loss:  0.8152334 \t acc:  0.59\n",
      "1664 : loss:  0.7394357 \t acc:  0.72\n",
      "1666 : loss:  0.8319939 \t acc:  0.6\n",
      "1668 : loss:  0.8644186 \t acc:  0.59\n",
      "1670 : loss:  0.7224285 \t acc:  0.69\n",
      "1672 : loss:  0.7473394 \t acc:  0.71\n",
      "1674 : loss:  0.82889557 \t acc:  0.67\n",
      "1676 : loss:  0.8720934 \t acc:  0.53\n",
      "1678 : loss:  0.7354363 \t acc:  0.73\n",
      "1680 : loss:  0.8323993 \t acc:  0.57\n",
      "1682 : loss:  0.7483318 \t acc:  0.66\n",
      "1684 : loss:  0.7732192 \t acc:  0.57\n",
      "1686 : loss:  0.7728722 \t acc:  0.63\n",
      "1688 : loss:  0.69995505 \t acc:  0.69\n",
      "1690 : loss:  0.7217469 \t acc:  0.68\n",
      "1692 : loss:  0.7752227 \t acc:  0.69\n",
      "1694 : loss:  0.8082989 \t acc:  0.63\n",
      "1696 : loss:  0.7740219 \t acc:  0.69\n",
      "1698 : loss:  0.84170115 \t acc:  0.59\n",
      "1700 : loss:  0.79539603 \t acc:  0.61\n",
      "1702 : loss:  0.93371737 \t acc:  0.55\n",
      "1704 : loss:  0.8726237 \t acc:  0.54\n",
      "1706 : loss:  0.73897445 \t acc:  0.65\n",
      "1708 : loss:  0.75922066 \t acc:  0.64\n",
      "1710 : loss:  0.73766786 \t acc:  0.65\n",
      "1712 : loss:  0.83443457 \t acc:  0.63\n",
      "1714 : loss:  0.84836066 \t acc:  0.57\n",
      "1716 : loss:  0.8248745 \t acc:  0.65\n",
      "1718 : loss:  0.7376226 \t acc:  0.65\n",
      "1720 : loss:  0.90604174 \t acc:  0.58\n",
      "1722 : loss:  0.89672667 \t acc:  0.53\n",
      "1724 : loss:  0.84481 \t acc:  0.62\n",
      "1726 : loss:  0.89919806 \t acc:  0.56\n",
      "1728 : loss:  0.8495717 \t acc:  0.6\n",
      "1730 : loss:  0.8790274 \t acc:  0.56\n",
      "1732 : loss:  0.9172841 \t acc:  0.6\n",
      "1734 : loss:  0.7700893 \t acc:  0.63\n",
      "1736 : loss:  0.8350528 \t acc:  0.63\n",
      "1738 : loss:  0.7904619 \t acc:  0.61\n",
      "1740 : loss:  0.88250613 \t acc:  0.54\n",
      "1742 : loss:  0.8516962 \t acc:  0.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1744 : loss:  0.7924287 \t acc:  0.66\n",
      "1746 : loss:  0.74039346 \t acc:  0.66\n",
      "1748 : loss:  0.68944275 \t acc:  0.73\n",
      "1750 : loss:  0.7363442 \t acc:  0.69\n",
      "1752 : loss:  0.82011926 \t acc:  0.6\n",
      "1754 : loss:  0.80640197 \t acc:  0.59\n",
      "1756 : loss:  0.69965255 \t acc:  0.71\n",
      "1758 : loss:  0.8323486 \t acc:  0.57\n",
      "1760 : loss:  0.86555564 \t acc:  0.58\n",
      "1762 : loss:  0.7648331 \t acc:  0.68\n",
      "1764 : loss:  0.8362453 \t acc:  0.59\n",
      "1766 : loss:  0.7340974 \t acc:  0.63\n",
      "1768 : loss:  0.88266975 \t acc:  0.59\n",
      "1770 : loss:  0.79456764 \t acc:  0.67\n",
      "1772 : loss:  0.7499066 \t acc:  0.61\n",
      "1774 : loss:  0.8969925 \t acc:  0.65\n",
      "1776 : loss:  0.88119555 \t acc:  0.61\n",
      "1778 : loss:  0.6980032 \t acc:  0.73\n",
      "1780 : loss:  0.865911 \t acc:  0.58\n",
      "1782 : loss:  0.75935864 \t acc:  0.68\n",
      "1784 : loss:  0.87347066 \t acc:  0.52\n",
      "1786 : loss:  0.81551605 \t acc:  0.63\n",
      "1788 : loss:  0.80126023 \t acc:  0.66\n",
      "1790 : loss:  0.72777665 \t acc:  0.67\n",
      "1792 : loss:  0.8509151 \t acc:  0.51\n",
      "1794 : loss:  0.7602387 \t acc:  0.64\n",
      "1796 : loss:  0.6791873 \t acc:  0.73\n",
      "1798 : loss:  0.77462447 \t acc:  0.61\n",
      "1800 : loss:  0.71185565 \t acc:  0.65\n",
      "1802 : loss:  0.91465133 \t acc:  0.61\n",
      "1804 : loss:  0.8430689 \t acc:  0.6\n",
      "1806 : loss:  0.86899614 \t acc:  0.61\n",
      "1808 : loss:  0.81277114 \t acc:  0.61\n",
      "1810 : loss:  0.7541806 \t acc:  0.66\n",
      "1812 : loss:  0.8085192 \t acc:  0.61\n",
      "1814 : loss:  0.8370631 \t acc:  0.57\n",
      "1816 : loss:  0.752211 \t acc:  0.66\n",
      "1818 : loss:  0.78409 \t acc:  0.64\n",
      "1820 : loss:  0.8436692 \t acc:  0.58\n",
      "1822 : loss:  0.78343236 \t acc:  0.6\n",
      "1824 : loss:  0.90891975 \t acc:  0.55\n",
      "1826 : loss:  0.8706169 \t acc:  0.6\n",
      "1828 : loss:  0.73039955 \t acc:  0.63\n",
      "1830 : loss:  0.6953179 \t acc:  0.65\n",
      "1832 : loss:  0.8123668 \t acc:  0.62\n",
      "1834 : loss:  0.80813295 \t acc:  0.59\n",
      "1836 : loss:  0.8668951 \t acc:  0.62\n",
      "1838 : loss:  0.85805684 \t acc:  0.6\n",
      "1840 : loss:  0.7720214 \t acc:  0.63\n",
      "1842 : loss:  0.767528 \t acc:  0.63\n",
      "1844 : loss:  0.8378851 \t acc:  0.55\n",
      "1846 : loss:  0.72613525 \t acc:  0.61\n",
      "1848 : loss:  0.77056104 \t acc:  0.66\n",
      "1850 : loss:  0.90182585 \t acc:  0.59\n",
      "1852 : loss:  0.83076507 \t acc:  0.65\n",
      "1854 : loss:  0.7954587 \t acc:  0.65\n",
      "1856 : loss:  0.88948077 \t acc:  0.56\n",
      "1858 : loss:  0.88293123 \t acc:  0.59\n",
      "\n",
      "Saving...\n",
      "saved to models/LSTM/pretrained_model.ckpt-1860\n",
      "\n",
      "100 \t [66.98309516 61.78629952 54.45369779]\n",
      "4 \tval accuracy:  0.6131 \t f_! score:  [0.66983095 0.617863   0.54453698]\n",
      "\n",
      "1860 : loss:  0.7242148 \t acc:  0.66\n",
      "1862 : loss:  0.7005073 \t acc:  0.73\n",
      "1864 : loss:  0.79712784 \t acc:  0.66\n",
      "1866 : loss:  0.8317486 \t acc:  0.58\n",
      "1868 : loss:  0.7439858 \t acc:  0.65\n",
      "1870 : loss:  0.8506127 \t acc:  0.69\n",
      "1872 : loss:  0.73045313 \t acc:  0.71\n",
      "1874 : loss:  0.7948308 \t acc:  0.64\n",
      "1876 : loss:  0.72155595 \t acc:  0.63\n",
      "1878 : loss:  0.7836818 \t acc:  0.68\n",
      "1880 : loss:  0.8818169 \t acc:  0.59\n",
      "1882 : loss:  0.7423994 \t acc:  0.65\n",
      "1884 : loss:  0.84854513 \t acc:  0.64\n",
      "1886 : loss:  0.8503477 \t acc:  0.58\n",
      "1888 : loss:  0.82143205 \t acc:  0.62\n",
      "1890 : loss:  0.8456402 \t acc:  0.56\n",
      "1892 : loss:  0.7461686 \t acc:  0.66\n",
      "1894 : loss:  0.82761574 \t acc:  0.64\n",
      "1896 : loss:  0.7506011 \t acc:  0.64\n",
      "1898 : loss:  0.84778535 \t acc:  0.62\n",
      "1900 : loss:  0.72319174 \t acc:  0.69\n",
      "1902 : loss:  0.8230811 \t acc:  0.68\n",
      "1904 : loss:  0.8172276 \t acc:  0.61\n",
      "1906 : loss:  0.8419697 \t acc:  0.64\n",
      "1908 : loss:  0.8820992 \t acc:  0.54\n",
      "1910 : loss:  0.745126 \t acc:  0.62\n",
      "1912 : loss:  0.6928147 \t acc:  0.66\n",
      "1914 : loss:  0.8387163 \t acc:  0.6\n",
      "1916 : loss:  0.76812094 \t acc:  0.65\n",
      "1918 : loss:  0.7452167 \t acc:  0.65\n",
      "1920 : loss:  0.73007476 \t acc:  0.65\n",
      "1922 : loss:  0.72021604 \t acc:  0.66\n",
      "1924 : loss:  0.8516024 \t acc:  0.59\n",
      "1926 : loss:  0.8518141 \t acc:  0.57\n",
      "1928 : loss:  0.74314576 \t acc:  0.71\n",
      "1930 : loss:  0.83213234 \t acc:  0.57\n",
      "1932 : loss:  0.8858338 \t acc:  0.52\n",
      "1934 : loss:  0.78536487 \t acc:  0.66\n",
      "1936 : loss:  0.83279586 \t acc:  0.64\n",
      "1938 : loss:  0.82818514 \t acc:  0.59\n",
      "1940 : loss:  0.8261612 \t acc:  0.64\n",
      "1942 : loss:  0.83705735 \t acc:  0.64\n",
      "1944 : loss:  0.9792375 \t acc:  0.49\n",
      "1946 : loss:  0.8749258 \t acc:  0.58\n",
      "1948 : loss:  0.8329702 \t acc:  0.57\n",
      "1950 : loss:  0.88657206 \t acc:  0.57\n",
      "1952 : loss:  0.8255477 \t acc:  0.6\n",
      "1954 : loss:  0.8518862 \t acc:  0.56\n",
      "1956 : loss:  0.7029659 \t acc:  0.66\n",
      "1958 : loss:  0.7268956 \t acc:  0.67\n",
      "1960 : loss:  0.75312036 \t acc:  0.63\n",
      "1962 : loss:  0.79026383 \t acc:  0.63\n",
      "1964 : loss:  0.8285087 \t acc:  0.6\n",
      "1966 : loss:  0.76717013 \t acc:  0.64\n",
      "1968 : loss:  0.8445642 \t acc:  0.57\n",
      "1970 : loss:  0.89561707 \t acc:  0.59\n",
      "1972 : loss:  0.73750985 \t acc:  0.62\n",
      "1974 : loss:  0.86126304 \t acc:  0.58\n",
      "1976 : loss:  0.7901936 \t acc:  0.64\n",
      "1978 : loss:  0.73760283 \t acc:  0.63\n",
      "1980 : loss:  0.7589125 \t acc:  0.68\n",
      "1982 : loss:  0.8083164 \t acc:  0.63\n",
      "1984 : loss:  0.774366 \t acc:  0.63\n",
      "1986 : loss:  0.75282663 \t acc:  0.62\n",
      "1988 : loss:  0.90887153 \t acc:  0.6\n",
      "1990 : loss:  0.91995347 \t acc:  0.55\n",
      "1992 : loss:  0.8760595 \t acc:  0.65\n",
      "1994 : loss:  0.77020293 \t acc:  0.62\n",
      "1996 : loss:  0.77335376 \t acc:  0.61\n",
      "1998 : loss:  0.8018759 \t acc:  0.64\n",
      "2000 : loss:  0.7312444 \t acc:  0.76666665\n",
      "2002 : loss:  0.9892744 \t acc:  0.48\n",
      "2004 : loss:  0.7768024 \t acc:  0.65\n",
      "2006 : loss:  0.8913601 \t acc:  0.6\n",
      "2008 : loss:  0.8062254 \t acc:  0.6\n",
      "2010 : loss:  0.8267915 \t acc:  0.62\n",
      "2012 : loss:  0.7510433 \t acc:  0.67\n",
      "2014 : loss:  0.7092154 \t acc:  0.68\n",
      "2016 : loss:  0.7684575 \t acc:  0.69\n",
      "2018 : loss:  0.8747727 \t acc:  0.61\n",
      "2020 : loss:  0.76472116 \t acc:  0.63\n",
      "2022 : loss:  0.7943026 \t acc:  0.62\n",
      "2024 : loss:  0.810048 \t acc:  0.68\n",
      "2026 : loss:  0.88218737 \t acc:  0.57\n",
      "2028 : loss:  0.76194936 \t acc:  0.68\n",
      "2030 : loss:  0.8247371 \t acc:  0.65\n",
      "2032 : loss:  0.7909691 \t acc:  0.57\n",
      "2034 : loss:  0.82249296 \t acc:  0.62\n",
      "2036 : loss:  0.83367676 \t acc:  0.62\n",
      "2038 : loss:  0.74323726 \t acc:  0.64\n",
      "2040 : loss:  0.8441119 \t acc:  0.61\n",
      "2042 : loss:  0.78180087 \t acc:  0.63\n",
      "2044 : loss:  0.80706567 \t acc:  0.66\n",
      "2046 : loss:  0.76220214 \t acc:  0.66\n",
      "2048 : loss:  0.86332095 \t acc:  0.56\n",
      "2050 : loss:  0.8995779 \t acc:  0.59\n",
      "2052 : loss:  0.7605342 \t acc:  0.67\n",
      "2054 : loss:  0.8297813 \t acc:  0.63\n",
      "2056 : loss:  0.75608146 \t acc:  0.63\n",
      "2058 : loss:  0.67329705 \t acc:  0.7\n",
      "2060 : loss:  0.87025887 \t acc:  0.65\n",
      "2062 : loss:  0.86256295 \t acc:  0.57\n",
      "2064 : loss:  0.81249315 \t acc:  0.64\n",
      "2066 : loss:  0.7672835 \t acc:  0.68\n",
      "2068 : loss:  0.82686865 \t acc:  0.59\n",
      "2070 : loss:  0.8903225 \t acc:  0.58\n",
      "2072 : loss:  0.87861776 \t acc:  0.6\n",
      "2074 : loss:  0.71459395 \t acc:  0.72\n",
      "2076 : loss:  0.7958039 \t acc:  0.62\n",
      "2078 : loss:  0.7805117 \t acc:  0.66\n",
      "2080 : loss:  0.774068 \t acc:  0.63\n",
      "2082 : loss:  0.86133975 \t acc:  0.6\n",
      "2084 : loss:  0.9099395 \t acc:  0.55\n",
      "2086 : loss:  0.8808322 \t acc:  0.65\n",
      "2088 : loss:  0.6937288 \t acc:  0.66\n",
      "2090 : loss:  0.90805024 \t acc:  0.61\n",
      "2092 : loss:  0.7866525 \t acc:  0.6\n",
      "2094 : loss:  0.72063386 \t acc:  0.6\n",
      "2096 : loss:  0.838155 \t acc:  0.6\n",
      "2098 : loss:  0.7850518 \t acc:  0.69\n",
      "2100 : loss:  0.7887424 \t acc:  0.61\n",
      "2102 : loss:  0.8129122 \t acc:  0.61\n",
      "2104 : loss:  0.909359 \t acc:  0.58\n",
      "2106 : loss:  0.89606667 \t acc:  0.57\n",
      "2108 : loss:  0.8711915 \t acc:  0.57\n",
      "2110 : loss:  0.75905496 \t acc:  0.64\n",
      "2112 : loss:  0.8383628 \t acc:  0.58\n",
      "2114 : loss:  0.8542312 \t acc:  0.62\n",
      "2116 : loss:  0.89764094 \t acc:  0.57\n",
      "2118 : loss:  0.7585229 \t acc:  0.65\n",
      "2120 : loss:  0.9247172 \t acc:  0.54\n",
      "2122 : loss:  0.8920067 \t acc:  0.59\n",
      "2124 : loss:  0.82121855 \t acc:  0.63\n",
      "2126 : loss:  0.8530751 \t acc:  0.6\n",
      "2128 : loss:  0.76255417 \t acc:  0.64\n",
      "2130 : loss:  0.8226277 \t acc:  0.63\n",
      "2132 : loss:  0.82606906 \t acc:  0.6\n",
      "2134 : loss:  0.76856315 \t acc:  0.65\n",
      "2136 : loss:  0.6908646 \t acc:  0.72\n",
      "2138 : loss:  0.82359016 \t acc:  0.59\n",
      "2140 : loss:  0.91767275 \t acc:  0.57\n",
      "2142 : loss:  0.8198518 \t acc:  0.64\n",
      "2144 : loss:  0.8140209 \t acc:  0.65\n",
      "2146 : loss:  0.70238525 \t acc:  0.7\n",
      "2148 : loss:  0.74934363 \t acc:  0.63\n",
      "2150 : loss:  0.71160054 \t acc:  0.69\n",
      "2152 : loss:  0.89044154 \t acc:  0.57\n",
      "2154 : loss:  0.7798077 \t acc:  0.63\n",
      "2156 : loss:  0.7870894 \t acc:  0.6\n",
      "2158 : loss:  0.8118232 \t acc:  0.58\n",
      "2160 : loss:  0.8701679 \t acc:  0.56\n",
      "2162 : loss:  0.7469217 \t acc:  0.69\n",
      "2164 : loss:  0.72891694 \t acc:  0.65\n",
      "2166 : loss:  0.86963105 \t acc:  0.59\n",
      "2168 : loss:  0.82654357 \t acc:  0.65\n",
      "2170 : loss:  0.70739806 \t acc:  0.72\n",
      "2172 : loss:  0.77510625 \t acc:  0.66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2174 : loss:  0.8460426 \t acc:  0.61\n",
      "2176 : loss:  0.8012257 \t acc:  0.62\n",
      "2178 : loss:  0.85226 \t acc:  0.56\n",
      "2180 : loss:  0.8379871 \t acc:  0.58\n",
      "2182 : loss:  0.93659884 \t acc:  0.56\n",
      "2184 : loss:  0.82009244 \t acc:  0.58\n",
      "2186 : loss:  0.87406164 \t acc:  0.54\n",
      "2188 : loss:  0.77773404 \t acc:  0.63\n",
      "2190 : loss:  0.79114944 \t acc:  0.66\n",
      "2192 : loss:  0.90928686 \t acc:  0.58\n",
      "2194 : loss:  0.73003554 \t acc:  0.7\n",
      "2196 : loss:  0.7675537 \t acc:  0.69\n",
      "2198 : loss:  0.8356587 \t acc:  0.58\n",
      "2200 : loss:  0.8220738 \t acc:  0.66\n",
      "2202 : loss:  0.800688 \t acc:  0.6\n",
      "2204 : loss:  0.74517894 \t acc:  0.7\n",
      "2206 : loss:  0.82452536 \t acc:  0.63\n",
      "2208 : loss:  0.8245378 \t acc:  0.65\n",
      "2210 : loss:  0.773668 \t acc:  0.67\n",
      "2212 : loss:  0.8802444 \t acc:  0.55\n",
      "2214 : loss:  0.7861216 \t acc:  0.64\n",
      "2216 : loss:  0.84564817 \t acc:  0.6\n",
      "2218 : loss:  0.88929343 \t acc:  0.56\n",
      "2220 : loss:  0.7700596 \t acc:  0.62\n",
      "2222 : loss:  0.94155806 \t acc:  0.59\n",
      "2224 : loss:  0.78469104 \t acc:  0.65\n",
      "2226 : loss:  0.7109552 \t acc:  0.69\n",
      "2228 : loss:  0.78181547 \t acc:  0.63\n",
      "2230 : loss:  0.8548989 \t acc:  0.59\n",
      "\n",
      "Saving...\n",
      "saved to models/LSTM/pretrained_model.ckpt-2232\n",
      "\n",
      "100 \t [64.84282472 64.14410647 55.17510663]\n",
      "5 \tval accuracy:  0.6171 \t f_! score:  [0.64842825 0.64144106 0.55175107]\n",
      "\n",
      "2232 : loss:  0.85779023 \t acc:  0.63\n",
      "2234 : loss:  0.8355178 \t acc:  0.6\n",
      "2236 : loss:  0.6945589 \t acc:  0.7\n",
      "2238 : loss:  0.9193016 \t acc:  0.58\n",
      "2240 : loss:  0.82607406 \t acc:  0.61\n",
      "2242 : loss:  0.82600325 \t acc:  0.63\n",
      "2244 : loss:  0.6600604 \t acc:  0.7\n",
      "2246 : loss:  0.8465329 \t acc:  0.6\n",
      "2248 : loss:  0.84960455 \t acc:  0.58\n",
      "2250 : loss:  0.86442393 \t acc:  0.58\n",
      "2252 : loss:  0.69498426 \t acc:  0.71\n",
      "2254 : loss:  0.7448777 \t acc:  0.64\n",
      "2256 : loss:  0.84921426 \t acc:  0.63\n",
      "2258 : loss:  0.85426855 \t acc:  0.56\n",
      "2260 : loss:  0.70687896 \t acc:  0.68\n",
      "2262 : loss:  0.66482925 \t acc:  0.65\n",
      "2264 : loss:  0.76730067 \t acc:  0.63\n",
      "2266 : loss:  0.78065526 \t acc:  0.62\n",
      "2268 : loss:  0.7131075 \t acc:  0.67\n",
      "2270 : loss:  0.7997511 \t acc:  0.58\n",
      "2272 : loss:  0.7703263 \t acc:  0.68\n",
      "2274 : loss:  0.87435853 \t acc:  0.61\n",
      "2276 : loss:  0.7233451 \t acc:  0.66\n",
      "2278 : loss:  0.74177915 \t acc:  0.72\n",
      "2280 : loss:  0.8819403 \t acc:  0.57\n",
      "2282 : loss:  0.77026606 \t acc:  0.63\n",
      "2284 : loss:  0.7087106 \t acc:  0.72\n",
      "2286 : loss:  0.8545082 \t acc:  0.62\n",
      "2288 : loss:  0.7207393 \t acc:  0.73333335\n",
      "2290 : loss:  0.63740724 \t acc:  0.73\n",
      "2292 : loss:  0.8102358 \t acc:  0.62\n",
      "2294 : loss:  0.8145918 \t acc:  0.63\n",
      "2296 : loss:  0.7319362 \t acc:  0.72\n",
      "2298 : loss:  0.72126776 \t acc:  0.7\n",
      "2300 : loss:  0.7965733 \t acc:  0.61\n",
      "2302 : loss:  0.894946 \t acc:  0.56\n",
      "2304 : loss:  0.8031208 \t acc:  0.58\n",
      "2306 : loss:  0.8047508 \t acc:  0.59\n",
      "2308 : loss:  0.815709 \t acc:  0.67\n",
      "2310 : loss:  0.8837931 \t acc:  0.58\n",
      "2312 : loss:  0.76284057 \t acc:  0.66\n",
      "2314 : loss:  0.8482357 \t acc:  0.62\n",
      "2316 : loss:  0.6757795 \t acc:  0.71\n",
      "2318 : loss:  0.7237953 \t acc:  0.66\n",
      "2320 : loss:  0.774876 \t acc:  0.62\n",
      "2322 : loss:  0.8136044 \t acc:  0.59\n",
      "2324 : loss:  0.7088192 \t acc:  0.71\n",
      "2326 : loss:  0.770432 \t acc:  0.68\n",
      "2328 : loss:  0.75952333 \t acc:  0.66\n",
      "2330 : loss:  0.74856615 \t acc:  0.63\n",
      "2332 : loss:  0.84740883 \t acc:  0.57\n",
      "2334 : loss:  0.7820658 \t acc:  0.7\n",
      "2336 : loss:  0.81525856 \t acc:  0.59\n",
      "2338 : loss:  0.91332215 \t acc:  0.5\n",
      "2340 : loss:  0.71687907 \t acc:  0.73\n",
      "2342 : loss:  0.835931 \t acc:  0.6\n",
      "2344 : loss:  0.80917627 \t acc:  0.62\n",
      "2346 : loss:  0.7746977 \t acc:  0.62\n",
      "2348 : loss:  0.95465416 \t acc:  0.49\n",
      "2350 : loss:  0.7166802 \t acc:  0.63\n",
      "2352 : loss:  0.83749366 \t acc:  0.57\n",
      "2354 : loss:  0.7413532 \t acc:  0.68\n",
      "2356 : loss:  0.8109911 \t acc:  0.62\n",
      "2358 : loss:  0.78926706 \t acc:  0.66\n",
      "2360 : loss:  0.81025696 \t acc:  0.62\n",
      "2362 : loss:  0.67210066 \t acc:  0.74\n",
      "2364 : loss:  0.83804166 \t acc:  0.61\n",
      "2366 : loss:  0.7656732 \t acc:  0.66\n",
      "2368 : loss:  0.7493084 \t acc:  0.67\n",
      "2370 : loss:  0.80838144 \t acc:  0.61\n",
      "2372 : loss:  0.82214755 \t acc:  0.67\n",
      "2374 : loss:  0.7891853 \t acc:  0.63\n",
      "2376 : loss:  0.72186434 \t acc:  0.7\n",
      "2378 : loss:  0.7515103 \t acc:  0.7\n",
      "2380 : loss:  0.79871917 \t acc:  0.67\n",
      "2382 : loss:  0.7658999 \t acc:  0.63\n",
      "2384 : loss:  0.84603065 \t acc:  0.61\n",
      "2386 : loss:  0.8133137 \t acc:  0.59\n",
      "2388 : loss:  0.6495411 \t acc:  0.72\n",
      "2390 : loss:  0.9544357 \t acc:  0.58\n",
      "2392 : loss:  0.698449 \t acc:  0.69\n",
      "2394 : loss:  0.8551667 \t acc:  0.61\n",
      "2396 : loss:  0.84910375 \t acc:  0.61\n",
      "2398 : loss:  0.81205446 \t acc:  0.64\n",
      "2400 : loss:  0.7127955 \t acc:  0.66\n",
      "2402 : loss:  0.683069 \t acc:  0.67\n",
      "2404 : loss:  0.7689084 \t acc:  0.62\n",
      "2406 : loss:  0.7545999 \t acc:  0.64\n",
      "2408 : loss:  0.72408265 \t acc:  0.67\n",
      "2410 : loss:  0.7397646 \t acc:  0.68\n",
      "2412 : loss:  0.68997335 \t acc:  0.69\n",
      "2414 : loss:  0.89152 \t acc:  0.54\n",
      "2416 : loss:  0.77443475 \t acc:  0.66\n",
      "2418 : loss:  0.8144393 \t acc:  0.62\n",
      "2420 : loss:  0.6765039 \t acc:  0.68\n",
      "2422 : loss:  0.8099253 \t acc:  0.58\n",
      "2424 : loss:  0.82506216 \t acc:  0.58\n",
      "2426 : loss:  0.86916953 \t acc:  0.58\n",
      "2428 : loss:  0.84213966 \t acc:  0.58\n",
      "2430 : loss:  0.83028764 \t acc:  0.61\n",
      "2432 : loss:  0.7142859 \t acc:  0.72\n",
      "2434 : loss:  0.7841965 \t acc:  0.64\n",
      "2436 : loss:  0.7306633 \t acc:  0.69\n",
      "2438 : loss:  0.75693756 \t acc:  0.71\n",
      "2440 : loss:  0.7738565 \t acc:  0.64\n",
      "2442 : loss:  0.8708801 \t acc:  0.63\n",
      "2444 : loss:  0.77899057 \t acc:  0.62\n",
      "2446 : loss:  0.7638707 \t acc:  0.61\n",
      "2448 : loss:  0.8057421 \t acc:  0.64\n",
      "2450 : loss:  0.7619836 \t acc:  0.61\n",
      "2452 : loss:  0.7777568 \t acc:  0.64\n",
      "2454 : loss:  0.7380463 \t acc:  0.63\n",
      "2456 : loss:  0.8302157 \t acc:  0.61\n",
      "2458 : loss:  0.8928735 \t acc:  0.59\n",
      "2460 : loss:  0.77668875 \t acc:  0.62\n",
      "2462 : loss:  0.726593 \t acc:  0.68\n",
      "2464 : loss:  0.78638375 \t acc:  0.61\n",
      "2466 : loss:  0.7456599 \t acc:  0.65\n",
      "2468 : loss:  0.8293525 \t acc:  0.68\n",
      "2470 : loss:  0.8395828 \t acc:  0.61\n",
      "2472 : loss:  0.7575637 \t acc:  0.66\n",
      "2474 : loss:  0.7893306 \t acc:  0.64\n",
      "2476 : loss:  0.8800046 \t acc:  0.57\n",
      "2478 : loss:  0.8461761 \t acc:  0.58\n",
      "2480 : loss:  0.7141603 \t acc:  0.65\n",
      "2482 : loss:  0.8202553 \t acc:  0.61\n",
      "2484 : loss:  0.715813 \t acc:  0.68\n",
      "2486 : loss:  0.7956548 \t acc:  0.59\n",
      "2488 : loss:  0.78931063 \t acc:  0.63\n",
      "2490 : loss:  0.81537664 \t acc:  0.62\n",
      "2492 : loss:  0.77783775 \t acc:  0.64\n",
      "2494 : loss:  0.71185255 \t acc:  0.73\n",
      "2496 : loss:  0.74060434 \t acc:  0.66\n",
      "2498 : loss:  0.8319026 \t acc:  0.61\n",
      "2500 : loss:  0.818378 \t acc:  0.63\n",
      "2502 : loss:  0.78538895 \t acc:  0.59\n",
      "2504 : loss:  0.8636878 \t acc:  0.58\n",
      "2506 : loss:  0.6868999 \t acc:  0.7\n",
      "2508 : loss:  0.82477915 \t acc:  0.69\n",
      "2510 : loss:  0.8966068 \t acc:  0.62\n",
      "2512 : loss:  0.7305754 \t acc:  0.62\n",
      "2514 : loss:  0.80058473 \t acc:  0.6\n",
      "2516 : loss:  0.74125516 \t acc:  0.65\n",
      "2518 : loss:  0.7346549 \t acc:  0.68\n",
      "2520 : loss:  0.7480171 \t acc:  0.63\n",
      "2522 : loss:  0.75255686 \t acc:  0.67\n",
      "2524 : loss:  0.74633324 \t acc:  0.66\n",
      "2526 : loss:  0.81079096 \t acc:  0.63\n",
      "2528 : loss:  0.689805 \t acc:  0.64\n",
      "2530 : loss:  0.7925731 \t acc:  0.63\n",
      "2532 : loss:  0.73782295 \t acc:  0.67\n",
      "2534 : loss:  0.7663953 \t acc:  0.68\n",
      "2536 : loss:  0.8211828 \t acc:  0.65\n",
      "2538 : loss:  0.8380297 \t acc:  0.59\n",
      "2540 : loss:  0.86469215 \t acc:  0.64\n",
      "2542 : loss:  0.7617878 \t acc:  0.66\n",
      "2544 : loss:  0.8541919 \t acc:  0.65\n",
      "2546 : loss:  0.88221985 \t acc:  0.58\n",
      "2548 : loss:  0.7905026 \t acc:  0.71\n",
      "2550 : loss:  0.69929403 \t acc:  0.69\n",
      "2552 : loss:  0.76210696 \t acc:  0.65\n",
      "2554 : loss:  0.6772934 \t acc:  0.7\n",
      "2556 : loss:  0.659383 \t acc:  0.69\n",
      "2558 : loss:  0.8326462 \t acc:  0.69\n",
      "2560 : loss:  0.82611865 \t acc:  0.61\n",
      "2562 : loss:  0.75988066 \t acc:  0.67\n",
      "2564 : loss:  0.83500534 \t acc:  0.6\n",
      "2566 : loss:  0.9277747 \t acc:  0.54\n",
      "2568 : loss:  0.8130641 \t acc:  0.62\n",
      "2570 : loss:  0.7588394 \t acc:  0.64\n",
      "2572 : loss:  0.71286374 \t acc:  0.69\n",
      "2574 : loss:  0.7044008 \t acc:  0.67\n",
      "2576 : loss:  0.6898052 \t acc:  0.72\n",
      "2578 : loss:  0.82055074 \t acc:  0.61\n",
      "2580 : loss:  0.7892581 \t acc:  0.68\n",
      "2582 : loss:  0.8587184 \t acc:  0.6\n",
      "2584 : loss:  0.84134793 \t acc:  0.56\n",
      "2586 : loss:  0.6267564 \t acc:  0.76\n",
      "2588 : loss:  0.7203896 \t acc:  0.73\n",
      "2590 : loss:  0.8924225 \t acc:  0.62\n",
      "2592 : loss:  0.746472 \t acc:  0.63\n",
      "2594 : loss:  0.91729265 \t acc:  0.61\n",
      "2596 : loss:  0.97426057 \t acc:  0.52\n",
      "2598 : loss:  0.6829951 \t acc:  0.69\n",
      "2600 : loss:  0.8819887 \t acc:  0.59\n",
      "2602 : loss:  0.8618393 \t acc:  0.58\n",
      "\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to models/LSTM/pretrained_model.ckpt-2604\n",
      "\n",
      "100 \t [68.77122245 51.07888514 58.64039861]\n",
      "6 \tval accuracy:  0.6035 \t f_! score:  [0.68771222 0.51078885 0.58640399]\n",
      "\n",
      "100 \t [68.77122245 51.07888514 58.64039861]\n",
      "100 \t [66.81278671 67.05997661 52.04138522]\n",
      "100 \t [71.44694476 41.77207971 67.89206648]\n",
      "100 \t [3338. 3333. 3329.]\n",
      "--- Test   Twitter ---\n",
      "0.6035\n",
      "f1:  [0.68771222 0.51078885 0.58640399]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yannik/ba/TestHelper.py:120: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  input_data = self.decide_which_input(input_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509411\n",
      "Vorsicht! es muss vorher ein  LSTM  Netz abgespeichert worden sein\n",
      "\n",
      "Restoring...\n",
      "models/LSTM/pretrained_model.ckpt-2604\n",
      "INFO:tensorflow:Restoring parameters from models/LSTM/pretrained_model.ckpt-2604\n",
      "acc:  0.45\n",
      "acc:  0.32\n",
      "acc:  0.33\n",
      "acc:  0.35\n",
      "acc:  0.45\n",
      "acc:  0.32\n",
      "acc:  0.39\n",
      "acc:  0.35\n",
      "acc:  0.4\n",
      "acc:  0.37\n",
      "acc:  0.38\n",
      "acc:  0.36\n",
      "acc:  0.35\n",
      "acc:  0.38\n",
      "acc:  0.31\n",
      "acc:  0.36\n",
      "acc:  0.36\n",
      "acc:  0.44\n",
      "acc:  0.33\n",
      "acc:  0.41\n",
      "acc:  0.37\n",
      "acc:  0.28\n",
      "acc:  0.41\n",
      "acc:  0.36\n",
      "acc:  0.38\n",
      "acc:  0.4\n",
      "acc:  0.38\n",
      "acc:  0.44\n",
      "acc:  0.32\n",
      "acc:  0.41\n",
      "acc:  0.39\n",
      "acc:  0.39\n",
      "acc:  0.41\n",
      "acc:  0.34\n",
      "acc:  0.27\n",
      "acc:  0.37\n",
      "acc:  0.36\n",
      "acc:  0.38\n",
      "acc:  0.44\n",
      "acc:  0.39\n",
      "acc:  0.43\n",
      "acc:  0.38\n",
      "acc:  0.38\n",
      "acc:  0.35\n",
      "acc:  0.38\n",
      "acc:  0.44\n",
      "acc:  0.36\n",
      "acc:  0.28\n",
      "acc:  0.41\n",
      "acc:  0.36\n",
      "acc:  0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yannik/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.38\n",
      "acc:  0.42\n",
      "acc:  0.45\n",
      "acc:  0.35\n",
      "acc:  0.35\n",
      "acc:  0.35\n",
      "acc:  0.33\n",
      "acc:  0.4\n",
      "acc:  0.39\n",
      "acc:  0.41\n",
      "acc:  0.37\n",
      "acc:  0.5\n",
      "acc:  0.3\n",
      "acc:  0.42\n",
      "acc:  0.39\n",
      "acc:  0.47\n",
      "acc:  0.39\n",
      "acc:  0.29\n",
      "acc:  0.34\n",
      "acc:  0.31\n",
      "acc:  0.38\n",
      "acc:  0.27\n",
      "acc:  0.35\n",
      "acc:  0.5\n",
      "acc:  0.38\n",
      "acc:  0.27\n",
      "acc:  0.38\n",
      "acc:  0.37\n",
      "acc:  0.45\n",
      "acc:  0.38\n",
      "acc:  0.42\n",
      "acc:  0.31\n",
      "acc:  0.36\n",
      "acc:  0.39\n",
      "acc:  0.36\n",
      "acc:  0.3\n",
      "acc:  0.31\n",
      "acc:  0.39\n",
      "acc:  0.41\n",
      "acc:  0.4\n",
      "acc:  0.45\n",
      "acc:  0.41\n",
      "acc:  0.37\n",
      "acc:  0.41\n",
      "acc:  0.37\n",
      "acc:  0.39\n",
      "acc:  0.39\n",
      "acc:  0.39\n",
      "acc:  0.47\n",
      "acc:  0.41\n",
      "acc:  0.42\n",
      "acc:  0.44\n",
      "acc:  0.41\n",
      "acc:  0.4\n",
      "acc:  0.4\n",
      "acc:  0.32\n",
      "acc:  0.36\n",
      "acc:  0.35\n",
      "acc:  0.44\n",
      "acc:  0.45\n",
      "acc:  0.33\n",
      "acc:  0.34\n",
      "acc:  0.37\n",
      "acc:  0.28\n",
      "acc:  0.46\n",
      "acc:  0.4\n",
      "acc:  0.41\n",
      "acc:  0.41\n",
      "acc:  0.27\n",
      "acc:  0.39\n",
      "acc:  0.31\n",
      "acc:  0.35\n",
      "acc:  0.43\n",
      "acc:  0.32\n",
      "acc:  0.42\n",
      "acc:  0.41\n",
      "acc:  0.46\n",
      "acc:  0.44\n",
      "acc:  0.38\n",
      "acc:  0.37\n",
      "acc:  0.4\n",
      "acc:  0.45\n",
      "acc:  0.39\n",
      "acc:  0.34\n",
      "acc:  0.32\n",
      "acc:  0.35\n",
      "acc:  0.42\n",
      "acc:  0.37\n",
      "acc:  0.37\n",
      "acc:  0.35\n",
      "acc:  0.37\n",
      "acc:  0.3\n",
      "acc:  0.3\n",
      "acc:  0.33\n",
      "acc:  0.32\n",
      "acc:  0.39\n",
      "acc:  0.4\n",
      "acc:  0.35\n",
      "acc:  0.34\n",
      "acc:  0.42\n",
      "acc:  0.46\n",
      "acc:  0.42\n",
      "acc:  0.34\n",
      "acc:  0.44\n",
      "acc:  0.46\n",
      "acc:  0.33\n",
      "acc:  0.37\n",
      "acc:  0.45\n",
      "acc:  0.31\n",
      "acc:  0.31\n",
      "acc:  0.39\n",
      "acc:  0.31\n",
      "acc:  0.39\n",
      "acc:  0.39\n",
      "acc:  0.35\n",
      "acc:  0.39\n",
      "acc:  0.44\n",
      "acc:  0.35\n",
      "acc:  0.39\n",
      "acc:  0.43\n",
      "acc:  0.4\n",
      "acc:  0.43\n",
      "acc:  0.38\n",
      "acc:  0.39\n",
      "acc:  0.37\n",
      "acc:  0.45\n",
      "acc:  0.38\n",
      "acc:  0.28\n",
      "acc:  0.3\n",
      "acc:  0.38\n",
      "acc:  0.43\n",
      "acc:  0.42\n",
      "acc:  0.36\n",
      "acc:  0.4\n",
      "acc:  0.4\n",
      "acc:  0.36\n",
      "acc:  0.47\n",
      "acc:  0.37\n",
      "acc:  0.28\n",
      "acc:  0.43\n",
      "acc:  0.34\n",
      "acc:  0.42\n",
      "acc:  0.4\n",
      "acc:  0.42\n",
      "acc:  0.34\n",
      "acc:  0.25\n",
      "acc:  0.35\n",
      "acc:  0.4\n",
      "acc:  0.35\n",
      "acc:  0.44\n",
      "acc:  0.35\n",
      "acc:  0.41\n",
      "acc:  0.42\n",
      "acc:  0.31\n",
      "acc:  0.32\n",
      "acc:  0.39\n",
      "acc:  0.44\n",
      "acc:  0.37\n",
      "acc:  0.39\n",
      "acc:  0.42\n",
      "acc:  0.35\n",
      "acc:  0.42\n",
      "acc:  0.5\n",
      "acc:  0.4\n",
      "acc:  0.29\n",
      "acc:  0.38\n",
      "acc:  0.3\n",
      "acc:  0.47\n",
      "acc:  0.35\n",
      "acc:  0.4\n",
      "acc:  0.24\n",
      "acc:  0.49\n",
      "acc:  0.36\n",
      "acc:  0.33\n",
      "acc:  0.48\n",
      "acc:  0.43\n",
      "acc:  0.37\n",
      "acc:  0.39\n",
      "acc:  0.32\n",
      "acc:  0.39\n",
      "acc:  0.43\n",
      "acc:  0.34\n",
      "acc:  0.41\n",
      "acc:  0.39\n",
      "acc:  0.36\n",
      "acc:  0.41\n",
      "acc:  0.41\n",
      "acc:  0.38\n",
      "acc:  0.33\n",
      "acc:  0.37\n",
      "acc:  0.36\n",
      "acc:  0.36\n",
      "acc:  0.32\n",
      "acc:  0.32\n",
      "acc:  0.47\n",
      "acc:  0.38\n",
      "acc:  0.42\n",
      "acc:  0.45\n",
      "acc:  0.29\n",
      "acc:  0.39\n",
      "acc:  0.34\n",
      "acc:  0.36\n",
      "acc:  0.4\n",
      "acc:  0.44\n",
      "acc:  0.38\n",
      "acc:  0.37\n",
      "acc:  0.41\n",
      "acc:  0.44\n",
      "acc:  0.41\n",
      "acc:  0.3\n",
      "acc:  0.43\n",
      "acc:  0.39\n",
      "acc:  0.35\n",
      "acc:  0.42\n",
      "acc:  0.47\n",
      "acc:  0.41\n",
      "acc:  0.34\n",
      "acc:  0.4\n",
      "acc:  0.42\n",
      "acc:  0.35\n",
      "acc:  0.4\n",
      "acc:  0.36\n",
      "acc:  0.32\n",
      "acc:  0.42\n",
      "acc:  0.38\n",
      "acc:  0.43\n",
      "acc:  0.37\n",
      "acc:  0.37\n",
      "acc:  0.36\n",
      "acc:  0.34\n",
      "acc:  0.44\n",
      "acc:  0.32\n",
      "acc:  0.5\n",
      "acc:  0.38\n",
      "acc:  0.29\n",
      "acc:  0.4\n",
      "acc:  0.34\n",
      "acc:  0.39\n",
      "acc:  0.31\n",
      "acc:  0.45\n",
      "acc:  0.43\n",
      "acc:  0.4\n",
      "acc:  0.42\n",
      "acc:  0.34\n",
      "acc:  0.45\n",
      "acc:  0.39\n",
      "acc:  0.41\n",
      "acc:  0.37\n",
      "acc:  0.41\n",
      "acc:  0.35\n",
      "acc:  0.42\n",
      "acc:  0.37\n",
      "acc:  0.31\n",
      "acc:  0.39\n",
      "acc:  0.44\n",
      "acc:  0.34\n",
      "acc:  0.34\n",
      "acc:  0.46\n",
      "acc:  0.46\n",
      "acc:  0.37\n",
      "acc:  0.34\n",
      "acc:  0.38\n",
      "acc:  0.36\n",
      "acc:  0.38\n",
      "acc:  0.38\n",
      "acc:  0.33\n",
      "acc:  0.41\n",
      "acc:  0.4\n",
      "acc:  0.35\n",
      "acc:  0.28\n",
      "acc:  0.49\n",
      "acc:  0.35\n",
      "acc:  0.46\n",
      "acc:  0.33\n",
      "acc:  0.43\n",
      "acc:  0.4\n",
      "acc:  0.39\n",
      "acc:  0.4\n",
      "acc:  0.4\n",
      "acc:  0.41\n",
      "acc:  0.34\n",
      "acc:  0.43\n",
      "acc:  0.39\n",
      "acc:  0.4\n",
      "acc:  0.39\n",
      "acc:  0.35\n",
      "acc:  0.32\n",
      "acc:  0.42\n",
      "acc:  0.39\n",
      "acc:  0.37\n",
      "acc:  0.39\n",
      "acc:  0.35\n",
      "acc:  0.44\n",
      "acc:  0.38\n",
      "acc:  0.39\n",
      "acc:  0.4\n",
      "acc:  0.41\n",
      "acc:  0.3\n",
      "acc:  0.48\n",
      "acc:  0.48\n",
      "acc:  0.33\n",
      "acc:  0.35\n",
      "acc:  0.37\n",
      "acc:  0.38\n",
      "acc:  0.36\n",
      "acc:  0.38\n",
      "acc:  0.4\n",
      "acc:  0.37\n",
      "acc:  0.44\n",
      "acc:  0.43\n",
      "acc:  0.38\n",
      "acc:  0.44\n",
      "acc:  0.48\n",
      "acc:  0.28\n",
      "acc:  0.39\n",
      "acc:  0.4\n",
      "acc:  0.38\n",
      "acc:  0.35\n",
      "acc:  0.31\n",
      "acc:  0.39\n",
      "acc:  0.36\n",
      "acc:  0.39\n",
      "acc:  0.27\n",
      "acc:  0.4\n",
      "acc:  0.38\n",
      "acc:  0.31\n",
      "acc:  0.37\n",
      "acc:  0.38\n",
      "acc:  0.42\n",
      "acc:  0.39\n",
      "acc:  0.3\n",
      "acc:  0.38\n",
      "acc:  0.41\n",
      "acc:  0.35\n",
      "acc:  0.42\n",
      "acc:  0.36\n",
      "acc:  0.41\n",
      "acc:  0.44\n",
      "acc:  0.4\n",
      "acc:  0.45\n",
      "acc:  0.42\n",
      "acc:  0.38\n",
      "acc:  0.43\n",
      "acc:  0.4\n",
      "acc:  0.34\n",
      "acc:  0.35\n",
      "acc:  0.38\n",
      "acc:  0.37\n",
      "acc:  0.26\n",
      "acc:  0.26\n",
      "acc:  0.4\n",
      "acc:  0.43\n",
      "acc:  0.39\n",
      "acc:  0.35\n",
      "acc:  0.51\n",
      "acc:  0.42\n",
      "acc:  0.38\n",
      "acc:  0.43\n",
      "acc:  0.3\n",
      "acc:  0.36\n",
      "acc:  0.37\n",
      "acc:  0.5\n",
      "acc:  0.45\n",
      "acc:  0.38\n",
      "acc:  0.35\n",
      "acc:  0.36\n",
      "acc:  0.42\n",
      "acc:  0.34\n",
      "acc:  0.38\n",
      "acc:  0.34\n",
      "acc:  0.45\n",
      "acc:  0.35\n",
      "acc:  0.36\n",
      "acc:  0.33\n",
      "acc:  0.26\n",
      "acc:  0.4\n",
      "acc:  0.35\n",
      "acc:  0.44\n",
      "acc:  0.31\n",
      "acc:  0.31\n",
      "acc:  0.29\n",
      "acc:  0.31\n",
      "acc:  0.37\n",
      "acc:  0.49\n",
      "acc:  0.38\n",
      "acc:  0.37\n",
      "acc:  0.38\n",
      "acc:  0.46\n",
      "acc:  0.47\n",
      "acc:  0.4\n",
      "acc:  0.34\n",
      "acc:  0.31\n",
      "acc:  0.36\n",
      "acc:  0.45\n",
      "acc:  0.34\n",
      "acc:  0.34\n",
      "acc:  0.41\n",
      "acc:  0.38\n",
      "acc:  0.46\n",
      "acc:  0.41\n",
      "acc:  0.36\n",
      "acc:  0.36\n",
      "acc:  0.35\n",
      "acc:  0.3\n",
      "acc:  0.42\n",
      "acc:  0.44\n",
      "acc:  0.33\n",
      "acc:  0.46\n",
      "acc:  0.4\n",
      "acc:  0.4\n",
      "acc:  0.37\n",
      "acc:  0.34\n",
      "acc:  0.42\n",
      "acc:  0.4\n",
      "acc:  0.35\n",
      "acc:  0.38\n",
      "acc:  0.34\n",
      "acc:  0.35\n",
      "acc:  0.49\n",
      "acc:  0.33\n",
      "acc:  0.38\n",
      "acc:  0.34\n",
      "acc:  0.26\n",
      "acc:  0.38\n",
      "acc:  0.44\n",
      "acc:  0.42\n",
      "acc:  0.35\n",
      "acc:  0.37\n",
      "acc:  0.41\n",
      "acc:  0.27\n",
      "acc:  0.37\n",
      "acc:  0.33\n",
      "acc:  0.46\n",
      "acc:  0.29\n",
      "acc:  0.43\n",
      "acc:  0.37\n",
      "acc:  0.34\n",
      "acc:  0.29\n",
      "acc:  0.4\n",
      "acc:  0.42\n",
      "acc:  0.37\n",
      "acc:  0.3\n",
      "acc:  0.37\n",
      "acc:  0.4\n",
      "acc:  0.45\n",
      "acc:  0.41\n",
      "acc:  0.36\n",
      "acc:  0.44\n",
      "acc:  0.36\n",
      "\n",
      "500 \t [250.68952573  36.33026702 154.82917659]\n",
      "val accuracy:  0.38016 \t f_! score:  [0.50137905 0.07266053 0.30965835]\n",
      "\n",
      "500 \t [250.68952573  36.33026702 154.82917659]\n",
      "500 \t [190.57746542 123.28533966 200.76776065]\n",
      "500 \t [370.78575389  22.00928783 128.27843823]\n",
      "500 \t [18794. 13823. 17383.]\n",
      "---just Test  Review ---\n",
      "0.38016\n",
      "f1:  [0.50137905 0.07266053 0.30965835]\n",
      "(7733, 3)\n",
      "7733\n",
      "Vorsicht! es muss vorher ein  LSTM  Netz abgespeichert worden sein\n",
      "\n",
      "Restoring...\n",
      "models/LSTM/pretrained_model.ckpt-2604\n",
      "INFO:tensorflow:Restoring parameters from models/LSTM/pretrained_model.ckpt-2604\n",
      "acc:  0.17\n",
      "acc:  0.14\n",
      "acc:  0.13\n",
      "acc:  0.16\n",
      "acc:  0.27\n",
      "acc:  0.26\n",
      "acc:  0.15\n",
      "acc:  0.14\n",
      "acc:  0.12\n",
      "acc:  0.18\n",
      "acc:  0.17\n",
      "acc:  0.11\n",
      "acc:  0.18\n",
      "acc:  0.22\n",
      "\n",
      "14 \t [1.722513   2.37307957 2.6200177 ]\n",
      "val accuracy:  0.17142855 \t f_! score:  [0.12303664 0.16950568 0.18714412]\n",
      "\n",
      "14 \t [1.722513   2.37307957 2.6200177 ]\n",
      "14 \t [ 1.12419302  1.3861866  12.28278388]\n",
      "14 \t [5.56703297 9.61800699 1.48516713]\n",
      "14 \t [ 105.  116. 1179.]\n",
      "---just Test  Medical ---\n",
      "0.17142855\n",
      "f1:  [0.12303664 0.16950568 0.18714412]\n",
      "(37126,)\n",
      "(37126,)\n",
      "37126\n",
      "Vorsicht! es muss vorher ein  LSTM  Netz abgespeichert worden sein\n",
      "\n",
      "Restoring...\n",
      "models/LSTM/pretrained_model.ckpt-2604\n",
      "INFO:tensorflow:Restoring parameters from models/LSTM/pretrained_model.ckpt-2604\n",
      "acc:  0.82\n",
      "acc:  0.68\n",
      "acc:  0.79\n",
      "acc:  0.76\n",
      "acc:  0.78\n",
      "acc:  0.82\n",
      "acc:  0.75\n",
      "acc:  0.71\n",
      "acc:  0.78\n",
      "acc:  0.84\n",
      "acc:  0.64\n",
      "acc:  0.55\n",
      "acc:  0.69\n",
      "acc:  0.76\n",
      "acc:  0.73\n",
      "acc:  0.51\n",
      "acc:  0.54\n",
      "acc:  0.7\n",
      "acc:  0.63\n",
      "acc:  0.74\n",
      "acc:  0.72\n",
      "acc:  0.78\n",
      "acc:  0.89\n",
      "acc:  0.67\n",
      "acc:  0.78\n",
      "acc:  0.79\n",
      "acc:  0.6\n",
      "acc:  0.85\n",
      "acc:  0.77\n",
      "acc:  0.83\n",
      "acc:  0.64\n",
      "acc:  0.85\n",
      "acc:  0.87\n",
      "acc:  0.77\n",
      "acc:  0.8\n",
      "acc:  0.9\n",
      "acc:  0.67\n",
      "acc:  0.8\n",
      "acc:  0.87\n",
      "acc:  0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yannik/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.73\n",
      "acc:  0.79\n",
      "acc:  0.76\n",
      "acc:  0.61\n",
      "acc:  0.91\n",
      "acc:  0.77\n",
      "acc:  0.93\n",
      "acc:  0.81\n",
      "acc:  0.73\n",
      "acc:  0.77\n",
      "acc:  0.64\n",
      "acc:  0.9\n",
      "acc:  0.77\n",
      "acc:  0.72\n",
      "acc:  0.77\n",
      "acc:  0.71\n",
      "acc:  0.59\n",
      "acc:  0.82\n",
      "acc:  0.73\n",
      "acc:  0.72\n",
      "acc:  0.83\n",
      "acc:  0.69\n",
      "acc:  0.66\n",
      "acc:  0.73\n",
      "acc:  0.68\n",
      "acc:  0.6\n",
      "acc:  0.82\n",
      "acc:  0.6\n",
      "acc:  0.61\n",
      "acc:  0.85\n",
      "\n",
      "70 \t [ 4.77304634  1.95632586 59.94619777]\n",
      "val accuracy:  0.74571425 \t f_! score:  [0.06818638 0.02794751 0.85637425]\n",
      "\n",
      "70 \t [ 4.77304634  1.95632586 59.94619777]\n",
      "70 \t [ 4.7353284   5.075      58.54752177]\n",
      "70 \t [ 6.41247126  1.26526173 61.74455768]\n",
      "70 \t [ 501.  664. 5835.]\n",
      "---just Test  Prime ---\n",
      "0.74571425\n",
      "f1:  [0.06818638 0.02794751 0.85637425]\n"
     ]
    }
   ],
   "source": [
    "tw_loss, tw_acc = testhelper.train_input(\"Twitter\", net_name)\n",
    "\n",
    "testhelper.just_test(\"Review\", net_name)\n",
    "testhelper.just_test(\"Medical\", net_name)\n",
    "testhelper.just_test(\"Prime\", net_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  Medical\n",
      "delete old models\n",
      "(7733, 3)\n",
      "7733\n",
      "Tensor(\"lstm_net/dense/BiasAdd:0\", shape=(?, 3), dtype=float32)\n",
      "Tensor(\"lstm_net_1/dense/BiasAdd:0\", shape=(?, 3), dtype=float32)\n",
      "---init ready   LSTM ---\n",
      "\n",
      "Saving...\n",
      "saved to models/LSTM/pretrained_model.ckpt-0\n",
      "0 : loss:  2.2128773 \t acc:  0.52\n",
      "2 : loss:  0.67792183 \t acc:  0.76\n",
      "4 : loss:  2.9498804 \t acc:  0.02\n",
      "6 : loss:  1.1541477 \t acc:  0.35\n",
      "8 : loss:  1.1715969 \t acc:  0.68\n",
      "10 : loss:  1.472343 \t acc:  0.56\n",
      "12 : loss:  0.5231287 \t acc:  0.88\n",
      "14 : loss:  1.0144219 \t acc:  0.46\n",
      "16 : loss:  1.2480682 \t acc:  0.1\n",
      "18 : loss:  0.8457688 \t acc:  0.71\n",
      "20 : loss:  0.5612693 \t acc:  0.83\n",
      "22 : loss:  1.4191884 \t acc:  0.51\n",
      "24 : loss:  0.8188049 \t acc:  0.7\n",
      "26 : loss:  1.044214 \t acc:  0.55\n",
      "28 : loss:  0.5898422 \t acc:  0.92\n",
      "30 : loss:  0.89040816 \t acc:  0.71\n",
      "32 : loss:  0.71179044 \t acc:  0.79\n",
      "34 : loss:  0.78705335 \t acc:  0.73\n",
      "36 : loss:  1.10481 \t acc:  0.46\n",
      "38 : loss:  0.55550367 \t acc:  0.88\n",
      "40 : loss:  0.75812507 \t acc:  0.74\n",
      "\n",
      "Saving...\n",
      "saved to models/LSTM/pretrained_model.ckpt-42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yannik/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14 \t [ 0.          0.         12.77481666]\n",
      "0 \tval accuracy:  0.84214294 \t f_! score:  [0.        0.        0.9124869]\n",
      "\n",
      "42 : loss:  0.78806984 \t acc:  0.7\n",
      "44 : loss:  0.67370296 \t acc:  0.81\n",
      "46 : loss:  0.6353809 \t acc:  0.85\n",
      "48 : loss:  0.92930436 \t acc:  0.59\n",
      "50 : loss:  0.7011035 \t acc:  0.84\n",
      "52 : loss:  0.9463245 \t acc:  0.6\n",
      "54 : loss:  0.5589864 \t acc:  0.86\n",
      "56 : loss:  0.92105365 \t acc:  0.65\n",
      "58 : loss:  0.64124465 \t acc:  0.83\n",
      "60 : loss:  0.6987888 \t acc:  0.83\n",
      "62 : loss:  1.1756668 \t acc:  0.37\n",
      "64 : loss:  0.6997123 \t acc:  0.76\n",
      "66 : loss:  0.9669933 \t acc:  0.61\n",
      "68 : loss:  0.880849 \t acc:  0.65\n",
      "70 : loss:  0.6497176 \t acc:  0.8\n",
      "72 : loss:  1.1638417 \t acc:  0.51\n",
      "74 : loss:  0.85562456 \t acc:  0.64\n",
      "76 : loss:  0.8482833 \t acc:  0.69\n",
      "78 : loss:  0.5595473 \t acc:  0.92\n",
      "80 : loss:  0.8471164 \t acc:  0.68\n",
      "82 : loss:  1.1373085 \t acc:  0.38\n",
      "\n",
      "Saving...\n",
      "saved to models/LSTM/pretrained_model.ckpt-84\n",
      "\n",
      "14 \t [ 0.          0.         12.76947507]\n",
      "1 \tval accuracy:  0.84142864 \t f_! score:  [0.         0.         0.91210536]\n",
      "\n",
      "84 : loss:  0.75583893 \t acc:  0.7\n",
      "86 : loss:  1.0832155 \t acc:  0.38\n",
      "88 : loss:  0.84370065 \t acc:  0.64\n",
      "90 : loss:  0.8564794 \t acc:  0.64\n",
      "92 : loss:  1.1457118 \t acc:  0.46\n",
      "94 : loss:  0.742243 \t acc:  0.71\n",
      "96 : loss:  0.5390719 \t acc:  0.88\n",
      "98 : loss:  1.0401866 \t acc:  0.5\n",
      "100 : loss:  0.97261083 \t acc:  0.48\n",
      "102 : loss:  0.82909495 \t acc:  0.74\n",
      "104 : loss:  0.82857096 \t acc:  0.75\n",
      "106 : loss:  0.87753385 \t acc:  0.61\n",
      "108 : loss:  0.91803247 \t acc:  0.57\n",
      "110 : loss:  0.81623155 \t acc:  0.7\n",
      "112 : loss:  0.882576 \t acc:  0.66\n",
      "114 : loss:  1.0672176 \t acc:  0.55\n",
      "116 : loss:  0.49099272 \t acc:  0.88\n",
      "118 : loss:  0.67015386 \t acc:  0.79\n",
      "120 : loss:  0.80532 \t acc:  0.71\n",
      "122 : loss:  0.86295366 \t acc:  0.69\n",
      "124 : loss:  0.7122516 \t acc:  0.77\n",
      "\n",
      "Saving...\n",
      "saved to models/LSTM/pretrained_model.ckpt-126\n",
      "\n",
      "14 \t [ 0.          0.         12.77481666]\n",
      "2 \tval accuracy:  0.84214294 \t f_! score:  [0.        0.        0.9124869]\n",
      "\n",
      "126 : loss:  0.92879105 \t acc:  0.58\n",
      "128 : loss:  0.7323077 \t acc:  0.72\n",
      "130 : loss:  0.9991076 \t acc:  0.57\n",
      "132 : loss:  0.9135326 \t acc:  0.52\n",
      "134 : loss:  0.8704487 \t acc:  0.61\n",
      "136 : loss:  0.62401086 \t acc:  0.88\n",
      "138 : loss:  0.64333224 \t acc:  0.81\n",
      "140 : loss:  0.79712594 \t acc:  0.77\n",
      "142 : loss:  1.0181549 \t acc:  0.54\n",
      "144 : loss:  0.8987073 \t acc:  0.59\n",
      "146 : loss:  0.6320381 \t acc:  0.75757575\n",
      "148 : loss:  0.48430473 \t acc:  0.85\n",
      "150 : loss:  0.613901 \t acc:  0.88\n",
      "152 : loss:  0.8252241 \t acc:  0.65\n",
      "154 : loss:  0.89867157 \t acc:  0.59\n",
      "156 : loss:  0.8842871 \t acc:  0.65\n",
      "158 : loss:  0.7260619 \t acc:  0.81\n",
      "160 : loss:  0.7037854 \t acc:  0.74\n",
      "162 : loss:  0.8682731 \t acc:  0.69\n",
      "164 : loss:  0.93035936 \t acc:  0.64\n",
      "166 : loss:  0.85426795 \t acc:  0.7\n",
      "\n",
      "Saving...\n",
      "saved to models/LSTM/pretrained_model.ckpt-168\n",
      "\n",
      "14 \t [ 0.          0.         12.77481666]\n",
      "3 \tval accuracy:  0.84214294 \t f_! score:  [0.        0.        0.9124869]\n",
      "\n",
      "168 : loss:  0.5455225 \t acc:  0.83\n",
      "170 : loss:  0.69063604 \t acc:  0.76\n",
      "172 : loss:  0.83830225 \t acc:  0.73\n",
      "174 : loss:  0.9577526 \t acc:  0.6\n",
      "176 : loss:  0.83959854 \t acc:  0.71\n",
      "178 : loss:  0.687054 \t acc:  0.83\n",
      "180 : loss:  0.81337565 \t acc:  0.69\n",
      "182 : loss:  0.61779344 \t acc:  0.81\n",
      "184 : loss:  0.49840268 \t acc:  0.86\n",
      "186 : loss:  0.7719626 \t acc:  0.69\n",
      "188 : loss:  0.6772106 \t acc:  0.76\n",
      "190 : loss:  0.784992 \t acc:  0.68\n",
      "192 : loss:  0.8831489 \t acc:  0.63\n",
      "194 : loss:  0.6913829 \t acc:  0.83\n",
      "196 : loss:  0.86752045 \t acc:  0.63\n",
      "198 : loss:  0.88291466 \t acc:  0.64\n",
      "200 : loss:  0.7033149 \t acc:  0.81\n",
      "202 : loss:  1.1345618 \t acc:  0.52\n",
      "204 : loss:  0.68950593 \t acc:  0.76\n",
      "206 : loss:  1.3324236 \t acc:  0.46\n",
      "208 : loss:  0.7362669 \t acc:  0.74\n",
      "\n",
      "Saving...\n",
      "saved to models/LSTM/pretrained_model.ckpt-210\n",
      "\n",
      "14 \t [ 0.          0.         12.76894122]\n",
      "4 \tval accuracy:  0.84142864 \t f_! score:  [0.         0.         0.91206723]\n",
      "\n",
      "210 : loss:  0.8090262 \t acc:  0.64\n",
      "212 : loss:  0.7245571 \t acc:  0.7\n",
      "214 : loss:  0.79932326 \t acc:  0.7\n",
      "216 : loss:  1.0071434 \t acc:  0.5\n",
      "218 : loss:  0.9205055 \t acc:  0.53\n",
      "220 : loss:  1.0348687 \t acc:  0.53\n",
      "222 : loss:  0.92109245 \t acc:  0.56\n",
      "224 : loss:  0.6924788 \t acc:  0.76\n",
      "226 : loss:  0.86914617 \t acc:  0.62\n",
      "228 : loss:  1.0007206 \t acc:  0.5\n",
      "230 : loss:  0.5994097 \t acc:  0.72\n",
      "232 : loss:  0.8656148 \t acc:  0.54\n",
      "234 : loss:  0.8014943 \t acc:  0.64\n",
      "236 : loss:  0.6583299 \t acc:  0.81\n",
      "238 : loss:  0.6850914 \t acc:  0.76\n",
      "240 : loss:  0.6009622 \t acc:  0.84\n",
      "242 : loss:  0.5803276 \t acc:  0.88\n",
      "244 : loss:  0.39092132 \t acc:  0.94\n",
      "246 : loss:  0.6731602 \t acc:  0.75\n",
      "248 : loss:  0.8047898 \t acc:  0.7\n",
      "250 : loss:  0.6561048 \t acc:  0.79\n",
      "\n",
      "Saving...\n",
      "saved to models/LSTM/pretrained_model.ckpt-252\n",
      "\n",
      "14 \t [ 0.59722222  0.         12.66289767]\n",
      "5 \tval accuracy:  0.8271429 \t f_! score:  [0.04265873 0.         0.90449269]\n",
      "\n",
      "252 : loss:  0.7774991 \t acc:  0.73\n",
      "254 : loss:  0.9336371 \t acc:  0.58\n",
      "256 : loss:  0.70490766 \t acc:  0.77\n",
      "258 : loss:  0.6547146 \t acc:  0.83\n",
      "260 : loss:  0.6703341 \t acc:  0.74\n",
      "262 : loss:  0.85743856 \t acc:  0.63\n",
      "264 : loss:  0.7286878 \t acc:  0.71\n",
      "266 : loss:  1.1084312 \t acc:  0.53\n",
      "268 : loss:  0.7403101 \t acc:  0.65\n",
      "270 : loss:  0.49888724 \t acc:  0.86\n",
      "272 : loss:  0.81535286 \t acc:  0.68\n",
      "274 : loss:  0.80463326 \t acc:  0.65\n",
      "276 : loss:  0.58942044 \t acc:  0.8\n",
      "278 : loss:  0.97727394 \t acc:  0.44\n",
      "280 : loss:  0.64187247 \t acc:  0.81\n",
      "282 : loss:  0.47405064 \t acc:  0.87\n",
      "284 : loss:  0.875391 \t acc:  0.58\n",
      "286 : loss:  0.74431616 \t acc:  0.66\n",
      "288 : loss:  0.79429644 \t acc:  0.69\n",
      "290 : loss:  0.6793008 \t acc:  0.7\n",
      "292 : loss:  0.5882078 \t acc:  0.8181818\n",
      "\n",
      "Saving...\n",
      "saved to models/LSTM/pretrained_model.ckpt-294\n",
      "\n",
      "14 \t [ 0.45        0.15384615 12.65823837]\n",
      "6 \tval accuracy:  0.82714283 \t f_! score:  [0.03214286 0.01098901 0.90415988]\n",
      "\n",
      "14 \t [ 0.45        0.15384615 12.65823837]\n",
      "14 \t [ 0.9         1.         11.80462488]\n",
      "14 \t [ 0.3         0.08333333 13.69853054]\n",
      "14 \t [ 105.  116. 1179.]\n",
      "--- Test   Medical ---\n",
      "0.82714283\n",
      "f1:  [0.03214286 0.01098901 0.90415988]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yannik/ba/TestHelper.py:120: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  input_data = self.decide_which_input(input_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509411\n",
      "Vorsicht! es muss vorher ein  LSTM  Netz abgespeichert worden sein\n",
      "\n",
      "Restoring...\n",
      "models/LSTM/pretrained_model.ckpt-294\n",
      "INFO:tensorflow:Restoring parameters from models/LSTM/pretrained_model.ckpt-294\n",
      "acc:  0.31\n",
      "acc:  0.33\n",
      "acc:  0.35\n",
      "acc:  0.34\n",
      "acc:  0.29\n",
      "acc:  0.3\n",
      "acc:  0.25\n",
      "acc:  0.33\n",
      "acc:  0.35\n",
      "acc:  0.33\n",
      "acc:  0.32\n",
      "acc:  0.26\n",
      "acc:  0.32\n",
      "acc:  0.4\n",
      "acc:  0.35\n",
      "acc:  0.39\n",
      "acc:  0.32\n",
      "acc:  0.37\n",
      "acc:  0.37\n",
      "acc:  0.29\n",
      "acc:  0.37\n",
      "acc:  0.37\n",
      "acc:  0.29\n",
      "acc:  0.35\n",
      "acc:  0.26\n",
      "acc:  0.35\n",
      "acc:  0.27\n",
      "acc:  0.4\n",
      "acc:  0.39\n",
      "acc:  0.28\n",
      "acc:  0.38\n",
      "acc:  0.34\n",
      "acc:  0.35\n",
      "acc:  0.34\n",
      "acc:  0.26\n",
      "acc:  0.36\n",
      "acc:  0.31\n",
      "acc:  0.39\n",
      "acc:  0.29\n",
      "acc:  0.33\n",
      "acc:  0.3\n",
      "acc:  0.31\n",
      "acc:  0.3\n",
      "acc:  0.34\n",
      "acc:  0.31\n",
      "acc:  0.26\n",
      "acc:  0.31\n",
      "acc:  0.31\n",
      "acc:  0.28\n",
      "acc:  0.3\n",
      "acc:  0.34\n",
      "acc:  0.43\n",
      "acc:  0.31\n",
      "acc:  0.37\n",
      "acc:  0.28\n",
      "acc:  0.34\n",
      "acc:  0.29\n",
      "acc:  0.28\n",
      "acc:  0.4\n",
      "acc:  0.26\n",
      "acc:  0.28\n",
      "acc:  0.36\n",
      "acc:  0.38\n",
      "acc:  0.34\n",
      "acc:  0.34\n",
      "acc:  0.4\n",
      "acc:  0.36\n",
      "acc:  0.29\n",
      "acc:  0.36\n",
      "acc:  0.38\n",
      "acc:  0.3\n",
      "acc:  0.33\n",
      "acc:  0.35\n",
      "acc:  0.39\n",
      "acc:  0.37\n",
      "acc:  0.36\n",
      "acc:  0.37\n",
      "acc:  0.25\n",
      "acc:  0.36\n",
      "acc:  0.27\n",
      "acc:  0.39\n",
      "acc:  0.3\n",
      "acc:  0.3\n",
      "acc:  0.28\n",
      "acc:  0.38\n",
      "acc:  0.35\n",
      "acc:  0.28\n",
      "acc:  0.31\n",
      "acc:  0.3\n",
      "acc:  0.37\n",
      "acc:  0.31\n",
      "acc:  0.27\n",
      "acc:  0.31\n",
      "acc:  0.33\n",
      "acc:  0.35\n",
      "acc:  0.28\n",
      "acc:  0.3\n",
      "acc:  0.36\n",
      "acc:  0.27\n",
      "acc:  0.29\n",
      "acc:  0.31\n",
      "acc:  0.31\n",
      "acc:  0.32\n",
      "acc:  0.28\n",
      "acc:  0.37\n",
      "acc:  0.34\n",
      "acc:  0.32\n",
      "acc:  0.23\n",
      "acc:  0.34\n",
      "acc:  0.44\n",
      "acc:  0.37\n",
      "acc:  0.38\n",
      "acc:  0.35\n",
      "acc:  0.3\n",
      "acc:  0.31\n",
      "acc:  0.39\n",
      "acc:  0.28\n",
      "acc:  0.36\n",
      "acc:  0.29\n",
      "acc:  0.34\n",
      "acc:  0.27\n",
      "acc:  0.34\n",
      "acc:  0.34\n",
      "acc:  0.36\n",
      "acc:  0.32\n",
      "acc:  0.34\n",
      "acc:  0.36\n",
      "acc:  0.33\n",
      "acc:  0.39\n",
      "acc:  0.3\n",
      "acc:  0.29\n",
      "acc:  0.41\n",
      "acc:  0.33\n",
      "acc:  0.35\n",
      "acc:  0.39\n",
      "acc:  0.39\n",
      "acc:  0.32\n",
      "acc:  0.24\n",
      "acc:  0.36\n",
      "acc:  0.42\n",
      "acc:  0.38\n",
      "acc:  0.37\n",
      "acc:  0.23\n",
      "acc:  0.35\n",
      "acc:  0.29\n",
      "acc:  0.38\n",
      "acc:  0.33\n",
      "acc:  0.32\n",
      "acc:  0.37\n",
      "acc:  0.35\n",
      "acc:  0.34\n",
      "acc:  0.34\n",
      "acc:  0.35\n",
      "acc:  0.37\n",
      "acc:  0.3\n",
      "acc:  0.34\n",
      "acc:  0.3\n",
      "acc:  0.34\n",
      "acc:  0.38\n",
      "acc:  0.27\n",
      "acc:  0.32\n",
      "acc:  0.41\n",
      "acc:  0.36\n",
      "acc:  0.29\n",
      "acc:  0.38\n",
      "acc:  0.26\n",
      "acc:  0.42\n",
      "acc:  0.3\n",
      "acc:  0.25\n",
      "acc:  0.34\n",
      "acc:  0.34\n",
      "acc:  0.36\n",
      "acc:  0.42\n",
      "acc:  0.33\n",
      "acc:  0.35\n",
      "acc:  0.32\n",
      "acc:  0.31\n",
      "acc:  0.28\n",
      "acc:  0.22\n",
      "acc:  0.29\n",
      "acc:  0.38\n",
      "acc:  0.32\n",
      "acc:  0.32\n",
      "acc:  0.32\n",
      "acc:  0.39\n",
      "acc:  0.39\n",
      "acc:  0.33\n",
      "acc:  0.3\n",
      "acc:  0.38\n",
      "acc:  0.28\n",
      "acc:  0.32\n",
      "acc:  0.34\n",
      "acc:  0.32\n",
      "acc:  0.33\n",
      "acc:  0.33\n",
      "acc:  0.28\n",
      "acc:  0.37\n",
      "acc:  0.35\n",
      "acc:  0.37\n",
      "acc:  0.38\n",
      "acc:  0.35\n",
      "acc:  0.35\n",
      "acc:  0.31\n",
      "acc:  0.38\n",
      "acc:  0.36\n",
      "acc:  0.33\n",
      "acc:  0.35\n",
      "acc:  0.32\n",
      "acc:  0.31\n",
      "acc:  0.19\n",
      "acc:  0.31\n",
      "acc:  0.23\n",
      "acc:  0.21\n",
      "acc:  0.25\n",
      "acc:  0.3\n",
      "acc:  0.32\n",
      "acc:  0.28\n",
      "acc:  0.31\n",
      "acc:  0.34\n",
      "acc:  0.25\n",
      "acc:  0.36\n",
      "acc:  0.33\n",
      "acc:  0.35\n",
      "acc:  0.32\n",
      "acc:  0.35\n",
      "acc:  0.38\n",
      "acc:  0.31\n",
      "acc:  0.41\n",
      "acc:  0.35\n",
      "acc:  0.38\n",
      "acc:  0.29\n",
      "acc:  0.27\n",
      "acc:  0.41\n",
      "acc:  0.32\n",
      "acc:  0.35\n",
      "acc:  0.34\n",
      "acc:  0.35\n",
      "acc:  0.32\n",
      "acc:  0.38\n",
      "acc:  0.39\n",
      "acc:  0.34\n",
      "acc:  0.35\n",
      "acc:  0.3\n",
      "acc:  0.41\n",
      "acc:  0.29\n",
      "acc:  0.33\n",
      "acc:  0.4\n",
      "acc:  0.35\n",
      "acc:  0.4\n",
      "acc:  0.33\n",
      "acc:  0.25\n",
      "acc:  0.28\n",
      "acc:  0.4\n",
      "acc:  0.35\n",
      "acc:  0.36\n",
      "acc:  0.29\n",
      "acc:  0.28\n",
      "acc:  0.32\n",
      "acc:  0.38\n",
      "acc:  0.24\n",
      "acc:  0.28\n",
      "acc:  0.38\n",
      "acc:  0.37\n",
      "acc:  0.37\n",
      "acc:  0.26\n",
      "acc:  0.35\n",
      "acc:  0.36\n",
      "acc:  0.3\n",
      "acc:  0.36\n",
      "acc:  0.32\n",
      "acc:  0.26\n",
      "acc:  0.35\n",
      "acc:  0.32\n",
      "acc:  0.29\n",
      "acc:  0.34\n",
      "acc:  0.36\n",
      "acc:  0.31\n",
      "acc:  0.37\n",
      "acc:  0.23\n",
      "acc:  0.34\n",
      "acc:  0.26\n",
      "acc:  0.36\n",
      "acc:  0.41\n",
      "acc:  0.34\n",
      "acc:  0.34\n",
      "acc:  0.3\n",
      "acc:  0.4\n",
      "acc:  0.31\n",
      "acc:  0.31\n",
      "acc:  0.28\n",
      "acc:  0.33\n",
      "acc:  0.32\n",
      "acc:  0.3\n",
      "acc:  0.31\n",
      "acc:  0.28\n",
      "acc:  0.32\n",
      "acc:  0.28\n",
      "acc:  0.39\n",
      "acc:  0.33\n",
      "acc:  0.28\n",
      "acc:  0.37\n",
      "acc:  0.29\n",
      "acc:  0.29\n",
      "acc:  0.32\n",
      "acc:  0.36\n",
      "acc:  0.32\n",
      "acc:  0.39\n",
      "acc:  0.37\n",
      "acc:  0.29\n",
      "acc:  0.35\n",
      "acc:  0.42\n",
      "acc:  0.3\n",
      "acc:  0.31\n",
      "acc:  0.28\n",
      "acc:  0.38\n",
      "acc:  0.36\n",
      "acc:  0.32\n",
      "acc:  0.36\n",
      "acc:  0.3\n",
      "acc:  0.3\n",
      "acc:  0.4\n",
      "acc:  0.34\n",
      "acc:  0.22\n",
      "acc:  0.36\n",
      "acc:  0.37\n",
      "acc:  0.31\n",
      "acc:  0.28\n",
      "acc:  0.35\n",
      "acc:  0.26\n",
      "acc:  0.27\n",
      "acc:  0.36\n",
      "acc:  0.26\n",
      "acc:  0.38\n",
      "acc:  0.36\n",
      "acc:  0.29\n",
      "acc:  0.43\n",
      "acc:  0.34\n",
      "acc:  0.37\n",
      "acc:  0.29\n",
      "acc:  0.31\n",
      "acc:  0.29\n",
      "acc:  0.34\n",
      "acc:  0.37\n",
      "acc:  0.3\n",
      "acc:  0.29\n",
      "acc:  0.4\n",
      "acc:  0.35\n",
      "acc:  0.27\n",
      "acc:  0.4\n",
      "acc:  0.37\n",
      "acc:  0.33\n",
      "acc:  0.32\n",
      "acc:  0.31\n",
      "acc:  0.46\n",
      "acc:  0.39\n",
      "acc:  0.33\n",
      "acc:  0.23\n",
      "acc:  0.34\n",
      "acc:  0.37\n",
      "acc:  0.27\n",
      "acc:  0.18\n",
      "acc:  0.33\n",
      "acc:  0.35\n",
      "acc:  0.27\n",
      "acc:  0.37\n",
      "acc:  0.32\n",
      "acc:  0.31\n",
      "acc:  0.38\n",
      "acc:  0.25\n",
      "acc:  0.33\n",
      "acc:  0.36\n",
      "acc:  0.37\n",
      "acc:  0.4\n",
      "acc:  0.39\n",
      "acc:  0.35\n",
      "acc:  0.33\n",
      "acc:  0.36\n",
      "acc:  0.36\n",
      "acc:  0.32\n",
      "acc:  0.28\n",
      "acc:  0.38\n",
      "acc:  0.4\n",
      "acc:  0.38\n",
      "acc:  0.24\n",
      "acc:  0.4\n",
      "acc:  0.29\n",
      "acc:  0.31\n",
      "acc:  0.4\n",
      "acc:  0.37\n",
      "acc:  0.41\n",
      "acc:  0.38\n",
      "acc:  0.38\n",
      "acc:  0.35\n",
      "acc:  0.32\n",
      "acc:  0.28\n",
      "acc:  0.33\n",
      "acc:  0.31\n",
      "acc:  0.31\n",
      "acc:  0.35\n",
      "acc:  0.25\n",
      "acc:  0.35\n",
      "acc:  0.33\n",
      "acc:  0.28\n",
      "acc:  0.35\n",
      "acc:  0.33\n",
      "acc:  0.37\n",
      "acc:  0.39\n",
      "acc:  0.36\n",
      "acc:  0.39\n",
      "acc:  0.41\n",
      "acc:  0.31\n",
      "acc:  0.33\n",
      "acc:  0.26\n",
      "acc:  0.29\n",
      "acc:  0.33\n",
      "acc:  0.27\n",
      "acc:  0.37\n",
      "acc:  0.32\n",
      "acc:  0.35\n",
      "acc:  0.35\n",
      "acc:  0.29\n",
      "acc:  0.26\n",
      "acc:  0.28\n",
      "acc:  0.33\n",
      "acc:  0.34\n",
      "acc:  0.32\n",
      "acc:  0.35\n",
      "acc:  0.33\n",
      "acc:  0.34\n",
      "acc:  0.31\n",
      "acc:  0.32\n",
      "acc:  0.36\n",
      "acc:  0.36\n",
      "acc:  0.26\n",
      "acc:  0.34\n",
      "acc:  0.35\n",
      "acc:  0.33\n",
      "acc:  0.29\n",
      "acc:  0.27\n",
      "acc:  0.3\n",
      "acc:  0.38\n",
      "acc:  0.27\n",
      "acc:  0.4\n",
      "acc:  0.27\n",
      "acc:  0.4\n",
      "acc:  0.36\n",
      "acc:  0.42\n",
      "acc:  0.35\n",
      "acc:  0.32\n",
      "acc:  0.35\n",
      "acc:  0.23\n",
      "acc:  0.34\n",
      "acc:  0.4\n",
      "acc:  0.37\n",
      "acc:  0.31\n",
      "acc:  0.41\n",
      "acc:  0.37\n",
      "acc:  0.32\n",
      "acc:  0.41\n",
      "acc:  0.29\n",
      "acc:  0.42\n",
      "acc:  0.32\n",
      "acc:  0.33\n",
      "acc:  0.33\n",
      "acc:  0.27\n",
      "acc:  0.38\n",
      "acc:  0.37\n",
      "acc:  0.26\n",
      "acc:  0.35\n",
      "acc:  0.35\n",
      "acc:  0.35\n",
      "acc:  0.29\n",
      "acc:  0.35\n",
      "acc:  0.3\n",
      "acc:  0.38\n",
      "acc:  0.34\n",
      "acc:  0.4\n",
      "acc:  0.31\n",
      "acc:  0.41\n",
      "acc:  0.42\n",
      "acc:  0.37\n",
      "acc:  0.33\n",
      "acc:  0.2\n",
      "acc:  0.37\n",
      "acc:  0.3\n",
      "acc:  0.32\n",
      "acc:  0.35\n",
      "acc:  0.26\n",
      "acc:  0.37\n",
      "acc:  0.34\n",
      "acc:  0.36\n",
      "acc:  0.33\n",
      "acc:  0.36\n",
      "acc:  0.28\n",
      "acc:  0.32\n",
      "acc:  0.36\n",
      "acc:  0.28\n",
      "acc:  0.3\n",
      "acc:  0.23\n",
      "acc:  0.39\n",
      "\n",
      "500 \t [204.69475873 183.83875382  38.28017416]\n",
      "val accuracy:  0.33140004 \t f_! score:  [0.40938952 0.36767751 0.07656035]\n",
      "\n",
      "500 \t [204.69475873 183.83875382  38.28017416]\n",
      "500 \t [183.37394805 146.23489008 185.63535354]\n",
      "500 \t [235.1528034  253.18665051  21.85311108]\n",
      "500 \t [18794. 13823. 17383.]\n",
      "---just Test  Review ---\n",
      "0.33140004\n",
      "f1:  [0.40938952 0.36767751 0.07656035]\n",
      "65730\n",
      "Vorsicht! es muss vorher ein  LSTM  Netz abgespeichert worden sein\n",
      "\n",
      "Restoring...\n",
      "models/LSTM/pretrained_model.ckpt-294\n",
      "INFO:tensorflow:Restoring parameters from models/LSTM/pretrained_model.ckpt-294\n",
      "acc:  0.3\n",
      "acc:  0.34\n",
      "acc:  0.31\n",
      "acc:  0.39\n",
      "acc:  0.31\n",
      "acc:  0.26\n",
      "acc:  0.27\n",
      "acc:  0.19\n",
      "acc:  0.35\n",
      "acc:  0.36\n",
      "acc:  0.25\n",
      "acc:  0.39\n",
      "acc:  0.42\n",
      "acc:  0.28\n",
      "acc:  0.32\n",
      "acc:  0.33\n",
      "acc:  0.39\n",
      "acc:  0.37\n",
      "acc:  0.33\n",
      "acc:  0.28\n",
      "acc:  0.27\n",
      "acc:  0.32\n",
      "acc:  0.31\n",
      "acc:  0.34\n",
      "acc:  0.26\n",
      "acc:  0.28\n",
      "acc:  0.29\n",
      "acc:  0.3\n",
      "acc:  0.36\n",
      "acc:  0.27\n",
      "acc:  0.23\n",
      "acc:  0.36\n",
      "acc:  0.26\n",
      "acc:  0.33\n",
      "acc:  0.27\n",
      "acc:  0.3\n",
      "acc:  0.23\n",
      "acc:  0.33\n",
      "acc:  0.17\n",
      "acc:  0.27\n",
      "acc:  0.31\n",
      "acc:  0.34\n",
      "acc:  0.37\n",
      "acc:  0.34\n",
      "acc:  0.42\n",
      "acc:  0.36\n",
      "acc:  0.22\n",
      "acc:  0.3\n",
      "acc:  0.26\n",
      "acc:  0.34\n",
      "acc:  0.36\n",
      "acc:  0.2\n",
      "acc:  0.25\n",
      "acc:  0.35\n",
      "acc:  0.31\n",
      "acc:  0.29\n",
      "acc:  0.37\n",
      "acc:  0.34\n",
      "acc:  0.29\n",
      "acc:  0.41\n",
      "acc:  0.3\n",
      "acc:  0.32\n",
      "acc:  0.3\n",
      "acc:  0.34\n",
      "acc:  0.36\n",
      "acc:  0.32\n",
      "acc:  0.36\n",
      "acc:  0.32\n",
      "acc:  0.34\n",
      "acc:  0.3\n",
      "acc:  0.35\n",
      "acc:  0.38\n",
      "acc:  0.32\n",
      "acc:  0.34\n",
      "acc:  0.29\n",
      "acc:  0.3\n",
      "acc:  0.28\n",
      "acc:  0.34\n",
      "acc:  0.16\n",
      "acc:  0.32\n",
      "acc:  0.27\n",
      "acc:  0.32\n",
      "acc:  0.3\n",
      "acc:  0.25\n",
      "acc:  0.27\n",
      "acc:  0.35\n",
      "acc:  0.34\n",
      "acc:  0.32\n",
      "acc:  0.28\n",
      "acc:  0.41\n",
      "acc:  0.31\n",
      "acc:  0.35\n",
      "acc:  0.25\n",
      "acc:  0.28\n",
      "acc:  0.36\n",
      "acc:  0.36\n",
      "acc:  0.31\n",
      "acc:  0.32\n",
      "acc:  0.36\n",
      "acc:  0.31\n",
      "\n",
      "100 \t [34.55393328 20.39800912 34.77969992]\n",
      "val accuracy:  0.3128 \t f_! score:  [0.34553933 0.20398009 0.347797  ]\n",
      "\n",
      "100 \t [34.55393328 20.39800912 34.77969992]\n",
      "100 \t [30.09904109 29.5719674  33.64053939]\n",
      "100 \t [41.28470989 16.03185351 36.48391989]\n",
      "100 \t [3321. 3337. 3342.]\n",
      "---just Test  Twitter ---\n",
      "0.3128\n",
      "f1:  [0.34553933 0.20398009 0.347797  ]\n",
      "(37126,)\n",
      "(37126,)\n",
      "37126\n",
      "Vorsicht! es muss vorher ein  LSTM  Netz abgespeichert worden sein\n",
      "\n",
      "Restoring...\n",
      "models/LSTM/pretrained_model.ckpt-294\n",
      "INFO:tensorflow:Restoring parameters from models/LSTM/pretrained_model.ckpt-294\n",
      "acc:  0.3\n",
      "acc:  0.34\n",
      "acc:  0.35\n",
      "acc:  0.26\n",
      "acc:  0.3\n",
      "acc:  0.22\n",
      "acc:  0.19\n",
      "acc:  0.2\n",
      "acc:  0.2\n",
      "acc:  0.39\n",
      "acc:  0.27\n",
      "acc:  0.21\n",
      "acc:  0.28\n",
      "acc:  0.16\n",
      "acc:  0.19\n",
      "acc:  0.23\n",
      "acc:  0.38\n",
      "acc:  0.33\n",
      "(2,)\n",
      "acc:  0.28\n",
      "acc:  0.42\n",
      "acc:  0.29\n",
      "acc:  0.38\n",
      "acc:  0.41\n",
      "acc:  0.23\n",
      "acc:  0.19\n",
      "acc:  0.36\n",
      "acc:  0.36\n",
      "acc:  0.23\n",
      "acc:  0.33\n",
      "acc:  0.44\n",
      "acc:  0.34\n",
      "acc:  0.27\n",
      "acc:  0.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.31\n",
      "acc:  0.39\n",
      "acc:  0.19\n",
      "acc:  0.29\n",
      "acc:  0.44\n",
      "acc:  0.24\n",
      "acc:  0.33\n",
      "acc:  0.17\n",
      "acc:  0.37\n",
      "acc:  0.21\n",
      "acc:  0.37\n",
      "acc:  0.25\n",
      "acc:  0.4\n",
      "acc:  0.22\n",
      "acc:  0.37\n",
      "acc:  0.24\n",
      "acc:  0.41\n",
      "acc:  0.39\n",
      "acc:  0.21\n",
      "acc:  0.28\n",
      "acc:  0.29\n",
      "acc:  0.33\n",
      "acc:  0.46\n",
      "acc:  0.25\n",
      "acc:  0.26\n",
      "acc:  0.44\n",
      "acc:  0.28\n",
      "acc:  0.31\n",
      "acc:  0.24\n",
      "acc:  0.37\n",
      "acc:  0.32\n",
      "acc:  0.24\n",
      "acc:  0.36\n",
      "acc:  0.14\n",
      "(2,)\n",
      "acc:  0.29\n",
      "acc:  0.42\n",
      "acc:  0.36\n",
      "\n",
      "68 \t [ 1.97522478  9.6637523  28.70592477]\n",
      "val accuracy:  0.30065218 \t f_! score:  [0.02904742 0.142114   0.42214595]\n",
      "\n",
      "68 \t [ 1.97522478  9.6637523  28.70592477]\n",
      "68 \t [ 6.          5.730513   54.03691514]\n",
      "68 \t [ 1.32777778 41.23359491 20.2558879 ]\n",
      "68 \t [ 501.  657. 5642.]\n",
      "---just Test  Prime ---\n",
      "0.30065218\n",
      "f1:  [0.02904742 0.142114   0.42214595]\n"
     ]
    }
   ],
   "source": [
    "med_loss, med_acc = testhelper.train_input(\"Medical\", net_name)\n",
    "\n",
    "testhelper.just_test(\"Review\", net_name)\n",
    "testhelper.just_test(\"Twitter\", net_name)\n",
    "testhelper.just_test(\"Prime\", net_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  Prime\n",
      "delete old models\n",
      "(37126,)\n",
      "(37126,)\n",
      "37126\n",
      "Tensor(\"lstm_net/dense/BiasAdd:0\", shape=(?, 3), dtype=float32)\n",
      "Tensor(\"lstm_net_1/dense/BiasAdd:0\", shape=(?, 3), dtype=float32)\n",
      "---init ready   LSTM ---\n",
      "\n",
      "Saving...\n",
      "saved to models/LSTM/pretrained_model.ckpt-0\n",
      "0 : loss:  0.77230614 \t acc:  0.85\n",
      "2 : loss:  1.2951229 \t acc:  0.76\n",
      "4 : loss:  0.87416726 \t acc:  0.77\n",
      "6 : loss:  0.91408354 \t acc:  0.64\n",
      "8 : loss:  0.32966363 \t acc:  0.93\n",
      "10 : loss:  0.36691108 \t acc:  0.9\n",
      "12 : loss:  0.997554 \t acc:  0.72\n",
      "14 : loss:  0.62344116 \t acc:  0.84\n",
      "16 : loss:  0.70856094 \t acc:  0.81\n",
      "18 : loss:  0.5796658 \t acc:  0.83\n",
      "20 : loss:  1.0568565 \t acc:  0.61\n",
      "22 : loss:  0.6987372 \t acc:  0.77\n",
      "24 : loss:  0.61215484 \t acc:  0.83\n",
      "26 : loss:  0.4210171 \t acc:  0.88\n",
      "28 : loss:  0.665847 \t acc:  0.75\n",
      "30 : loss:  0.65668833 \t acc:  0.79\n",
      "32 : loss:  0.9677287 \t acc:  0.65\n",
      "34 : loss:  0.6272474 \t acc:  0.8\n",
      "36 : loss:  0.35763967 \t acc:  0.92\n",
      "38 : loss:  0.9990999 \t acc:  0.61\n",
      "40 : loss:  0.5378544 \t acc:  0.82\n",
      "42 : loss:  0.3390837 \t acc:  0.9\n",
      "44 : loss:  0.97452456 \t acc:  0.59\n",
      "46 : loss:  1.0519361 \t acc:  0.59\n",
      "48 : loss:  0.75707334 \t acc:  0.71\n",
      "50 : loss:  0.5346442 \t acc:  0.85\n",
      "52 : loss:  0.82488996 \t acc:  0.66\n",
      "54 : loss:  0.503174 \t acc:  0.85\n",
      "56 : loss:  0.58317 \t acc:  0.81\n",
      "58 : loss:  0.90397364 \t acc:  0.65384614\n",
      "60 : loss:  0.580075 \t acc:  0.83\n",
      "62 : loss:  0.7981647 \t acc:  0.69\n",
      "64 : loss:  0.6251681 \t acc:  0.8\n",
      "66 : loss:  0.83407104 \t acc:  0.65\n",
      "68 : loss:  0.6420165 \t acc:  0.81\n",
      "70 : loss:  0.7604294 \t acc:  0.68\n",
      "72 : loss:  0.9276737 \t acc:  0.65\n",
      "74 : loss:  0.7548107 \t acc:  0.72\n",
      "76 : loss:  1.1450863 \t acc:  0.59\n",
      "78 : loss:  0.66407526 \t acc:  0.78\n",
      "80 : loss:  0.7192726 \t acc:  0.73\n",
      "82 : loss:  0.7547087 \t acc:  0.73\n",
      "84 : loss:  0.5158006 \t acc:  0.88\n",
      "86 : loss:  0.59257436 \t acc:  0.83\n",
      "88 : loss:  0.7365786 \t acc:  0.72\n",
      "90 : loss:  0.7313127 \t acc:  0.74\n",
      "92 : loss:  0.4782911 \t acc:  0.87\n",
      "94 : loss:  0.5574815 \t acc:  0.84\n",
      "96 : loss:  0.35863757 \t acc:  0.91\n",
      "98 : loss:  1.5120807 \t acc:  0.48\n",
      "100 : loss:  0.4192905 \t acc:  0.88\n",
      "102 : loss:  0.6068906 \t acc:  0.8\n",
      "104 : loss:  1.0092256 \t acc:  0.48\n",
      "106 : loss:  0.67874086 \t acc:  0.74\n",
      "108 : loss:  0.36160168 \t acc:  0.9\n",
      "110 : loss:  0.9567315 \t acc:  0.67\n",
      "112 : loss:  0.85376215 \t acc:  0.72\n",
      "114 : loss:  0.42403927 \t acc:  0.88\n",
      "116 : loss:  1.1461763 \t acc:  0.56\n",
      "118 : loss:  0.52485687 \t acc:  0.81\n",
      "120 : loss:  0.5723325 \t acc:  0.79\n",
      "122 : loss:  0.6072104 \t acc:  0.78\n",
      "124 : loss:  0.63294613 \t acc:  0.81\n",
      "126 : loss:  0.64023584 \t acc:  0.78\n",
      "128 : loss:  0.6061306 \t acc:  0.8\n",
      "130 : loss:  0.4429347 \t acc:  0.9\n",
      "132 : loss:  0.58523506 \t acc:  0.79\n",
      "134 : loss:  0.7219389 \t acc:  0.72\n",
      "136 : loss:  0.5989458 \t acc:  0.81\n",
      "138 : loss:  0.43355876 \t acc:  0.91\n",
      "140 : loss:  0.4272157 \t acc:  0.93\n",
      "142 : loss:  0.53005713 \t acc:  0.84\n",
      "144 : loss:  0.66393733 \t acc:  0.77\n",
      "146 : loss:  0.54348093 \t acc:  0.78\n",
      "148 : loss:  1.2164159 \t acc:  0.49\n",
      "150 : loss:  0.6168906 \t acc:  0.8\n",
      "152 : loss:  0.617635 \t acc:  0.77\n",
      "154 : loss:  0.5989131 \t acc:  0.78\n",
      "156 : loss:  0.4824249 \t acc:  0.89\n",
      "158 : loss:  0.5828447 \t acc:  0.79\n",
      "160 : loss:  0.35686222 \t acc:  0.9\n",
      "162 : loss:  0.35727322 \t acc:  0.9\n",
      "164 : loss:  0.5643022 \t acc:  0.83\n",
      "166 : loss:  0.6874942 \t acc:  0.78\n",
      "168 : loss:  0.76744986 \t acc:  0.7\n",
      "170 : loss:  0.51989555 \t acc:  0.84\n",
      "172 : loss:  0.58819026 \t acc:  0.76\n",
      "174 : loss:  0.74598837 \t acc:  0.74\n",
      "176 : loss:  0.32109728 \t acc:  0.92\n",
      "178 : loss:  0.5611667 \t acc:  0.82\n",
      "180 : loss:  1.3956442 \t acc:  0.45\n",
      "182 : loss:  0.55648863 \t acc:  0.83\n",
      "184 : loss:  0.29244477 \t acc:  0.94\n",
      "186 : loss:  0.7750418 \t acc:  0.66\n",
      "188 : loss:  0.67374945 \t acc:  0.75\n",
      "190 : loss:  0.47497025 \t acc:  0.86\n",
      "192 : loss:  0.5697483 \t acc:  0.79\n",
      "194 : loss:  0.35034856 \t acc:  0.91\n",
      "196 : loss:  0.38978383 \t acc:  0.89\n",
      "198 : loss:  0.5502999 \t acc:  0.79\n",
      "200 : loss:  0.759655 \t acc:  0.72\n",
      "\n",
      "Saving...\n",
      "saved to models/LSTM/pretrained_model.ckpt-201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yannik/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(2,)\n",
      "\n",
      "68 \t [ 2.20875236  3.29631376 61.27976867]\n",
      "0 \tval accuracy:  0.82442033 \t f_! score:  [0.03248165 0.0484752  0.90117307]\n",
      "\n",
      "202 : loss:  0.6222306 \t acc:  0.78\n",
      "204 : loss:  0.5493548 \t acc:  0.82\n",
      "206 : loss:  0.56681466 \t acc:  0.81\n",
      "208 : loss:  0.41796905 \t acc:  0.87\n",
      "210 : loss:  0.45699155 \t acc:  0.86\n",
      "212 : loss:  0.3012731 \t acc:  0.93\n",
      "214 : loss:  0.6250648 \t acc:  0.79\n",
      "216 : loss:  0.47900474 \t acc:  0.85\n",
      "218 : loss:  0.92336196 \t acc:  0.59\n",
      "220 : loss:  1.0425944 \t acc:  0.51\n",
      "222 : loss:  0.7373378 \t acc:  0.72\n",
      "224 : loss:  0.57385194 \t acc:  0.76\n",
      "226 : loss:  0.4763088 \t acc:  0.87\n",
      "228 : loss:  0.65522295 \t acc:  0.75\n",
      "230 : loss:  0.4435056 \t acc:  0.86\n",
      "232 : loss:  0.54516596 \t acc:  0.83\n",
      "234 : loss:  0.5439442 \t acc:  0.8\n",
      "236 : loss:  0.87982607 \t acc:  0.63\n",
      "238 : loss:  0.6793757 \t acc:  0.72\n",
      "240 : loss:  0.7673052 \t acc:  0.68\n",
      "242 : loss:  0.34520692 \t acc:  0.92\n",
      "244 : loss:  0.49786082 \t acc:  0.84\n",
      "246 : loss:  0.6794711 \t acc:  0.76\n",
      "248 : loss:  0.5412744 \t acc:  0.79\n",
      "250 : loss:  0.4405476 \t acc:  0.86\n",
      "252 : loss:  0.82024693 \t acc:  0.64\n",
      "254 : loss:  0.329976 \t acc:  0.96\n",
      "256 : loss:  0.6589802 \t acc:  0.77\n",
      "258 : loss:  0.7525932 \t acc:  0.75\n",
      "260 : loss:  0.6396823 \t acc:  0.74\n",
      "262 : loss:  0.54473066 \t acc:  0.81\n",
      "264 : loss:  0.3902597 \t acc:  0.89\n",
      "266 : loss:  0.34881705 \t acc:  0.9\n",
      "268 : loss:  0.7997689 \t acc:  0.68\n",
      "270 : loss:  0.41788676 \t acc:  0.91\n",
      "272 : loss:  0.4699775 \t acc:  0.85\n",
      "274 : loss:  0.32856983 \t acc:  0.92\n",
      "276 : loss:  0.5780138 \t acc:  0.79\n",
      "278 : loss:  0.54982257 \t acc:  0.81\n",
      "280 : loss:  0.47675523 \t acc:  0.81\n",
      "282 : loss:  1.0723325 \t acc:  0.5\n",
      "284 : loss:  0.54787946 \t acc:  0.82\n",
      "286 : loss:  1.2223058 \t acc:  0.5\n",
      "288 : loss:  0.27195907 \t acc:  0.98\n",
      "290 : loss:  0.5931269 \t acc:  0.78\n",
      "292 : loss:  0.74934083 \t acc:  0.7\n",
      "294 : loss:  0.4487662 \t acc:  0.88\n",
      "296 : loss:  0.39617127 \t acc:  0.91\n",
      "298 : loss:  0.50425005 \t acc:  0.83\n",
      "300 : loss:  0.5971329 \t acc:  0.79\n",
      "302 : loss:  0.8730425 \t acc:  0.61\n",
      "304 : loss:  0.5679085 \t acc:  0.78\n",
      "306 : loss:  0.6134328 \t acc:  0.76\n",
      "308 : loss:  0.6089331 \t acc:  0.81\n",
      "310 : loss:  1.0032172 \t acc:  0.48\n",
      "312 : loss:  0.6395889 \t acc:  0.73\n",
      "314 : loss:  0.5132726 \t acc:  0.84\n",
      "316 : loss:  0.5471708 \t acc:  0.77\n",
      "318 : loss:  0.31540623 \t acc:  0.92\n",
      "320 : loss:  0.7673223 \t acc:  0.63\n",
      "322 : loss:  0.34532338 \t acc:  0.89\n",
      "324 : loss:  0.2000496 \t acc:  0.96\n",
      "326 : loss:  0.4346515 \t acc:  0.85\n",
      "328 : loss:  0.25467658 \t acc:  0.93\n",
      "330 : loss:  0.3757238 \t acc:  0.89\n",
      "332 : loss:  0.75913125 \t acc:  0.7\n",
      "334 : loss:  0.6538919 \t acc:  0.74\n",
      "336 : loss:  0.58093536 \t acc:  0.8\n",
      "338 : loss:  0.75763106 \t acc:  0.68\n",
      "340 : loss:  0.35761774 \t acc:  0.9\n",
      "342 : loss:  0.33046573 \t acc:  0.94\n",
      "344 : loss:  0.53738064 \t acc:  0.8\n",
      "346 : loss:  0.6515111 \t acc:  0.74\n",
      "348 : loss:  0.47768754 \t acc:  0.85\n",
      "350 : loss:  0.52941364 \t acc:  0.81\n",
      "352 : loss:  0.4086338 \t acc:  0.87\n",
      "354 : loss:  0.80027956 \t acc:  0.64\n",
      "356 : loss:  0.8630001 \t acc:  0.69\n",
      "358 : loss:  0.38395804 \t acc:  0.9\n",
      "360 : loss:  0.7843035 \t acc:  0.65\n",
      "362 : loss:  0.39316422 \t acc:  0.92\n",
      "364 : loss:  0.91291404 \t acc:  0.6\n",
      "366 : loss:  0.6422059 \t acc:  0.75\n",
      "368 : loss:  0.93157876 \t acc:  0.61\n",
      "370 : loss:  0.63799584 \t acc:  0.74\n",
      "372 : loss:  0.54244035 \t acc:  0.8\n",
      "374 : loss:  0.2237095 \t acc:  0.95\n",
      "376 : loss:  0.63648736 \t acc:  0.77\n",
      "378 : loss:  0.7808818 \t acc:  0.66\n",
      "380 : loss:  0.8004355 \t acc:  0.65\n",
      "382 : loss:  0.22355837 \t acc:  0.97\n",
      "384 : loss:  0.26262334 \t acc:  0.96\n",
      "386 : loss:  0.41171536 \t acc:  0.86\n",
      "388 : loss:  0.47824985 \t acc:  0.86\n",
      "390 : loss:  0.57221675 \t acc:  0.79\n",
      "392 : loss:  0.6081684 \t acc:  0.81\n",
      "394 : loss:  0.58558154 \t acc:  0.77\n",
      "396 : loss:  0.4450395 \t acc:  0.86\n",
      "398 : loss:  0.58123076 \t acc:  0.79\n",
      "400 : loss:  0.783266 \t acc:  0.64\n",
      "\n",
      "Saving...\n",
      "saved to models/LSTM/pretrained_model.ckpt-402\n",
      "(2,)\n",
      "(2,)\n",
      "\n",
      "68 \t [ 6.28377019  0.58531746 61.25217354]\n",
      "1 \tval accuracy:  0.8226812 \t f_! score:  [0.09240839 0.00860761 0.90076726]\n",
      "\n",
      "402 : loss:  0.8003247 \t acc:  0.65\n",
      "404 : loss:  0.58754855 \t acc:  0.78\n",
      "406 : loss:  0.59500945 \t acc:  0.78\n",
      "408 : loss:  0.46042502 \t acc:  0.79\n",
      "410 : loss:  0.5483452 \t acc:  0.82\n",
      "412 : loss:  0.3681826 \t acc:  0.88\n",
      "414 : loss:  0.528832 \t acc:  0.82\n",
      "416 : loss:  0.67169905 \t acc:  0.74\n",
      "418 : loss:  0.60382074 \t acc:  0.75\n",
      "420 : loss:  1.0671426 \t acc:  0.56\n",
      "422 : loss:  0.5727834 \t acc:  0.77\n",
      "424 : loss:  0.49692237 \t acc:  0.83\n",
      "426 : loss:  0.8224002 \t acc:  0.68\n",
      "428 : loss:  0.3585118 \t acc:  0.89\n",
      "430 : loss:  0.5774191 \t acc:  0.8\n",
      "432 : loss:  0.56646734 \t acc:  0.77\n",
      "434 : loss:  0.7580917 \t acc:  0.68\n",
      "436 : loss:  0.531399 \t acc:  0.8\n",
      "438 : loss:  0.42032683 \t acc:  0.86\n",
      "440 : loss:  0.73736054 \t acc:  0.71\n",
      "442 : loss:  0.6456759 \t acc:  0.76\n",
      "444 : loss:  0.69467896 \t acc:  0.73\n",
      "446 : loss:  0.8908109 \t acc:  0.67\n",
      "448 : loss:  0.41531536 \t acc:  0.89\n",
      "450 : loss:  0.49823692 \t acc:  0.85\n",
      "452 : loss:  0.4259203 \t acc:  0.84\n",
      "454 : loss:  0.6618595 \t acc:  0.73\n",
      "456 : loss:  0.5109175 \t acc:  0.83\n",
      "458 : loss:  0.5529911 \t acc:  0.81\n",
      "460 : loss:  0.72649986 \t acc:  0.7\n",
      "462 : loss:  0.74485564 \t acc:  0.69\n",
      "464 : loss:  0.83486277 \t acc:  0.65\n",
      "466 : loss:  0.43051723 \t acc:  0.92\n",
      "468 : loss:  0.9376896 \t acc:  0.56\n",
      "470 : loss:  0.6994004 \t acc:  0.73\n",
      "472 : loss:  0.4649872 \t acc:  0.86\n",
      "474 : loss:  0.32760432 \t acc:  0.91\n",
      "476 : loss:  0.6059655 \t acc:  0.8\n",
      "478 : loss:  0.6920676 \t acc:  0.73\n",
      "480 : loss:  0.67411286 \t acc:  0.78\n",
      "482 : loss:  0.5070958 \t acc:  0.83\n",
      "484 : loss:  0.64697975 \t acc:  0.75\n",
      "486 : loss:  0.65568525 \t acc:  0.76\n",
      "488 : loss:  0.7321703 \t acc:  0.66\n",
      "490 : loss:  0.46417022 \t acc:  0.85\n",
      "492 : loss:  0.73333776 \t acc:  0.66\n",
      "494 : loss:  0.61862123 \t acc:  0.76\n",
      "496 : loss:  0.3580538 \t acc:  0.89\n",
      "498 : loss:  0.5828322 \t acc:  0.79\n",
      "500 : loss:  0.41477948 \t acc:  0.89\n",
      "502 : loss:  0.39626867 \t acc:  0.92\n",
      "504 : loss:  0.7202852 \t acc:  0.7\n",
      "506 : loss:  0.6299897 \t acc:  0.75\n",
      "508 : loss:  0.6813319 \t acc:  0.74\n",
      "510 : loss:  0.6328826 \t acc:  0.73\n",
      "512 : loss:  0.800452 \t acc:  0.67\n",
      "514 : loss:  0.6959569 \t acc:  0.71\n",
      "516 : loss:  0.5686483 \t acc:  0.79\n",
      "518 : loss:  0.42584646 \t acc:  0.85\n",
      "520 : loss:  0.7334877 \t acc:  0.67\n",
      "522 : loss:  0.42078972 \t acc:  0.85\n",
      "524 : loss:  0.3341601 \t acc:  0.89\n",
      "526 : loss:  0.5162705 \t acc:  0.82\n",
      "528 : loss:  0.40630433 \t acc:  0.92\n",
      "530 : loss:  0.6376686 \t acc:  0.77\n",
      "532 : loss:  0.57634866 \t acc:  0.74\n",
      "534 : loss:  0.28606343 \t acc:  0.93\n",
      "536 : loss:  0.8162608 \t acc:  0.67\n",
      "538 : loss:  0.54823405 \t acc:  0.83\n",
      "540 : loss:  0.19862795 \t acc:  0.96\n",
      "542 : loss:  0.70905566 \t acc:  0.67\n",
      "544 : loss:  0.9687592 \t acc:  0.6\n",
      "546 : loss:  1.153303 \t acc:  0.49\n",
      "548 : loss:  1.031207 \t acc:  0.53\n",
      "550 : loss:  0.52202725 \t acc:  0.8\n",
      "552 : loss:  0.38347858 \t acc:  0.88\n",
      "554 : loss:  0.32803935 \t acc:  0.92\n",
      "556 : loss:  0.7687092 \t acc:  0.73\n",
      "558 : loss:  0.7435439 \t acc:  0.69\n",
      "560 : loss:  0.54861796 \t acc:  0.78\n",
      "562 : loss:  0.4294874 \t acc:  0.87\n",
      "564 : loss:  0.6141871 \t acc:  0.75\n",
      "566 : loss:  0.80365485 \t acc:  0.64\n",
      "568 : loss:  0.5163666 \t acc:  0.82\n",
      "570 : loss:  1.0019802 \t acc:  0.55\n",
      "572 : loss:  0.32686508 \t acc:  0.9\n",
      "574 : loss:  0.16519615 \t acc:  0.97\n",
      "576 : loss:  0.49727413 \t acc:  0.84\n",
      "578 : loss:  0.47430652 \t acc:  0.85\n",
      "580 : loss:  0.762994 \t acc:  0.69\n",
      "582 : loss:  0.35894555 \t acc:  0.9\n",
      "584 : loss:  0.5315949 \t acc:  0.81\n",
      "586 : loss:  0.5979144 \t acc:  0.78\n",
      "588 : loss:  0.9657296 \t acc:  0.48\n",
      "590 : loss:  0.6387991 \t acc:  0.77\n",
      "592 : loss:  0.5539704 \t acc:  0.79\n",
      "594 : loss:  0.5096481 \t acc:  0.83\n",
      "596 : loss:  0.65201396 \t acc:  0.73\n",
      "598 : loss:  0.25849795 \t acc:  0.93\n",
      "600 : loss:  0.45606393 \t acc:  0.85\n",
      "602 : loss:  0.25763953 \t acc:  0.94\n",
      "\n",
      "Saving...\n",
      "saved to models/LSTM/pretrained_model.ckpt-603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yannik/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "\n",
      "69 \t [10.54268381  3.63654343 62.13770311]\n",
      "2 \tval accuracy:  0.81949645 \t f_! score:  [0.15279252 0.05270353 0.90054642]\n",
      "\n",
      "604 : loss:  0.52555645 \t acc:  0.82\n",
      "606 : loss:  0.50227374 \t acc:  0.81\n",
      "608 : loss:  0.75131166 \t acc:  0.68\n",
      "610 : loss:  0.474834 \t acc:  0.85\n",
      "612 : loss:  1.1407872 \t acc:  0.51\n",
      "614 : loss:  0.40816468 \t acc:  0.87\n",
      "616 : loss:  0.71987206 \t acc:  0.66\n",
      "618 : loss:  0.6051438 \t acc:  0.75\n",
      "620 : loss:  0.67484236 \t acc:  0.71\n",
      "622 : loss:  0.28778514 \t acc:  0.93\n",
      "624 : loss:  0.67467886 \t acc:  0.71\n",
      "626 : loss:  0.6193986 \t acc:  0.77\n",
      "628 : loss:  0.97062373 \t acc:  0.55\n",
      "630 : loss:  0.38106674 \t acc:  0.91\n",
      "632 : loss:  0.28179437 \t acc:  0.92\n",
      "634 : loss:  0.5339491 \t acc:  0.79\n",
      "636 : loss:  0.5186994 \t acc:  0.82\n",
      "638 : loss:  0.43164504 \t acc:  0.88\n",
      "640 : loss:  0.4535402 \t acc:  0.85\n",
      "642 : loss:  0.32107854 \t acc:  0.91\n",
      "644 : loss:  0.55067074 \t acc:  0.8\n",
      "646 : loss:  0.47906807 \t acc:  0.83\n",
      "648 : loss:  0.27169767 \t acc:  0.93\n",
      "650 : loss:  0.33099946 \t acc:  0.91\n",
      "652 : loss:  0.808746 \t acc:  0.67\n",
      "654 : loss:  0.3981556 \t acc:  0.87\n",
      "656 : loss:  0.22946557 \t acc:  0.95\n",
      "658 : loss:  0.5799971 \t acc:  0.77\n",
      "660 : loss:  0.95034254 \t acc:  0.54\n",
      "662 : loss:  0.5817689 \t acc:  0.8\n",
      "664 : loss:  0.7239232 \t acc:  0.69\n",
      "666 : loss:  0.7601641 \t acc:  0.64\n",
      "668 : loss:  0.79155004 \t acc:  0.67\n",
      "670 : loss:  0.8565818 \t acc:  0.61\n",
      "672 : loss:  0.30915368 \t acc:  0.92\n",
      "674 : loss:  0.61194307 \t acc:  0.73\n",
      "676 : loss:  0.28244257 \t acc:  0.93\n",
      "678 : loss:  0.39829293 \t acc:  0.84\n",
      "680 : loss:  0.4169547 \t acc:  0.88\n",
      "682 : loss:  0.3783853 \t acc:  0.88\n",
      "684 : loss:  0.59072113 \t acc:  0.79\n",
      "686 : loss:  0.9707799 \t acc:  0.62\n",
      "688 : loss:  0.7777509 \t acc:  0.69\n",
      "690 : loss:  0.6778121 \t acc:  0.76\n",
      "692 : loss:  0.35547838 \t acc:  0.9\n",
      "694 : loss:  0.5005246 \t acc:  0.81\n",
      "696 : loss:  0.6672459 \t acc:  0.76\n",
      "698 : loss:  0.8049526 \t acc:  0.63\n",
      "700 : loss:  0.47325003 \t acc:  0.84\n",
      "702 : loss:  0.32024333 \t acc:  0.89\n",
      "704 : loss:  0.5269733 \t acc:  0.82\n",
      "706 : loss:  0.5126601 \t acc:  0.82\n",
      "708 : loss:  0.5528851 \t acc:  0.8\n",
      "710 : loss:  0.64675325 \t acc:  0.73\n",
      "712 : loss:  1.0191053 \t acc:  0.49\n",
      "714 : loss:  0.56470704 \t acc:  0.81\n",
      "716 : loss:  0.2533339 \t acc:  0.95\n",
      "718 : loss:  0.5421605 \t acc:  0.76\n",
      "720 : loss:  0.4564169 \t acc:  0.83\n",
      "722 : loss:  0.5049926 \t acc:  0.83\n",
      "724 : loss:  0.7002154 \t acc:  0.69\n",
      "726 : loss:  0.6988024 \t acc:  0.69\n",
      "728 : loss:  0.4317006 \t acc:  0.85\n",
      "730 : loss:  0.23195177 \t acc:  0.96\n",
      "732 : loss:  0.79040563 \t acc:  0.65\n",
      "734 : loss:  0.67297006 \t acc:  0.71\n",
      "736 : loss:  0.42647415 \t acc:  0.85\n",
      "738 : loss:  0.613152 \t acc:  0.79\n",
      "740 : loss:  0.26562926 \t acc:  0.92\n",
      "742 : loss:  0.9489157 \t acc:  0.62\n",
      "744 : loss:  0.4235551 \t acc:  0.85\n",
      "746 : loss:  0.34473476 \t acc:  0.91\n",
      "748 : loss:  0.3464133 \t acc:  0.9\n",
      "750 : loss:  0.57794046 \t acc:  0.79\n",
      "752 : loss:  0.50861394 \t acc:  0.82\n",
      "754 : loss:  0.32827196 \t acc:  0.91\n",
      "756 : loss:  0.60663104 \t acc:  0.81\n",
      "758 : loss:  0.46097308 \t acc:  0.84\n",
      "760 : loss:  0.27007148 \t acc:  0.92\n",
      "762 : loss:  0.8407232 \t acc:  0.61\n",
      "764 : loss:  0.3803647 \t acc:  0.87\n",
      "766 : loss:  0.5650642 \t acc:  0.77\n",
      "768 : loss:  0.5358716 \t acc:  0.79\n",
      "770 : loss:  0.39614174 \t acc:  0.87\n",
      "772 : loss:  0.5790133 \t acc:  0.75\n",
      "774 : loss:  0.54832 \t acc:  0.81\n",
      "776 : loss:  0.9850041 \t acc:  0.62\n",
      "778 : loss:  0.43962532 \t acc:  0.86\n",
      "780 : loss:  0.4853936 \t acc:  0.82\n",
      "782 : loss:  0.6916645 \t acc:  0.74\n",
      "784 : loss:  0.43177772 \t acc:  0.85\n",
      "786 : loss:  0.52441645 \t acc:  0.8\n",
      "788 : loss:  0.42486924 \t acc:  0.87\n",
      "790 : loss:  0.5865582 \t acc:  0.77\n",
      "792 : loss:  0.7225091 \t acc:  0.73\n",
      "794 : loss:  0.54021895 \t acc:  0.8\n",
      "796 : loss:  0.25178093 \t acc:  0.96\n",
      "798 : loss:  0.48700944 \t acc:  0.8\n",
      "800 : loss:  1.1739111 \t acc:  0.47\n",
      "802 : loss:  0.60335135 \t acc:  0.75\n",
      "\n",
      "Saving...\n",
      "saved to models/LSTM/pretrained_model.ckpt-804\n",
      "(2,)\n",
      "\n",
      "69 \t [11.48000043  4.83111585 62.10731388]\n",
      "3 \tval accuracy:  0.81877697 \t f_! score:  [0.16637682 0.07001617 0.900106  ]\n",
      "\n",
      "804 : loss:  0.7897155 \t acc:  0.64\n",
      "806 : loss:  0.54252553 \t acc:  0.8\n",
      "808 : loss:  0.79825944 \t acc:  0.61\n",
      "810 : loss:  0.40486783 \t acc:  0.86\n",
      "812 : loss:  0.56578434 \t acc:  0.8\n",
      "814 : loss:  0.43554452 \t acc:  0.85\n",
      "816 : loss:  0.36421135 \t acc:  0.87\n",
      "818 : loss:  0.37108943 \t acc:  0.85\n",
      "820 : loss:  0.47135594 \t acc:  0.85\n",
      "822 : loss:  0.5599138 \t acc:  0.8\n",
      "824 : loss:  0.34488934 \t acc:  0.89\n",
      "826 : loss:  0.5769628 \t acc:  0.77\n",
      "828 : loss:  0.4777756 \t acc:  0.78\n",
      "830 : loss:  0.131538 \t acc:  0.98\n",
      "832 : loss:  0.27012998 \t acc:  0.92\n",
      "834 : loss:  0.5241175 \t acc:  0.83\n",
      "836 : loss:  0.5154596 \t acc:  0.79\n",
      "838 : loss:  0.4181288 \t acc:  0.85\n",
      "840 : loss:  0.62856925 \t acc:  0.74\n",
      "842 : loss:  0.6944005 \t acc:  0.68\n",
      "844 : loss:  0.512151 \t acc:  0.81\n",
      "846 : loss:  0.5927825 \t acc:  0.79\n",
      "848 : loss:  0.52532506 \t acc:  0.8\n",
      "850 : loss:  0.29569137 \t acc:  0.92\n",
      "852 : loss:  0.5723838 \t acc:  0.72\n",
      "854 : loss:  0.48991078 \t acc:  0.84\n",
      "856 : loss:  0.36367762 \t acc:  0.88\n",
      "858 : loss:  0.47603908 \t acc:  0.81\n",
      "860 : loss:  0.5746541 \t acc:  0.76\n",
      "862 : loss:  0.4808243 \t acc:  0.82\n",
      "864 : loss:  0.8796738 \t acc:  0.54\n",
      "866 : loss:  0.59032804 \t acc:  0.75\n",
      "868 : loss:  0.68224114 \t acc:  0.69\n",
      "870 : loss:  0.50186896 \t acc:  0.82\n",
      "872 : loss:  0.6392552 \t acc:  0.74\n",
      "874 : loss:  0.59918004 \t acc:  0.81\n",
      "876 : loss:  0.66236603 \t acc:  0.71\n",
      "878 : loss:  0.4920652 \t acc:  0.85\n",
      "880 : loss:  0.630728 \t acc:  0.77\n",
      "882 : loss:  0.6989421 \t acc:  0.71\n",
      "884 : loss:  0.21555337 \t acc:  0.95\n",
      "886 : loss:  0.5477125 \t acc:  0.78\n",
      "888 : loss:  0.49959174 \t acc:  0.81\n",
      "890 : loss:  0.58452964 \t acc:  0.82\n",
      "892 : loss:  0.3833784 \t acc:  0.89\n",
      "894 : loss:  0.87649566 \t acc:  0.63\n",
      "896 : loss:  0.5225988 \t acc:  0.78\n",
      "898 : loss:  0.36657348 \t acc:  0.82\n",
      "900 : loss:  1.1480001 \t acc:  0.51\n",
      "902 : loss:  0.51108295 \t acc:  0.81\n",
      "904 : loss:  0.6658753 \t acc:  0.71\n",
      "906 : loss:  0.35025173 \t acc:  0.9\n",
      "908 : loss:  0.3369741 \t acc:  0.89\n",
      "910 : loss:  0.4302211 \t acc:  0.85\n",
      "912 : loss:  0.33998284 \t acc:  0.9\n",
      "914 : loss:  0.5559238 \t acc:  0.79\n",
      "916 : loss:  0.720907 \t acc:  0.7\n",
      "918 : loss:  0.6285262 \t acc:  0.79\n",
      "920 : loss:  0.64666045 \t acc:  0.71\n",
      "922 : loss:  0.29287925 \t acc:  0.93\n",
      "924 : loss:  0.5063938 \t acc:  0.81\n",
      "926 : loss:  0.89930636 \t acc:  0.61\n",
      "928 : loss:  0.3929609 \t acc:  0.88\n",
      "930 : loss:  0.7784737 \t acc:  0.67\n",
      "932 : loss:  0.21875927 \t acc:  0.94\n",
      "934 : loss:  0.47719684 \t acc:  0.84\n",
      "936 : loss:  0.5877396 \t acc:  0.79\n",
      "938 : loss:  0.4944259 \t acc:  0.81\n",
      "940 : loss:  0.23425509 \t acc:  0.95\n",
      "942 : loss:  0.40013474 \t acc:  0.83\n",
      "944 : loss:  0.45827523 \t acc:  0.84\n",
      "946 : loss:  1.0660042 \t acc:  0.48\n",
      "948 : loss:  0.32580823 \t acc:  0.88\n",
      "950 : loss:  0.5648819 \t acc:  0.78\n",
      "952 : loss:  0.6452149 \t acc:  0.72\n",
      "954 : loss:  0.6885651 \t acc:  0.7\n",
      "956 : loss:  0.74898905 \t acc:  0.72\n",
      "958 : loss:  0.55531234 \t acc:  0.81\n",
      "960 : loss:  0.61871946 \t acc:  0.77\n",
      "962 : loss:  0.21613654 \t acc:  0.95\n",
      "964 : loss:  0.39852473 \t acc:  0.86\n",
      "966 : loss:  0.54176277 \t acc:  0.82\n",
      "968 : loss:  0.7119262 \t acc:  0.71\n",
      "970 : loss:  0.4711462 \t acc:  0.81\n",
      "972 : loss:  0.39847156 \t acc:  0.85\n",
      "974 : loss:  0.347487 \t acc:  0.88\n",
      "976 : loss:  0.8711237 \t acc:  0.67\n",
      "978 : loss:  0.42535618 \t acc:  0.86\n",
      "980 : loss:  0.37447047 \t acc:  0.89\n",
      "982 : loss:  0.6460041 \t acc:  0.7\n",
      "984 : loss:  0.51557106 \t acc:  0.85\n",
      "986 : loss:  0.34433106 \t acc:  0.91\n",
      "988 : loss:  0.57226574 \t acc:  0.81\n",
      "990 : loss:  0.52631074 \t acc:  0.8\n",
      "992 : loss:  0.27926642 \t acc:  0.94\n",
      "994 : loss:  0.55801535 \t acc:  0.77\n",
      "996 : loss:  0.7829926 \t acc:  0.66\n",
      "998 : loss:  0.48075685 \t acc:  0.83\n",
      "1000 : loss:  0.4063179 \t acc:  0.87\n",
      "1002 : loss:  0.5477051 \t acc:  0.79\n",
      "1004 : loss:  0.37878367 \t acc:  0.87\n",
      "\n",
      "Saving...\n",
      "saved to models/LSTM/pretrained_model.ckpt-1005\n",
      "\n",
      "70 \t [20.64046817  0.9979982  63.16250037]\n",
      "4 \tval accuracy:  0.816 \t f_! score:  [0.29486383 0.01425712 0.90232143]\n",
      "\n",
      "1006 : loss:  0.5493183 \t acc:  0.79\n",
      "1008 : loss:  0.50285405 \t acc:  0.82\n",
      "1010 : loss:  0.25516042 \t acc:  0.95\n",
      "1012 : loss:  0.5106631 \t acc:  0.83\n",
      "1014 : loss:  0.71492577 \t acc:  0.75\n",
      "1016 : loss:  0.69461656 \t acc:  0.67\n",
      "1018 : loss:  0.34687454 \t acc:  0.9\n",
      "1020 : loss:  0.62959325 \t acc:  0.72\n",
      "1022 : loss:  0.7194115 \t acc:  0.66\n",
      "1024 : loss:  0.62447 \t acc:  0.75\n",
      "1026 : loss:  0.51886946 \t acc:  0.82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1028 : loss:  0.5316595 \t acc:  0.8\n",
      "1030 : loss:  0.48307922 \t acc:  0.81\n",
      "1032 : loss:  0.22208656 \t acc:  0.95\n",
      "1034 : loss:  0.6444395 \t acc:  0.74\n",
      "1036 : loss:  0.33408356 \t acc:  0.9\n",
      "1038 : loss:  0.40562987 \t acc:  0.91\n",
      "1040 : loss:  0.5606301 \t acc:  0.77\n",
      "1042 : loss:  0.47222903 \t acc:  0.82\n",
      "1044 : loss:  0.44778618 \t acc:  0.85\n",
      "1046 : loss:  0.5612959 \t acc:  0.78\n",
      "1048 : loss:  0.29390794 \t acc:  0.89\n",
      "1050 : loss:  0.35334674 \t acc:  0.88\n",
      "1052 : loss:  0.4139271 \t acc:  0.86\n",
      "1054 : loss:  0.32194242 \t acc:  0.92\n",
      "1056 : loss:  0.40915602 \t acc:  0.84\n",
      "1058 : loss:  0.29356462 \t acc:  0.93\n",
      "1060 : loss:  0.6202153 \t acc:  0.76\n",
      "1062 : loss:  0.87149775 \t acc:  0.65\n",
      "1064 : loss:  0.7070538 \t acc:  0.74\n",
      "1066 : loss:  0.36586994 \t acc:  0.88\n",
      "1068 : loss:  0.5782064 \t acc:  0.74\n",
      "1070 : loss:  0.44612342 \t acc:  0.84\n",
      "1072 : loss:  0.37510082 \t acc:  0.88\n",
      "1074 : loss:  0.2154276 \t acc:  0.97\n",
      "1076 : loss:  0.49954212 \t acc:  0.82\n",
      "1078 : loss:  0.5985784 \t acc:  0.76\n",
      "1080 : loss:  0.43087903 \t acc:  0.82\n",
      "1082 : loss:  0.50240225 \t acc:  0.8\n",
      "1084 : loss:  0.6043847 \t acc:  0.73\n",
      "1086 : loss:  0.61012834 \t acc:  0.72\n",
      "1088 : loss:  0.49684256 \t acc:  0.83\n",
      "1090 : loss:  0.27800018 \t acc:  0.98\n",
      "1092 : loss:  0.37540704 \t acc:  0.86\n",
      "1094 : loss:  0.43274158 \t acc:  0.84\n",
      "1096 : loss:  0.51669 \t acc:  0.76\n",
      "1098 : loss:  0.39966574 \t acc:  0.84\n",
      "1100 : loss:  0.69144213 \t acc:  0.71\n",
      "1102 : loss:  0.87289643 \t acc:  0.63\n",
      "1104 : loss:  0.36192825 \t acc:  0.89\n",
      "1106 : loss:  0.62735164 \t acc:  0.79\n",
      "1108 : loss:  0.57460445 \t acc:  0.81\n",
      "1110 : loss:  0.55669576 \t acc:  0.8\n",
      "1112 : loss:  0.58056366 \t acc:  0.73\n",
      "1114 : loss:  0.3367136 \t acc:  0.92\n",
      "1116 : loss:  0.27079082 \t acc:  0.93\n",
      "1118 : loss:  0.5023055 \t acc:  0.8\n",
      "1120 : loss:  0.47553694 \t acc:  0.85\n",
      "1122 : loss:  0.9786832 \t acc:  0.58\n",
      "1124 : loss:  0.7672914 \t acc:  0.66\n",
      "1126 : loss:  0.29805747 \t acc:  0.92\n",
      "1128 : loss:  0.3938473 \t acc:  0.86\n",
      "1130 : loss:  0.7323363 \t acc:  0.68\n",
      "1132 : loss:  0.64026475 \t acc:  0.78\n",
      "1134 : loss:  0.3817239 \t acc:  0.86\n",
      "1136 : loss:  0.25949332 \t acc:  0.92\n",
      "1138 : loss:  0.22675735 \t acc:  0.95\n",
      "1140 : loss:  0.5738326 \t acc:  0.76\n",
      "1142 : loss:  0.18989727 \t acc:  0.95\n",
      "1144 : loss:  0.5623783 \t acc:  0.77\n",
      "1146 : loss:  0.67472535 \t acc:  0.76\n",
      "1148 : loss:  0.40823907 \t acc:  0.84\n",
      "1150 : loss:  0.7094887 \t acc:  0.7\n",
      "1152 : loss:  0.4542588 \t acc:  0.83\n",
      "1154 : loss:  0.5042987 \t acc:  0.78\n",
      "1156 : loss:  0.81652254 \t acc:  0.7\n",
      "1158 : loss:  0.7880347 \t acc:  0.64\n",
      "1160 : loss:  0.5268083 \t acc:  0.82\n",
      "1162 : loss:  0.53466505 \t acc:  0.82\n",
      "1164 : loss:  0.72053766 \t acc:  0.69\n",
      "1166 : loss:  0.4692336 \t acc:  0.89\n",
      "1168 : loss:  0.36721867 \t acc:  0.89\n",
      "1170 : loss:  0.387417 \t acc:  0.88\n",
      "1172 : loss:  0.6638112 \t acc:  0.77\n",
      "1174 : loss:  0.73589516 \t acc:  0.69\n",
      "1176 : loss:  0.59975207 \t acc:  0.8\n",
      "1178 : loss:  0.48965698 \t acc:  0.81\n",
      "1180 : loss:  0.55213714 \t acc:  0.78\n",
      "1182 : loss:  0.3896266 \t acc:  0.87\n",
      "1184 : loss:  0.49892625 \t acc:  0.83\n",
      "1186 : loss:  0.5269106 \t acc:  0.83\n",
      "1188 : loss:  0.20909253 \t acc:  0.96\n",
      "1190 : loss:  0.61304724 \t acc:  0.74\n",
      "1192 : loss:  0.7019179 \t acc:  0.69\n",
      "1194 : loss:  0.30113584 \t acc:  0.89\n",
      "1196 : loss:  0.6703044 \t acc:  0.76\n",
      "1198 : loss:  0.5366356 \t acc:  0.82\n",
      "1200 : loss:  0.31425732 \t acc:  0.89\n",
      "1202 : loss:  0.47919503 \t acc:  0.85\n",
      "1204 : loss:  0.8637101 \t acc:  0.59\n",
      "\n",
      "Saving...\n",
      "saved to models/LSTM/pretrained_model.ckpt-1206\n",
      "(2,)\n",
      "\n",
      "69 \t [20.29817309  5.53820523 61.92571375]\n",
      "5 \tval accuracy:  0.813741 \t f_! score:  [0.29417642 0.08026384 0.89747411]\n",
      "\n",
      "1206 : loss:  0.53753746 \t acc:  0.8\n",
      "1208 : loss:  0.37616706 \t acc:  0.85\n",
      "1210 : loss:  0.54731375 \t acc:  0.78\n",
      "1212 : loss:  0.4219677 \t acc:  0.86\n",
      "1214 : loss:  0.33132362 \t acc:  0.9\n",
      "1216 : loss:  0.2335453 \t acc:  0.94\n",
      "1218 : loss:  0.29946432 \t acc:  0.9\n",
      "1220 : loss:  0.480101 \t acc:  0.78\n",
      "1222 : loss:  0.34143272 \t acc:  0.9\n",
      "1224 : loss:  0.17910397 \t acc:  0.96\n",
      "1226 : loss:  0.47432914 \t acc:  0.8\n",
      "1228 : loss:  0.43369374 \t acc:  0.83\n",
      "1230 : loss:  0.79179245 \t acc:  0.66\n",
      "1232 : loss:  0.38367262 \t acc:  0.86\n",
      "1234 : loss:  0.37366477 \t acc:  0.89\n",
      "1236 : loss:  0.49163407 \t acc:  0.77\n",
      "1238 : loss:  0.81376404 \t acc:  0.66\n",
      "1240 : loss:  0.43057975 \t acc:  0.83\n",
      "1242 : loss:  0.31740785 \t acc:  0.88\n",
      "1244 : loss:  0.6980615 \t acc:  0.69\n",
      "1246 : loss:  0.5485429 \t acc:  0.77\n",
      "1248 : loss:  0.48967808 \t acc:  0.81\n",
      "1250 : loss:  0.31066808 \t acc:  0.92\n",
      "1252 : loss:  0.7349357 \t acc:  0.75\n",
      "1254 : loss:  0.7524829 \t acc:  0.71\n",
      "1256 : loss:  0.7422605 \t acc:  0.66\n",
      "1258 : loss:  0.74211967 \t acc:  0.71\n",
      "1260 : loss:  0.29147744 \t acc:  0.92\n",
      "1262 : loss:  0.33844277 \t acc:  0.91\n",
      "1264 : loss:  0.6571122 \t acc:  0.75\n",
      "1266 : loss:  0.354932 \t acc:  0.88\n",
      "1268 : loss:  0.30109516 \t acc:  0.89\n",
      "1270 : loss:  0.62902945 \t acc:  0.73\n",
      "1272 : loss:  0.4648062 \t acc:  0.83\n",
      "1274 : loss:  0.53118664 \t acc:  0.8\n",
      "1276 : loss:  0.2597072 \t acc:  0.9\n",
      "1278 : loss:  0.39750028 \t acc:  0.85\n",
      "1280 : loss:  0.55360395 \t acc:  0.81\n",
      "1282 : loss:  0.65626127 \t acc:  0.69\n",
      "1284 : loss:  0.3866659 \t acc:  0.86\n",
      "1286 : loss:  0.24923453 \t acc:  0.94\n",
      "1288 : loss:  0.5274654 \t acc:  0.84\n",
      "1290 : loss:  0.25441954 \t acc:  0.9\n",
      "1292 : loss:  0.38474387 \t acc:  0.86\n",
      "1294 : loss:  0.47607914 \t acc:  0.79\n",
      "1296 : loss:  0.3916656 \t acc:  0.85\n",
      "1298 : loss:  0.52454394 \t acc:  0.77\n",
      "1300 : loss:  0.7582183 \t acc:  0.7\n",
      "1302 : loss:  0.42743692 \t acc:  0.85\n",
      "1304 : loss:  0.26281884 \t acc:  0.93\n",
      "1306 : loss:  0.5448484 \t acc:  0.77\n",
      "1308 : loss:  0.38050053 \t acc:  0.88\n",
      "1310 : loss:  0.6486765 \t acc:  0.73\n",
      "1312 : loss:  0.64705545 \t acc:  0.74\n",
      "1314 : loss:  0.77965707 \t acc:  0.63\n",
      "1316 : loss:  0.42508033 \t acc:  0.85\n",
      "1318 : loss:  0.5710714 \t acc:  0.75\n",
      "1320 : loss:  0.5075758 \t acc:  0.8\n",
      "1322 : loss:  0.37537205 \t acc:  0.86\n",
      "1324 : loss:  0.5521054 \t acc:  0.8\n",
      "1326 : loss:  0.37143916 \t acc:  0.88\n",
      "1328 : loss:  0.2894222 \t acc:  0.91\n",
      "1330 : loss:  0.2394094 \t acc:  0.96\n",
      "1332 : loss:  0.5283682 \t acc:  0.77\n",
      "1334 : loss:  0.35317895 \t acc:  0.88\n",
      "1336 : loss:  0.46758014 \t acc:  0.8\n",
      "1338 : loss:  0.5368785 \t acc:  0.85\n",
      "1340 : loss:  1.2497697 \t acc:  0.53\n",
      "1342 : loss:  0.63025373 \t acc:  0.73\n",
      "1344 : loss:  0.61645615 \t acc:  0.79\n",
      "1346 : loss:  0.32233647 \t acc:  0.89\n",
      "1348 : loss:  0.41542172 \t acc:  0.85\n",
      "1350 : loss:  0.45403096 \t acc:  0.84\n",
      "1352 : loss:  0.349999 \t acc:  0.88\n",
      "1354 : loss:  0.5678055 \t acc:  0.8\n",
      "1356 : loss:  0.5761377 \t acc:  0.8\n",
      "1358 : loss:  0.7892694 \t acc:  0.64\n",
      "1360 : loss:  0.5283848 \t acc:  0.8\n",
      "1362 : loss:  0.43099284 \t acc:  0.84\n",
      "1364 : loss:  0.6907439 \t acc:  0.69\n",
      "1366 : loss:  0.5099376 \t acc:  0.81\n",
      "1368 : loss:  0.524866 \t acc:  0.81\n",
      "1370 : loss:  0.45459777 \t acc:  0.84\n",
      "1372 : loss:  0.59681296 \t acc:  0.75\n",
      "1374 : loss:  0.6429691 \t acc:  0.78\n",
      "1376 : loss:  0.41100666 \t acc:  0.86\n",
      "1378 : loss:  0.47909915 \t acc:  0.79\n",
      "1380 : loss:  0.9612234 \t acc:  0.68\n",
      "1382 : loss:  0.37708628 \t acc:  0.88\n",
      "1384 : loss:  0.31021917 \t acc:  0.9\n",
      "1386 : loss:  0.37151894 \t acc:  0.85\n",
      "1388 : loss:  0.524 \t acc:  0.79\n",
      "1390 : loss:  0.55506545 \t acc:  0.76\n",
      "1392 : loss:  0.27111125 \t acc:  0.94\n",
      "1394 : loss:  0.36221972 \t acc:  0.88\n",
      "1396 : loss:  0.6040484 \t acc:  0.79\n",
      "1398 : loss:  0.43447417 \t acc:  0.83\n",
      "1400 : loss:  0.39039084 \t acc:  0.9\n",
      "1402 : loss:  0.37630644 \t acc:  0.87\n",
      "1404 : loss:  0.32992658 \t acc:  0.89\n",
      "1406 : loss:  0.99813986 \t acc:  0.57\n",
      "\n",
      "Saving...\n",
      "saved to models/LSTM/pretrained_model.ckpt-1407\n",
      "(2,)\n",
      "\n",
      "69 \t [18.54674622  5.53420964 62.33818055]\n",
      "6 \tval accuracy:  0.8242447 \t f_! score:  [0.26879342 0.08020594 0.90345189]\n",
      "\n",
      "69 \t [18.54674622  5.53420964 62.33818055]\n",
      "69 \t [25.25407648 10.78333333 59.23826113]\n",
      "69 \t [17.05766032  3.91111869 65.97501208]\n",
      "69 \t [ 501.  661. 5738.]\n",
      "--- Test   Prime ---\n",
      "0.8242447\n",
      "f1:  [0.26879342 0.08020594 0.90345189]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yannik/ba/TestHelper.py:120: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  input_data = self.decide_which_input(input_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509411\n",
      "Vorsicht! es muss vorher ein  LSTM  Netz abgespeichert worden sein\n",
      "\n",
      "Restoring...\n",
      "models/LSTM/pretrained_model.ckpt-1407\n",
      "INFO:tensorflow:Restoring parameters from models/LSTM/pretrained_model.ckpt-1407\n",
      "acc:  0.37\n",
      "acc:  0.32\n",
      "acc:  0.32\n",
      "acc:  0.39\n",
      "acc:  0.26\n",
      "acc:  0.31\n",
      "acc:  0.3\n",
      "acc:  0.38\n",
      "acc:  0.33\n",
      "acc:  0.37\n",
      "acc:  0.37\n",
      "acc:  0.35\n",
      "acc:  0.33\n",
      "acc:  0.28\n",
      "acc:  0.35\n",
      "acc:  0.34\n",
      "acc:  0.32\n",
      "acc:  0.33\n",
      "acc:  0.43\n",
      "acc:  0.39\n",
      "acc:  0.37\n",
      "acc:  0.33\n",
      "acc:  0.28\n",
      "acc:  0.39\n",
      "acc:  0.34\n",
      "acc:  0.33\n",
      "acc:  0.27\n",
      "acc:  0.37\n",
      "acc:  0.29\n",
      "acc:  0.31\n",
      "acc:  0.45\n",
      "acc:  0.28\n",
      "acc:  0.39\n",
      "acc:  0.33\n",
      "acc:  0.44\n",
      "acc:  0.35\n",
      "acc:  0.38\n",
      "acc:  0.33\n",
      "acc:  0.38\n",
      "acc:  0.36\n",
      "acc:  0.4\n",
      "acc:  0.33\n",
      "acc:  0.33\n",
      "acc:  0.42\n",
      "acc:  0.4\n",
      "acc:  0.34\n",
      "acc:  0.42\n",
      "acc:  0.31\n",
      "acc:  0.4\n",
      "acc:  0.3\n",
      "acc:  0.34\n",
      "acc:  0.34\n",
      "acc:  0.32\n",
      "acc:  0.36\n",
      "acc:  0.31\n",
      "acc:  0.32\n",
      "acc:  0.29\n",
      "acc:  0.31\n",
      "acc:  0.39\n",
      "acc:  0.42\n",
      "acc:  0.34\n",
      "acc:  0.25\n",
      "acc:  0.34\n",
      "acc:  0.31\n",
      "acc:  0.37\n",
      "acc:  0.26\n",
      "acc:  0.36\n",
      "acc:  0.33\n",
      "acc:  0.33\n",
      "acc:  0.33\n",
      "acc:  0.39\n",
      "acc:  0.42\n",
      "acc:  0.27\n",
      "acc:  0.33\n",
      "acc:  0.28\n",
      "acc:  0.32\n",
      "acc:  0.31\n",
      "acc:  0.34\n",
      "acc:  0.43\n",
      "acc:  0.31\n",
      "acc:  0.4\n",
      "acc:  0.32\n",
      "acc:  0.38\n",
      "acc:  0.32\n",
      "acc:  0.38\n",
      "acc:  0.35\n",
      "acc:  0.39\n",
      "acc:  0.3\n",
      "acc:  0.4\n",
      "acc:  0.42\n",
      "acc:  0.42\n",
      "acc:  0.31\n",
      "acc:  0.36\n",
      "acc:  0.35\n",
      "acc:  0.4\n",
      "acc:  0.36\n",
      "acc:  0.34\n",
      "acc:  0.34\n",
      "acc:  0.32\n",
      "acc:  0.33\n",
      "acc:  0.35\n",
      "acc:  0.31\n",
      "acc:  0.41\n",
      "acc:  0.36\n",
      "acc:  0.31\n",
      "acc:  0.33\n",
      "acc:  0.38\n",
      "acc:  0.36\n",
      "acc:  0.34\n",
      "acc:  0.39\n",
      "acc:  0.41\n",
      "acc:  0.41\n",
      "acc:  0.36\n",
      "acc:  0.35\n",
      "acc:  0.39\n",
      "acc:  0.39\n",
      "acc:  0.34\n",
      "acc:  0.44\n",
      "acc:  0.37\n",
      "acc:  0.31\n",
      "acc:  0.4\n",
      "acc:  0.33\n",
      "acc:  0.37\n",
      "acc:  0.36\n",
      "acc:  0.3\n",
      "acc:  0.29\n",
      "acc:  0.26\n",
      "acc:  0.43\n",
      "acc:  0.3\n",
      "acc:  0.32\n",
      "acc:  0.36\n",
      "acc:  0.26\n",
      "acc:  0.34\n",
      "acc:  0.37\n",
      "acc:  0.34\n",
      "acc:  0.37\n",
      "acc:  0.26\n",
      "acc:  0.39\n",
      "acc:  0.31\n",
      "acc:  0.33\n",
      "acc:  0.38\n",
      "acc:  0.32\n",
      "acc:  0.34\n",
      "acc:  0.37\n",
      "acc:  0.35\n",
      "acc:  0.39\n",
      "acc:  0.42\n",
      "acc:  0.3\n",
      "acc:  0.33\n",
      "acc:  0.41\n",
      "acc:  0.37\n",
      "acc:  0.3\n",
      "acc:  0.39\n",
      "acc:  0.39\n",
      "acc:  0.34\n",
      "acc:  0.23\n",
      "acc:  0.45\n",
      "acc:  0.37\n",
      "acc:  0.31\n",
      "acc:  0.35\n",
      "acc:  0.32\n",
      "acc:  0.32\n",
      "acc:  0.48\n",
      "acc:  0.42\n",
      "acc:  0.34\n",
      "acc:  0.32\n",
      "acc:  0.35\n",
      "acc:  0.47\n",
      "acc:  0.34\n",
      "acc:  0.34\n",
      "acc:  0.32\n",
      "acc:  0.45\n",
      "acc:  0.39\n",
      "acc:  0.4\n",
      "acc:  0.39\n",
      "acc:  0.35\n",
      "acc:  0.38\n",
      "acc:  0.45\n",
      "acc:  0.33\n",
      "acc:  0.34\n",
      "acc:  0.35\n",
      "acc:  0.33\n",
      "acc:  0.39\n",
      "acc:  0.34\n",
      "acc:  0.43\n",
      "acc:  0.35\n",
      "acc:  0.31\n",
      "acc:  0.34\n",
      "acc:  0.37\n",
      "acc:  0.29\n",
      "acc:  0.35\n",
      "acc:  0.43\n",
      "acc:  0.38\n",
      "acc:  0.32\n",
      "acc:  0.47\n",
      "acc:  0.29\n",
      "acc:  0.38\n",
      "acc:  0.29\n",
      "acc:  0.35\n",
      "acc:  0.39\n",
      "acc:  0.25\n",
      "acc:  0.41\n",
      "acc:  0.38\n",
      "acc:  0.31\n",
      "acc:  0.34\n",
      "acc:  0.24\n",
      "acc:  0.33\n",
      "acc:  0.3\n",
      "acc:  0.4\n",
      "acc:  0.38\n",
      "acc:  0.31\n",
      "acc:  0.28\n",
      "acc:  0.35\n",
      "acc:  0.28\n",
      "acc:  0.33\n",
      "acc:  0.29\n",
      "acc:  0.32\n",
      "acc:  0.32\n",
      "acc:  0.27\n",
      "acc:  0.39\n",
      "acc:  0.36\n",
      "acc:  0.34\n",
      "acc:  0.32\n",
      "acc:  0.28\n",
      "acc:  0.35\n",
      "acc:  0.38\n",
      "acc:  0.42\n",
      "acc:  0.32\n",
      "acc:  0.37\n",
      "acc:  0.29\n",
      "acc:  0.38\n",
      "acc:  0.34\n",
      "acc:  0.32\n",
      "acc:  0.36\n",
      "acc:  0.38\n",
      "acc:  0.24\n",
      "acc:  0.24\n",
      "acc:  0.29\n",
      "acc:  0.33\n",
      "acc:  0.32\n",
      "acc:  0.36\n",
      "acc:  0.39\n",
      "acc:  0.36\n",
      "acc:  0.34\n",
      "acc:  0.38\n",
      "acc:  0.28\n",
      "acc:  0.32\n",
      "acc:  0.41\n",
      "acc:  0.28\n",
      "acc:  0.33\n",
      "acc:  0.36\n",
      "acc:  0.35\n",
      "acc:  0.39\n",
      "acc:  0.38\n",
      "acc:  0.42\n",
      "acc:  0.38\n",
      "acc:  0.44\n",
      "acc:  0.36\n",
      "acc:  0.33\n",
      "acc:  0.37\n",
      "acc:  0.29\n",
      "acc:  0.41\n",
      "acc:  0.39\n",
      "acc:  0.42\n",
      "acc:  0.29\n",
      "acc:  0.35\n",
      "acc:  0.37\n",
      "acc:  0.34\n",
      "acc:  0.34\n",
      "acc:  0.34\n",
      "acc:  0.33\n",
      "acc:  0.38\n",
      "acc:  0.35\n",
      "acc:  0.33\n",
      "acc:  0.31\n",
      "acc:  0.38\n",
      "acc:  0.33\n",
      "acc:  0.3\n",
      "acc:  0.32\n",
      "acc:  0.29\n",
      "acc:  0.34\n",
      "acc:  0.34\n",
      "acc:  0.31\n",
      "acc:  0.29\n",
      "acc:  0.23\n",
      "acc:  0.37\n",
      "acc:  0.39\n",
      "acc:  0.34\n",
      "acc:  0.31\n",
      "acc:  0.4\n",
      "acc:  0.39\n",
      "acc:  0.38\n",
      "acc:  0.36\n",
      "acc:  0.35\n",
      "acc:  0.35\n",
      "acc:  0.34\n",
      "acc:  0.33\n",
      "acc:  0.32\n",
      "acc:  0.32\n",
      "acc:  0.27\n",
      "acc:  0.36\n",
      "acc:  0.38\n",
      "acc:  0.4\n",
      "acc:  0.35\n",
      "acc:  0.33\n",
      "acc:  0.34\n",
      "acc:  0.39\n",
      "acc:  0.38\n",
      "acc:  0.31\n",
      "acc:  0.43\n",
      "acc:  0.34\n",
      "acc:  0.34\n",
      "acc:  0.37\n",
      "acc:  0.36\n",
      "acc:  0.4\n",
      "acc:  0.31\n",
      "acc:  0.45\n",
      "acc:  0.38\n",
      "acc:  0.37\n",
      "acc:  0.35\n",
      "acc:  0.38\n",
      "acc:  0.33\n",
      "acc:  0.33\n",
      "acc:  0.39\n",
      "acc:  0.36\n",
      "acc:  0.3\n",
      "acc:  0.35\n",
      "acc:  0.38\n",
      "acc:  0.28\n",
      "acc:  0.35\n",
      "acc:  0.37\n",
      "acc:  0.32\n",
      "acc:  0.33\n",
      "acc:  0.29\n",
      "acc:  0.38\n",
      "acc:  0.32\n",
      "acc:  0.43\n",
      "acc:  0.37\n",
      "acc:  0.38\n",
      "acc:  0.38\n",
      "acc:  0.28\n",
      "acc:  0.41\n",
      "acc:  0.35\n",
      "acc:  0.32\n",
      "acc:  0.37\n",
      "acc:  0.33\n",
      "acc:  0.34\n",
      "acc:  0.38\n",
      "acc:  0.31\n",
      "acc:  0.33\n",
      "acc:  0.36\n",
      "acc:  0.42\n",
      "acc:  0.35\n",
      "acc:  0.34\n",
      "acc:  0.37\n",
      "acc:  0.37\n",
      "acc:  0.41\n",
      "acc:  0.34\n",
      "acc:  0.31\n",
      "acc:  0.39\n",
      "acc:  0.28\n",
      "acc:  0.34\n",
      "acc:  0.39\n",
      "acc:  0.35\n",
      "acc:  0.34\n",
      "acc:  0.28\n",
      "acc:  0.39\n",
      "acc:  0.31\n",
      "acc:  0.37\n",
      "acc:  0.39\n",
      "acc:  0.41\n",
      "acc:  0.29\n",
      "acc:  0.35\n",
      "acc:  0.32\n",
      "acc:  0.33\n",
      "acc:  0.33\n",
      "acc:  0.37\n",
      "acc:  0.34\n",
      "acc:  0.34\n",
      "acc:  0.33\n",
      "acc:  0.4\n",
      "acc:  0.34\n",
      "acc:  0.37\n",
      "acc:  0.27\n",
      "acc:  0.28\n",
      "acc:  0.39\n",
      "acc:  0.27\n",
      "acc:  0.35\n",
      "acc:  0.35\n",
      "acc:  0.37\n",
      "acc:  0.32\n",
      "acc:  0.33\n",
      "acc:  0.3\n",
      "acc:  0.27\n",
      "acc:  0.41\n",
      "acc:  0.28\n",
      "acc:  0.41\n",
      "acc:  0.3\n",
      "acc:  0.4\n",
      "acc:  0.31\n",
      "acc:  0.3\n",
      "acc:  0.42\n",
      "acc:  0.37\n",
      "acc:  0.26\n",
      "acc:  0.37\n",
      "acc:  0.37\n",
      "acc:  0.38\n",
      "acc:  0.35\n",
      "acc:  0.36\n",
      "acc:  0.27\n",
      "acc:  0.4\n",
      "acc:  0.35\n",
      "acc:  0.33\n",
      "acc:  0.42\n",
      "acc:  0.35\n",
      "acc:  0.39\n",
      "acc:  0.33\n",
      "acc:  0.29\n",
      "acc:  0.3\n",
      "acc:  0.35\n",
      "acc:  0.26\n",
      "acc:  0.35\n",
      "acc:  0.37\n",
      "acc:  0.24\n",
      "acc:  0.38\n",
      "acc:  0.38\n",
      "acc:  0.29\n",
      "acc:  0.32\n",
      "acc:  0.3\n",
      "acc:  0.32\n",
      "acc:  0.35\n",
      "acc:  0.38\n",
      "acc:  0.36\n",
      "acc:  0.34\n",
      "acc:  0.35\n",
      "acc:  0.41\n",
      "acc:  0.48\n",
      "acc:  0.36\n",
      "acc:  0.31\n",
      "acc:  0.33\n",
      "acc:  0.33\n",
      "acc:  0.39\n",
      "acc:  0.4\n",
      "acc:  0.32\n",
      "acc:  0.35\n",
      "acc:  0.35\n",
      "acc:  0.31\n",
      "acc:  0.29\n",
      "acc:  0.25\n",
      "acc:  0.32\n",
      "acc:  0.36\n",
      "acc:  0.4\n",
      "acc:  0.37\n",
      "acc:  0.37\n",
      "acc:  0.25\n",
      "acc:  0.36\n",
      "acc:  0.28\n",
      "acc:  0.36\n",
      "acc:  0.45\n",
      "acc:  0.31\n",
      "acc:  0.34\n",
      "acc:  0.32\n",
      "acc:  0.33\n",
      "acc:  0.36\n",
      "acc:  0.36\n",
      "acc:  0.35\n",
      "acc:  0.37\n",
      "acc:  0.44\n",
      "acc:  0.31\n",
      "acc:  0.35\n",
      "acc:  0.29\n",
      "acc:  0.39\n",
      "acc:  0.28\n",
      "acc:  0.32\n",
      "acc:  0.37\n",
      "acc:  0.33\n",
      "acc:  0.3\n",
      "acc:  0.34\n",
      "acc:  0.32\n",
      "acc:  0.35\n",
      "acc:  0.38\n",
      "acc:  0.4\n",
      "acc:  0.4\n",
      "acc:  0.34\n",
      "acc:  0.37\n",
      "acc:  0.45\n",
      "acc:  0.36\n",
      "acc:  0.3\n",
      "acc:  0.28\n",
      "acc:  0.36\n",
      "acc:  0.38\n",
      "acc:  0.35\n",
      "acc:  0.37\n",
      "acc:  0.29\n",
      "acc:  0.39\n",
      "acc:  0.34\n",
      "acc:  0.33\n",
      "acc:  0.34\n",
      "acc:  0.36\n",
      "acc:  0.31\n",
      "\n",
      "500 \t [ 14.96375547   7.14045127 255.64336975]\n",
      "val accuracy:  0.3479 \t f_! score:  [0.02992751 0.0142809  0.51128674]\n",
      "\n",
      "500 \t [ 14.96375547   7.14045127 255.64336975]\n",
      "500 \t [158.31666667  69.26666667 173.81812763]\n",
      "500 \t [  7.95573395   3.82053648 488.72000738]\n",
      "500 \t [18794. 13823. 17383.]\n",
      "---just Test  Review ---\n",
      "0.3479\n",
      "f1:  [0.02992751 0.0142809  0.51128674]\n",
      "65730\n",
      "Vorsicht! es muss vorher ein  LSTM  Netz abgespeichert worden sein\n",
      "\n",
      "Restoring...\n",
      "models/LSTM/pretrained_model.ckpt-1407\n",
      "INFO:tensorflow:Restoring parameters from models/LSTM/pretrained_model.ckpt-1407\n",
      "acc:  0.4\n",
      "acc:  0.38\n",
      "acc:  0.3\n",
      "acc:  0.37\n",
      "acc:  0.38\n",
      "acc:  0.28\n",
      "acc:  0.31\n",
      "acc:  0.36\n",
      "acc:  0.31\n",
      "acc:  0.33\n",
      "acc:  0.26\n",
      "acc:  0.35\n",
      "acc:  0.38\n",
      "acc:  0.31\n",
      "acc:  0.38\n",
      "acc:  0.27\n",
      "acc:  0.31\n",
      "acc:  0.33\n",
      "acc:  0.38\n",
      "acc:  0.27\n",
      "acc:  0.31\n",
      "acc:  0.37\n",
      "acc:  0.28\n",
      "acc:  0.31\n",
      "acc:  0.38\n",
      "acc:  0.37\n",
      "acc:  0.37\n",
      "acc:  0.25\n",
      "acc:  0.36\n",
      "acc:  0.33\n",
      "acc:  0.27\n",
      "acc:  0.34\n",
      "acc:  0.35\n",
      "acc:  0.31\n",
      "acc:  0.33\n",
      "acc:  0.41\n",
      "acc:  0.32\n",
      "acc:  0.37\n",
      "acc:  0.34\n",
      "acc:  0.41\n",
      "acc:  0.3\n",
      "acc:  0.34\n",
      "acc:  0.22\n",
      "acc:  0.37\n",
      "acc:  0.32\n",
      "acc:  0.37\n",
      "acc:  0.23\n",
      "acc:  0.32\n",
      "acc:  0.38\n",
      "acc:  0.36\n",
      "acc:  0.23\n",
      "acc:  0.33\n",
      "acc:  0.32\n",
      "acc:  0.3\n",
      "acc:  0.36\n",
      "acc:  0.32\n",
      "acc:  0.26\n",
      "acc:  0.27\n",
      "acc:  0.37\n",
      "acc:  0.33\n",
      "acc:  0.34\n",
      "acc:  0.35\n",
      "acc:  0.31\n",
      "acc:  0.32\n",
      "acc:  0.34\n",
      "acc:  0.35\n",
      "acc:  0.32\n",
      "acc:  0.27\n",
      "acc:  0.34\n",
      "acc:  0.31\n",
      "acc:  0.44\n",
      "acc:  0.39\n",
      "acc:  0.34\n",
      "acc:  0.32\n",
      "acc:  0.35\n",
      "acc:  0.26\n",
      "acc:  0.35\n",
      "acc:  0.3\n",
      "acc:  0.29\n",
      "acc:  0.31\n",
      "acc:  0.32\n",
      "acc:  0.29\n",
      "acc:  0.29\n",
      "acc:  0.37\n",
      "acc:  0.36\n",
      "acc:  0.37\n",
      "acc:  0.32\n",
      "acc:  0.32\n",
      "acc:  0.22\n",
      "acc:  0.27\n",
      "acc:  0.39\n",
      "acc:  0.35\n",
      "acc:  0.38\n",
      "acc:  0.33\n",
      "acc:  0.34\n",
      "acc:  0.39\n",
      "acc:  0.26\n",
      "acc:  0.32\n",
      "acc:  0.35\n",
      "acc:  0.33\n",
      "\n",
      "100 \t [ 0.29864681  2.36704399 49.23904835]\n",
      "val accuracy:  0.3291 \t f_! score:  [0.00298647 0.02367044 0.49239048]\n",
      "\n",
      "100 \t [ 0.29864681  2.36704399 49.23904835]\n",
      "100 \t [ 5.         19.5        33.01414544]\n",
      "100 \t [ 0.15396825  1.27884792 98.18793961]\n",
      "100 \t [3310. 3389. 3301.]\n",
      "---just Test  Twitter ---\n",
      "0.3291\n",
      "f1:  [0.00298647 0.02367044 0.49239048]\n",
      "(7733, 3)\n",
      "7733\n",
      "Vorsicht! es muss vorher ein  LSTM  Netz abgespeichert worden sein\n",
      "\n",
      "Restoring...\n",
      "models/LSTM/pretrained_model.ckpt-1407\n",
      "INFO:tensorflow:Restoring parameters from models/LSTM/pretrained_model.ckpt-1407\n",
      "acc:  0.16\n",
      "acc:  0.07\n",
      "acc:  0.17\n",
      "acc:  0.15\n",
      "acc:  0.04\n",
      "acc:  0.06\n",
      "acc:  0.03\n",
      "acc:  0.09\n",
      "acc:  0.04\n",
      "acc:  0.04\n",
      "acc:  0.09\n",
      "acc:  0.12\n",
      "acc:  0.12\n",
      "acc:  0.08\n",
      "\n",
      "14 \t [1.85360637 1.41481004 0.4889615 ]\n",
      "val accuracy:  0.08999998 \t f_! score:  [0.13240046 0.10105786 0.03492582]\n",
      "\n",
      "14 \t [1.85360637 1.41481004 0.4889615 ]\n",
      "14 \t [ 1.04645949  1.11323423 11.41666667]\n",
      "14 \t [11.75970696  2.38490676  0.25133474]\n",
      "14 \t [ 105.  116. 1179.]\n",
      "---just Test  Medical ---\n",
      "0.08999998\n",
      "f1:  [0.13240046 0.10105786 0.03492582]\n"
     ]
    }
   ],
   "source": [
    "prime_loss, prime_acc = testhelper.train_input(\"Prime\", net_name)\n",
    "\n",
    "testhelper.just_test(\"Review\", net_name)\n",
    "testhelper.just_test(\"Twitter\", net_name)\n",
    "testhelper.just_test(\"Medical\", net_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_result(a, b, c, d):\n",
    "    f, axarr = plt.subplots(2, 2)\n",
    "    axarr[0, 0].plot(a)\n",
    "    axarr[0, 0].set_title('Twitter')\n",
    "    axarr[0, 1].plot(b)\n",
    "    axarr[0, 1].set_title('Prime Video')\n",
    "    axarr[1, 0].plot(c)\n",
    "    axarr[1, 0].set_title('Amazon Review')\n",
    "    axarr[1, 1].plot(d)\n",
    "    axarr[1, 1].set_title('Medikamente')\n",
    "    plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.5,\n",
    "                        wspace=0.35)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_result(tw_loss, prime_loss, review_loss, med_loss)\n",
    "plot_result(tw_acc, prime_acc, review_acc, med_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
