{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yannik/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from TestHelper import TestHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  Review\n",
      "delete old models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yannik/ba/TestHelper.py:64: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  input_data = self.decide_which_input(input_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509411\n",
      "Tensor(\"ConvNet/dense/BiasAdd:0\", shape=(?, 3), dtype=float32)\n",
      "Tensor(\"ConvNet_1/dense/BiasAdd:0\", shape=(?, 3), dtype=float32)\n",
      "---init ready   CNN ---\n",
      "\n",
      "Saving...\n",
      "saved to models/CNN/pretrained_model.ckpt-0\n",
      "0 : loss:  3.9866266 \t acc:  0.39\n",
      "40 : loss:  1.0866594 \t acc:  0.34\n",
      "80 : loss:  1.0556129 \t acc:  0.44\n",
      "120 : loss:  1.0662053 \t acc:  0.46\n",
      "160 : loss:  1.0237589 \t acc:  0.59\n",
      "200 : loss:  1.2313827 \t acc:  0.34\n",
      "240 : loss:  1.0283551 \t acc:  0.49\n",
      "280 : loss:  0.8848484 \t acc:  0.59\n",
      "320 : loss:  0.9797041 \t acc:  0.52\n",
      "360 : loss:  1.0596713 \t acc:  0.43\n",
      "400 : loss:  1.0059942 \t acc:  0.48\n",
      "440 : loss:  0.8777803 \t acc:  0.61\n",
      "480 : loss:  0.9136562 \t acc:  0.58\n",
      "520 : loss:  0.9432236 \t acc:  0.5\n",
      "560 : loss:  1.0478618 \t acc:  0.44\n",
      "600 : loss:  0.93136376 \t acc:  0.49\n",
      "640 : loss:  0.866513 \t acc:  0.66\n",
      "680 : loss:  0.7908437 \t acc:  0.65\n",
      "720 : loss:  0.93436515 \t acc:  0.56\n",
      "760 : loss:  0.8386367 \t acc:  0.58\n",
      "800 : loss:  0.9226711 \t acc:  0.51\n",
      "840 : loss:  1.0550455 \t acc:  0.47\n",
      "880 : loss:  0.95029986 \t acc:  0.55\n",
      "920 : loss:  0.9604428 \t acc:  0.54\n",
      "960 : loss:  0.84554833 \t acc:  0.6\n",
      "1000 : loss:  0.9274711 \t acc:  0.56\n",
      "1040 : loss:  0.8269662 \t acc:  0.62\n",
      "1080 : loss:  0.8568268 \t acc:  0.54\n",
      "1120 : loss:  0.8581748 \t acc:  0.59\n",
      "1160 : loss:  0.829659 \t acc:  0.57\n",
      "1200 : loss:  0.77215093 \t acc:  0.63\n",
      "1240 : loss:  0.86295354 \t acc:  0.6\n",
      "1280 : loss:  0.8717663 \t acc:  0.59\n",
      "1320 : loss:  0.82682633 \t acc:  0.61\n",
      "1360 : loss:  0.8477581 \t acc:  0.63\n",
      "1400 : loss:  0.793799 \t acc:  0.65\n",
      "1440 : loss:  0.78120786 \t acc:  0.65\n",
      "1480 : loss:  0.9294659 \t acc:  0.56\n",
      "1520 : loss:  0.912211 \t acc:  0.55\n",
      "1560 : loss:  0.9120355 \t acc:  0.56\n",
      "1600 : loss:  0.78123116 \t acc:  0.64\n",
      "1640 : loss:  0.85215586 \t acc:  0.57\n",
      "1680 : loss:  0.79236084 \t acc:  0.62\n",
      "1720 : loss:  0.7175996 \t acc:  0.63\n",
      "1760 : loss:  0.87223023 \t acc:  0.62\n",
      "1800 : loss:  0.8318712 \t acc:  0.61\n",
      "1840 : loss:  0.87916 \t acc:  0.65\n",
      "1880 : loss:  0.8339069 \t acc:  0.59\n",
      "1920 : loss:  0.87746066 \t acc:  0.55\n",
      "1960 : loss:  0.8542653 \t acc:  0.61\n",
      "2000 : loss:  0.78338057 \t acc:  0.63\n",
      "2040 : loss:  0.92491716 \t acc:  0.55\n",
      "2080 : loss:  0.8797798 \t acc:  0.52\n",
      "2120 : loss:  0.77132463 \t acc:  0.65\n",
      "2160 : loss:  0.8577943 \t acc:  0.65\n",
      "2200 : loss:  0.8005826 \t acc:  0.62\n",
      "2240 : loss:  0.7917906 \t acc:  0.58\n",
      "2280 : loss:  0.7888146 \t acc:  0.61\n",
      "2320 : loss:  0.8078357 \t acc:  0.59\n",
      "2360 : loss:  0.830468 \t acc:  0.59\n",
      "2400 : loss:  0.7676121 \t acc:  0.65\n",
      "2440 : loss:  0.7950161 \t acc:  0.68\n",
      "2480 : loss:  0.7864301 \t acc:  0.61\n",
      "2520 : loss:  0.72814935 \t acc:  0.66\n",
      "2560 : loss:  0.8189295 \t acc:  0.62\n",
      "2600 : loss:  0.7769563 \t acc:  0.59\n",
      "2640 : loss:  0.7344684 \t acc:  0.66\n",
      "2680 : loss:  0.8005286 \t acc:  0.61\n",
      "2720 : loss:  0.7411594 \t acc:  0.68\n",
      "2760 : loss:  0.8672559 \t acc:  0.62\n",
      "2800 : loss:  0.7353196 \t acc:  0.64\n",
      "2840 : loss:  0.8424147 \t acc:  0.63\n",
      "2880 : loss:  0.73203874 \t acc:  0.67\n",
      "2920 : loss:  0.9152871 \t acc:  0.58\n",
      "2960 : loss:  0.7779688 \t acc:  0.62\n",
      "3000 : loss:  0.8388211 \t acc:  0.63\n",
      "3040 : loss:  0.7733941 \t acc:  0.59\n",
      "3080 : loss:  0.6740091 \t acc:  0.73\n",
      "3120 : loss:  0.8034147 \t acc:  0.62\n",
      "3160 : loss:  0.77780616 \t acc:  0.58\n",
      "3200 : loss:  0.8240804 \t acc:  0.61\n",
      "3240 : loss:  0.8068668 \t acc:  0.66\n",
      "3280 : loss:  0.667679 \t acc:  0.73\n",
      "3320 : loss:  0.81486505 \t acc:  0.63\n",
      "3360 : loss:  0.78612685 \t acc:  0.62\n",
      "3400 : loss:  0.8380831 \t acc:  0.57\n",
      "3440 : loss:  0.6969024 \t acc:  0.72\n",
      "3480 : loss:  0.7699952 \t acc:  0.66\n",
      "3520 : loss:  0.7216402 \t acc:  0.71\n",
      "3560 : loss:  0.8362595 \t acc:  0.66\n",
      "3600 : loss:  0.707865 \t acc:  0.74\n",
      "3640 : loss:  0.7670417 \t acc:  0.64\n",
      "3680 : loss:  0.77632254 \t acc:  0.64\n",
      "3720 : loss:  0.90117526 \t acc:  0.56\n",
      "3760 : loss:  0.7776625 \t acc:  0.65\n",
      "3800 : loss:  0.65744 \t acc:  0.71\n",
      "3840 : loss:  0.7635595 \t acc:  0.67\n",
      "3880 : loss:  0.63845706 \t acc:  0.71\n",
      "3920 : loss:  0.74361694 \t acc:  0.69\n",
      "3960 : loss:  0.821262 \t acc:  0.64\n",
      "4000 : loss:  0.72487557 \t acc:  0.71\n",
      "4040 : loss:  0.797929 \t acc:  0.65\n",
      "4080 : loss:  0.77077127 \t acc:  0.65\n",
      "4120 : loss:  0.9000816 \t acc:  0.57\n",
      "4160 : loss:  0.7426441 \t acc:  0.61\n",
      "4200 : loss:  0.6918829 \t acc:  0.68\n",
      "4240 : loss:  0.83783096 \t acc:  0.65\n",
      "4280 : loss:  0.76429826 \t acc:  0.68\n",
      "4320 : loss:  0.84935737 \t acc:  0.6\n",
      "4360 : loss:  0.7266582 \t acc:  0.63\n",
      "4400 : loss:  0.7263175 \t acc:  0.68\n",
      "4440 : loss:  0.80606645 \t acc:  0.63\n",
      "4480 : loss:  0.702458 \t acc:  0.64\n",
      "\n",
      "Saving...\n",
      "saved to models/CNN/pretrained_model.ckpt-4482\n",
      "\n",
      "500 \t [361.18810945 209.23103548 372.10812204]\n",
      "0 \tval accuracy:  0.66392004 \t f_! score:  [0.72237622 0.41846207 0.74421624]\n",
      "\n",
      "4520 : loss:  0.6838667 \t acc:  0.7\n",
      "4560 : loss:  0.7823979 \t acc:  0.56\n",
      "4600 : loss:  0.8175879 \t acc:  0.58\n",
      "4640 : loss:  0.6781845 \t acc:  0.67\n",
      "4680 : loss:  0.6225959 \t acc:  0.78\n",
      "4720 : loss:  0.6589874 \t acc:  0.68\n",
      "4760 : loss:  0.7445951 \t acc:  0.7\n",
      "4800 : loss:  0.6898216 \t acc:  0.67\n",
      "4840 : loss:  0.6984852 \t acc:  0.7\n",
      "4880 : loss:  0.77963454 \t acc:  0.64\n",
      "4920 : loss:  0.6730859 \t acc:  0.66\n",
      "4960 : loss:  0.66996354 \t acc:  0.74\n",
      "5000 : loss:  0.72173476 \t acc:  0.7\n",
      "5040 : loss:  0.69488144 \t acc:  0.68\n",
      "5080 : loss:  0.80992895 \t acc:  0.63\n",
      "5120 : loss:  0.7607853 \t acc:  0.63\n",
      "5160 : loss:  0.64266783 \t acc:  0.74\n",
      "5200 : loss:  0.6187079 \t acc:  0.72\n",
      "5240 : loss:  0.7586631 \t acc:  0.62\n",
      "5280 : loss:  0.7115729 \t acc:  0.75\n",
      "5320 : loss:  0.73766077 \t acc:  0.66\n",
      "5360 : loss:  0.85178506 \t acc:  0.63\n",
      "5400 : loss:  0.8638377 \t acc:  0.59\n",
      "5440 : loss:  0.7304308 \t acc:  0.7\n",
      "5480 : loss:  0.8140271 \t acc:  0.64\n",
      "5520 : loss:  0.8615474 \t acc:  0.65\n",
      "5560 : loss:  0.6926097 \t acc:  0.68\n",
      "5600 : loss:  0.8392117 \t acc:  0.64\n",
      "5640 : loss:  0.6509138 \t acc:  0.71\n",
      "5680 : loss:  0.67816186 \t acc:  0.67\n",
      "5720 : loss:  0.7422293 \t acc:  0.63\n",
      "5760 : loss:  0.7803163 \t acc:  0.63\n",
      "5800 : loss:  0.6704369 \t acc:  0.7\n",
      "5840 : loss:  0.66873205 \t acc:  0.7\n",
      "5880 : loss:  0.6803367 \t acc:  0.73\n",
      "5920 : loss:  0.6557824 \t acc:  0.74\n",
      "5960 : loss:  0.8076837 \t acc:  0.63\n",
      "6000 : loss:  0.7172062 \t acc:  0.64\n",
      "6040 : loss:  0.6598774 \t acc:  0.71\n",
      "6080 : loss:  0.7694814 \t acc:  0.62\n",
      "6120 : loss:  0.72202 \t acc:  0.61\n",
      "6160 : loss:  0.77650344 \t acc:  0.63\n",
      "6200 : loss:  0.6049549 \t acc:  0.8\n",
      "6240 : loss:  0.6314596 \t acc:  0.77\n",
      "6280 : loss:  0.70431685 \t acc:  0.65\n",
      "6320 : loss:  0.7860696 \t acc:  0.57\n",
      "6360 : loss:  0.81129533 \t acc:  0.59\n",
      "6400 : loss:  0.7645076 \t acc:  0.64\n",
      "6440 : loss:  0.83223665 \t acc:  0.65\n",
      "6480 : loss:  0.8514783 \t acc:  0.62\n",
      "6520 : loss:  0.66761374 \t acc:  0.69\n",
      "6560 : loss:  0.78882146 \t acc:  0.64\n",
      "6600 : loss:  0.7106414 \t acc:  0.67\n",
      "6640 : loss:  0.7431077 \t acc:  0.65\n",
      "6680 : loss:  0.73915213 \t acc:  0.65\n",
      "6720 : loss:  0.64017034 \t acc:  0.68\n",
      "6760 : loss:  0.83329177 \t acc:  0.61\n",
      "6800 : loss:  0.75917125 \t acc:  0.64\n",
      "6840 : loss:  0.725881 \t acc:  0.68\n",
      "6880 : loss:  0.78762686 \t acc:  0.66\n",
      "6920 : loss:  0.77183974 \t acc:  0.67\n",
      "6960 : loss:  0.6633917 \t acc:  0.68\n",
      "7000 : loss:  0.8680063 \t acc:  0.63\n",
      "7040 : loss:  0.61455286 \t acc:  0.74\n",
      "7080 : loss:  0.72071505 \t acc:  0.62\n",
      "7120 : loss:  0.7986257 \t acc:  0.68\n",
      "7160 : loss:  0.6498024 \t acc:  0.72\n",
      "7200 : loss:  0.71188486 \t acc:  0.7\n",
      "7240 : loss:  0.7607389 \t acc:  0.68\n",
      "7280 : loss:  0.74843836 \t acc:  0.73\n",
      "7320 : loss:  0.73486054 \t acc:  0.65\n",
      "7360 : loss:  0.6618262 \t acc:  0.74\n",
      "7400 : loss:  0.7164135 \t acc:  0.65\n",
      "7440 : loss:  0.5828856 \t acc:  0.74\n",
      "7480 : loss:  0.7060509 \t acc:  0.67\n",
      "7520 : loss:  0.71076864 \t acc:  0.71\n",
      "7560 : loss:  0.82407415 \t acc:  0.67\n",
      "7600 : loss:  0.8399729 \t acc:  0.64\n",
      "7640 : loss:  0.75467265 \t acc:  0.63\n",
      "7680 : loss:  0.63011396 \t acc:  0.71\n",
      "7720 : loss:  0.7101922 \t acc:  0.69\n",
      "7760 : loss:  0.8066399 \t acc:  0.64\n",
      "7800 : loss:  0.6260543 \t acc:  0.68\n",
      "7840 : loss:  0.7076019 \t acc:  0.67\n",
      "7880 : loss:  0.7196881 \t acc:  0.71\n",
      "7920 : loss:  0.7829367 \t acc:  0.63\n",
      "7960 : loss:  0.76594466 \t acc:  0.59\n",
      "8000 : loss:  0.8143702 \t acc:  0.63\n",
      "8040 : loss:  0.64674276 \t acc:  0.7\n",
      "8080 : loss:  0.6239314 \t acc:  0.75\n",
      "8120 : loss:  0.71972376 \t acc:  0.72\n",
      "8160 : loss:  0.7627245 \t acc:  0.71\n",
      "8200 : loss:  0.6594992 \t acc:  0.73\n",
      "8240 : loss:  0.5904142 \t acc:  0.76\n",
      "8280 : loss:  0.8044229 \t acc:  0.68\n",
      "8320 : loss:  0.74980307 \t acc:  0.69\n",
      "8360 : loss:  0.657775 \t acc:  0.69\n",
      "8400 : loss:  0.7211033 \t acc:  0.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8440 : loss:  0.7674645 \t acc:  0.61\n",
      "8480 : loss:  0.7177405 \t acc:  0.69\n",
      "8520 : loss:  0.6438629 \t acc:  0.68\n",
      "8560 : loss:  0.7979418 \t acc:  0.63\n",
      "8600 : loss:  0.5973513 \t acc:  0.76\n",
      "8640 : loss:  0.66694045 \t acc:  0.69\n",
      "8680 : loss:  0.69841367 \t acc:  0.72\n",
      "8720 : loss:  0.7008329 \t acc:  0.69\n",
      "8760 : loss:  0.82474077 \t acc:  0.66\n",
      "8800 : loss:  0.6996593 \t acc:  0.71\n",
      "8840 : loss:  0.6684941 \t acc:  0.68\n",
      "8880 : loss:  0.7225666 \t acc:  0.62\n",
      "8920 : loss:  0.80656683 \t acc:  0.69\n",
      "8960 : loss:  0.7595427 \t acc:  0.66\n",
      "\n",
      "Saving...\n",
      "saved to models/CNN/pretrained_model.ckpt-8965\n",
      "\n",
      "500 \t [356.30185772 257.015474   380.65589182]\n",
      "1 \tval accuracy:  0.67866 \t f_! score:  [0.71260372 0.51403095 0.76131178]\n",
      "\n",
      "9000 : loss:  0.72783256 \t acc:  0.65\n",
      "9040 : loss:  0.71123403 \t acc:  0.66\n",
      "9080 : loss:  0.77431226 \t acc:  0.6\n",
      "9120 : loss:  0.7903347 \t acc:  0.63\n",
      "9160 : loss:  0.85699713 \t acc:  0.54\n",
      "9200 : loss:  0.58055574 \t acc:  0.75\n",
      "9240 : loss:  0.77324384 \t acc:  0.6\n",
      "9280 : loss:  0.6952869 \t acc:  0.68\n",
      "9320 : loss:  0.7175858 \t acc:  0.65\n",
      "9360 : loss:  0.7252085 \t acc:  0.68\n",
      "9400 : loss:  0.5063465 \t acc:  0.83\n",
      "9440 : loss:  0.7831826 \t acc:  0.67\n",
      "9480 : loss:  0.7242229 \t acc:  0.69\n",
      "9520 : loss:  0.753264 \t acc:  0.66\n",
      "9560 : loss:  0.6521489 \t acc:  0.67\n",
      "9600 : loss:  0.76243883 \t acc:  0.63\n",
      "9640 : loss:  0.70086896 \t acc:  0.68\n",
      "9680 : loss:  0.7319534 \t acc:  0.73\n",
      "9720 : loss:  0.7971317 \t acc:  0.68\n",
      "9760 : loss:  0.6587718 \t acc:  0.69\n",
      "9800 : loss:  0.74178094 \t acc:  0.65\n",
      "9840 : loss:  0.82364583 \t acc:  0.58\n",
      "9880 : loss:  0.7799988 \t acc:  0.6\n",
      "9920 : loss:  0.712407 \t acc:  0.67\n",
      "9960 : loss:  0.68413466 \t acc:  0.71\n",
      "10000 : loss:  0.6044914 \t acc:  0.73\n",
      "10040 : loss:  0.62900007 \t acc:  0.72\n",
      "10080 : loss:  0.6282396 \t acc:  0.75\n",
      "10120 : loss:  0.74120855 \t acc:  0.66\n",
      "10160 : loss:  0.831682 \t acc:  0.62\n",
      "10200 : loss:  0.6892493 \t acc:  0.72\n",
      "10240 : loss:  0.69027907 \t acc:  0.72\n",
      "10280 : loss:  0.69051695 \t acc:  0.74\n",
      "10320 : loss:  0.5881713 \t acc:  0.77\n",
      "10360 : loss:  0.63233477 \t acc:  0.77\n",
      "10400 : loss:  0.58708626 \t acc:  0.74\n",
      "10440 : loss:  0.6917774 \t acc:  0.71\n",
      "10480 : loss:  0.68801767 \t acc:  0.68\n",
      "10520 : loss:  0.6435467 \t acc:  0.75\n",
      "10560 : loss:  0.7752813 \t acc:  0.67\n",
      "10600 : loss:  0.593147 \t acc:  0.79\n",
      "10640 : loss:  0.7687993 \t acc:  0.6\n",
      "10680 : loss:  0.65872216 \t acc:  0.67\n",
      "10720 : loss:  0.75400925 \t acc:  0.63\n",
      "10760 : loss:  0.8665283 \t acc:  0.55\n",
      "10800 : loss:  0.8470152 \t acc:  0.63\n",
      "10840 : loss:  0.65360594 \t acc:  0.71\n",
      "10880 : loss:  0.7726596 \t acc:  0.67\n",
      "10920 : loss:  0.7818442 \t acc:  0.64\n",
      "10960 : loss:  0.65216845 \t acc:  0.76\n",
      "11000 : loss:  0.5661457 \t acc:  0.75\n",
      "11040 : loss:  0.7516443 \t acc:  0.64\n",
      "11080 : loss:  0.6358333 \t acc:  0.72\n",
      "11120 : loss:  0.707101 \t acc:  0.71\n",
      "11160 : loss:  0.7846998 \t acc:  0.67\n",
      "11200 : loss:  0.7732261 \t acc:  0.72\n",
      "11240 : loss:  0.57231736 \t acc:  0.77\n",
      "11280 : loss:  0.72854114 \t acc:  0.69\n",
      "11320 : loss:  0.629228 \t acc:  0.73\n",
      "11360 : loss:  0.5715782 \t acc:  0.76\n",
      "11400 : loss:  0.7124495 \t acc:  0.68\n",
      "11440 : loss:  0.72845733 \t acc:  0.67\n",
      "11480 : loss:  0.5220716 \t acc:  0.81\n",
      "11520 : loss:  0.57057524 \t acc:  0.77\n",
      "11560 : loss:  0.70782715 \t acc:  0.63\n",
      "11600 : loss:  0.8326437 \t acc:  0.6\n",
      "11640 : loss:  0.7529449 \t acc:  0.69\n",
      "11680 : loss:  0.75107676 \t acc:  0.75\n",
      "11720 : loss:  0.6132862 \t acc:  0.68\n",
      "11760 : loss:  0.6604304 \t acc:  0.72\n",
      "11800 : loss:  0.62796223 \t acc:  0.69\n",
      "11840 : loss:  0.71987706 \t acc:  0.65\n",
      "11880 : loss:  0.7500641 \t acc:  0.68\n",
      "11920 : loss:  0.77380365 \t acc:  0.67\n",
      "11960 : loss:  0.6464988 \t acc:  0.69\n",
      "12000 : loss:  0.71440035 \t acc:  0.68\n",
      "12040 : loss:  0.8144359 \t acc:  0.69\n",
      "12080 : loss:  0.7522212 \t acc:  0.6\n",
      "12120 : loss:  0.66173315 \t acc:  0.69\n",
      "12160 : loss:  0.69611526 \t acc:  0.7\n",
      "12200 : loss:  0.72311217 \t acc:  0.68\n",
      "12240 : loss:  0.6509367 \t acc:  0.67\n",
      "12280 : loss:  0.7472348 \t acc:  0.69\n",
      "12320 : loss:  0.7639572 \t acc:  0.62\n",
      "12360 : loss:  0.640912 \t acc:  0.76\n",
      "12400 : loss:  0.73160493 \t acc:  0.71\n",
      "12440 : loss:  0.75642794 \t acc:  0.66\n",
      "12480 : loss:  0.6573391 \t acc:  0.62\n",
      "12520 : loss:  0.588039 \t acc:  0.79\n",
      "12560 : loss:  0.7448226 \t acc:  0.67\n",
      "12600 : loss:  0.6557839 \t acc:  0.75\n",
      "12640 : loss:  0.81017774 \t acc:  0.6\n",
      "12680 : loss:  0.8340466 \t acc:  0.65\n",
      "12720 : loss:  0.72236097 \t acc:  0.66\n",
      "12760 : loss:  0.6779786 \t acc:  0.68\n",
      "12800 : loss:  0.8061398 \t acc:  0.63\n",
      "12840 : loss:  0.7338974 \t acc:  0.67\n",
      "12880 : loss:  0.6703497 \t acc:  0.7\n",
      "12920 : loss:  0.7892251 \t acc:  0.69\n",
      "12960 : loss:  0.6943891 \t acc:  0.69\n",
      "13000 : loss:  0.6608261 \t acc:  0.7\n",
      "13040 : loss:  0.8240604 \t acc:  0.66\n",
      "13080 : loss:  0.6819498 \t acc:  0.68\n",
      "13120 : loss:  0.59432185 \t acc:  0.73\n",
      "13160 : loss:  0.76352173 \t acc:  0.7\n",
      "13200 : loss:  0.78417313 \t acc:  0.69\n",
      "13240 : loss:  0.81906974 \t acc:  0.6\n",
      "13280 : loss:  0.7014289 \t acc:  0.69\n",
      "13320 : loss:  0.67000353 \t acc:  0.69\n",
      "13360 : loss:  0.61734277 \t acc:  0.78\n",
      "13400 : loss:  0.73726445 \t acc:  0.68\n",
      "13440 : loss:  0.63800615 \t acc:  0.73\n",
      "\n",
      "Saving...\n",
      "saved to models/CNN/pretrained_model.ckpt-13448\n",
      "\n",
      "500 \t [371.34347453 240.03885198 384.22552244]\n",
      "2 \tval accuracy:  0.68812 \t f_! score:  [0.74268695 0.4800777  0.76845104]\n",
      "\n",
      "13480 : loss:  0.7134787 \t acc:  0.68\n",
      "13520 : loss:  0.6061712 \t acc:  0.72\n",
      "13560 : loss:  0.65901953 \t acc:  0.7\n",
      "13600 : loss:  0.75807714 \t acc:  0.66\n",
      "13640 : loss:  0.5830594 \t acc:  0.8\n",
      "13680 : loss:  0.6952458 \t acc:  0.7\n",
      "13720 : loss:  0.7037105 \t acc:  0.7\n",
      "13760 : loss:  0.7611963 \t acc:  0.63\n",
      "13800 : loss:  0.6449583 \t acc:  0.73\n",
      "13840 : loss:  0.621933 \t acc:  0.73\n",
      "13880 : loss:  0.57668555 \t acc:  0.73\n",
      "13920 : loss:  0.7291409 \t acc:  0.7\n",
      "13960 : loss:  0.6637241 \t acc:  0.7\n",
      "14000 : loss:  0.7881081 \t acc:  0.67\n",
      "14040 : loss:  0.7632003 \t acc:  0.6\n",
      "14080 : loss:  0.723995 \t acc:  0.67\n",
      "14120 : loss:  0.6157094 \t acc:  0.77\n",
      "14160 : loss:  0.76054156 \t acc:  0.67\n",
      "14200 : loss:  0.6845781 \t acc:  0.67\n",
      "14240 : loss:  0.610829 \t acc:  0.73\n",
      "14280 : loss:  0.71015364 \t acc:  0.66\n",
      "14320 : loss:  0.73136663 \t acc:  0.7\n",
      "14360 : loss:  0.8053815 \t acc:  0.58\n",
      "14400 : loss:  0.6462721 \t acc:  0.68\n",
      "14440 : loss:  0.6938136 \t acc:  0.72\n",
      "14480 : loss:  0.5604118 \t acc:  0.74\n",
      "14520 : loss:  0.80301607 \t acc:  0.66\n",
      "14560 : loss:  0.68375915 \t acc:  0.7\n",
      "14600 : loss:  0.7336018 \t acc:  0.66\n",
      "14640 : loss:  0.8023288 \t acc:  0.66\n",
      "14680 : loss:  0.733207 \t acc:  0.73\n",
      "14720 : loss:  0.6858791 \t acc:  0.66\n",
      "14760 : loss:  0.6367561 \t acc:  0.7\n",
      "14800 : loss:  0.6903531 \t acc:  0.67\n",
      "14840 : loss:  0.6954409 \t acc:  0.71\n",
      "14880 : loss:  0.7217235 \t acc:  0.66\n",
      "14920 : loss:  0.67072815 \t acc:  0.67\n",
      "14960 : loss:  0.6791679 \t acc:  0.69\n",
      "15000 : loss:  0.6929871 \t acc:  0.7\n",
      "15040 : loss:  0.62295085 \t acc:  0.73\n",
      "15080 : loss:  0.82386976 \t acc:  0.6\n",
      "15120 : loss:  0.64905035 \t acc:  0.73\n",
      "15160 : loss:  0.6406839 \t acc:  0.72\n",
      "15200 : loss:  0.600663 \t acc:  0.72\n",
      "15240 : loss:  0.7479292 \t acc:  0.67\n",
      "15280 : loss:  0.75416404 \t acc:  0.62\n",
      "15320 : loss:  0.5386168 \t acc:  0.86\n",
      "15360 : loss:  0.793551 \t acc:  0.65\n",
      "15400 : loss:  0.77513117 \t acc:  0.57\n",
      "15440 : loss:  0.8012399 \t acc:  0.67\n",
      "15480 : loss:  0.64245534 \t acc:  0.7\n",
      "15520 : loss:  0.67355406 \t acc:  0.72\n",
      "15560 : loss:  0.6313087 \t acc:  0.74\n",
      "15600 : loss:  0.70855814 \t acc:  0.72\n",
      "15640 : loss:  0.706784 \t acc:  0.68\n",
      "15680 : loss:  0.51412725 \t acc:  0.8\n",
      "15720 : loss:  0.6627876 \t acc:  0.7\n",
      "15760 : loss:  0.7094287 \t acc:  0.7\n",
      "15800 : loss:  0.59223676 \t acc:  0.75\n",
      "15840 : loss:  0.66442454 \t acc:  0.65\n",
      "15880 : loss:  0.66982794 \t acc:  0.74\n",
      "15920 : loss:  0.62626106 \t acc:  0.73\n",
      "15960 : loss:  0.5710004 \t acc:  0.78\n",
      "16000 : loss:  0.7413036 \t acc:  0.71\n",
      "16040 : loss:  0.6011371 \t acc:  0.74\n",
      "16080 : loss:  0.6169716 \t acc:  0.74\n",
      "16120 : loss:  0.63693416 \t acc:  0.74\n",
      "16160 : loss:  0.65086466 \t acc:  0.66\n",
      "16200 : loss:  0.71241546 \t acc:  0.67\n",
      "16240 : loss:  0.723955 \t acc:  0.68\n",
      "16280 : loss:  0.6304482 \t acc:  0.77\n",
      "16320 : loss:  0.77771074 \t acc:  0.63\n",
      "16360 : loss:  0.7228849 \t acc:  0.72\n",
      "16400 : loss:  0.7057079 \t acc:  0.69\n",
      "16440 : loss:  0.5621178 \t acc:  0.76\n",
      "16480 : loss:  0.51967436 \t acc:  0.81\n",
      "16520 : loss:  0.6471427 \t acc:  0.76\n",
      "16560 : loss:  0.62302595 \t acc:  0.75\n",
      "16600 : loss:  0.6933274 \t acc:  0.72\n",
      "16640 : loss:  0.7867665 \t acc:  0.65\n",
      "16680 : loss:  0.7310211 \t acc:  0.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16720 : loss:  0.7945753 \t acc:  0.65\n",
      "16760 : loss:  0.69122887 \t acc:  0.65\n",
      "16800 : loss:  0.609894 \t acc:  0.74\n",
      "16840 : loss:  0.7462967 \t acc:  0.7\n",
      "16880 : loss:  0.7855639 \t acc:  0.65\n",
      "16920 : loss:  0.64767706 \t acc:  0.73\n",
      "16960 : loss:  0.79307646 \t acc:  0.66\n",
      "17000 : loss:  0.7191674 \t acc:  0.65\n",
      "17040 : loss:  0.638483 \t acc:  0.68\n",
      "17080 : loss:  0.7572937 \t acc:  0.65\n",
      "17120 : loss:  0.6337648 \t acc:  0.76\n",
      "17160 : loss:  0.60712177 \t acc:  0.73\n",
      "17200 : loss:  0.7141955 \t acc:  0.67\n",
      "17240 : loss:  0.72683305 \t acc:  0.67\n",
      "17280 : loss:  0.703426 \t acc:  0.74\n",
      "17320 : loss:  0.6765646 \t acc:  0.65\n",
      "17360 : loss:  0.6515595 \t acc:  0.73\n",
      "17400 : loss:  0.7277893 \t acc:  0.65\n",
      "17440 : loss:  0.7110236 \t acc:  0.7\n",
      "17480 : loss:  0.69649625 \t acc:  0.62\n",
      "17520 : loss:  0.73562086 \t acc:  0.66\n",
      "17560 : loss:  0.70512635 \t acc:  0.69\n",
      "17600 : loss:  0.7348933 \t acc:  0.65\n",
      "17640 : loss:  0.67764175 \t acc:  0.7\n",
      "17680 : loss:  0.8204828 \t acc:  0.64\n",
      "17720 : loss:  0.75531244 \t acc:  0.61\n",
      "17760 : loss:  0.77086174 \t acc:  0.68\n",
      "17800 : loss:  0.72933584 \t acc:  0.67\n",
      "17840 : loss:  0.7436641 \t acc:  0.68\n",
      "17880 : loss:  0.77760094 \t acc:  0.67\n",
      "17920 : loss:  0.7425322 \t acc:  0.68\n",
      "\n",
      "Saving...\n",
      "saved to models/CNN/pretrained_model.ckpt-17931\n",
      "\n",
      "500 \t [367.54979912 256.40245132 384.81958209]\n",
      "3 \tval accuracy:  0.69023997 \t f_! score:  [0.7350996  0.5128049  0.76963916]\n",
      "\n",
      "500 \t [367.54979912 256.40245132 384.81958209]\n",
      "500 \t [356.65743552 267.84272116 391.36016889]\n",
      "500 \t [382.18221483 250.34635059 381.11776724]\n",
      "500 \t [18794. 13823. 17383.]\n",
      "--- Test   Review ---\n",
      "0.69023997\n",
      "f1:  [0.7350996  0.5128049  0.76963916]\n",
      "65730\n",
      "Vorsicht! es muss vorher ein  CNN  Netz abgespeichert worden sein\n",
      "\n",
      "Restoring...\n",
      "models/CNN/pretrained_model.ckpt-17931\n",
      "INFO:tensorflow:Restoring parameters from models/CNN/pretrained_model.ckpt-17931\n",
      "acc:  0.3\n",
      "acc:  0.37\n",
      "acc:  0.3\n",
      "acc:  0.32\n",
      "acc:  0.32\n",
      "acc:  0.3\n",
      "acc:  0.24\n",
      "acc:  0.26\n",
      "acc:  0.33\n",
      "acc:  0.3\n",
      "acc:  0.39\n",
      "acc:  0.29\n",
      "acc:  0.37\n",
      "acc:  0.37\n",
      "acc:  0.32\n",
      "acc:  0.3\n",
      "acc:  0.38\n",
      "acc:  0.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yannik/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.35\n",
      "acc:  0.29\n",
      "acc:  0.35\n",
      "acc:  0.32\n",
      "acc:  0.42\n",
      "acc:  0.33\n",
      "acc:  0.27\n",
      "acc:  0.36\n",
      "acc:  0.34\n",
      "acc:  0.35\n",
      "acc:  0.29\n",
      "acc:  0.28\n",
      "acc:  0.41\n",
      "acc:  0.37\n",
      "acc:  0.28\n",
      "acc:  0.28\n",
      "acc:  0.27\n",
      "acc:  0.34\n",
      "acc:  0.31\n",
      "acc:  0.38\n",
      "acc:  0.39\n",
      "acc:  0.36\n",
      "acc:  0.34\n",
      "acc:  0.34\n",
      "acc:  0.3\n",
      "acc:  0.33\n",
      "acc:  0.25\n",
      "acc:  0.35\n",
      "acc:  0.36\n",
      "acc:  0.36\n",
      "acc:  0.26\n",
      "acc:  0.34\n",
      "acc:  0.42\n",
      "acc:  0.35\n",
      "acc:  0.31\n",
      "acc:  0.38\n",
      "acc:  0.22\n",
      "acc:  0.43\n",
      "acc:  0.33\n",
      "acc:  0.27\n",
      "acc:  0.34\n",
      "acc:  0.34\n",
      "acc:  0.31\n",
      "acc:  0.31\n",
      "acc:  0.39\n",
      "acc:  0.26\n",
      "acc:  0.3\n",
      "acc:  0.31\n",
      "acc:  0.37\n",
      "acc:  0.29\n",
      "acc:  0.41\n",
      "acc:  0.35\n",
      "acc:  0.36\n",
      "acc:  0.29\n",
      "acc:  0.32\n",
      "acc:  0.36\n",
      "acc:  0.43\n",
      "acc:  0.31\n",
      "acc:  0.3\n",
      "acc:  0.35\n",
      "acc:  0.32\n",
      "acc:  0.37\n",
      "acc:  0.29\n",
      "acc:  0.35\n",
      "acc:  0.38\n",
      "acc:  0.38\n",
      "acc:  0.31\n",
      "acc:  0.29\n",
      "acc:  0.32\n",
      "acc:  0.35\n",
      "acc:  0.41\n",
      "acc:  0.4\n",
      "acc:  0.41\n",
      "acc:  0.3\n",
      "acc:  0.32\n",
      "acc:  0.33\n",
      "acc:  0.36\n",
      "acc:  0.33\n",
      "acc:  0.34\n",
      "acc:  0.36\n",
      "acc:  0.38\n",
      "acc:  0.39\n",
      "\n",
      "100 \t [49.99785421  0.          0.        ]\n",
      "val accuracy:  0.3348 \t f_! score:  [0.49997854 0.         0.        ]\n",
      "\n",
      "100 \t [49.99785421  0.          0.        ]\n",
      "100 \t [33.48  0.    0.  ]\n",
      "100 \t [100.   0.   0.]\n",
      "100 \t [3348. 3281. 3371.]\n",
      "---just Test  Twitter ---\n",
      "0.3348\n",
      "f1:  [0.49997854 0.         0.        ]\n",
      "--- 5.246291160583496 seconds ---\n",
      "65730\n",
      "Vorsicht! es muss vorher ein  CNN  Netz abgespeichert worden sein\n",
      "\n",
      "Restoring...\n",
      "models/CNN/pretrained_model.ckpt-17931\n",
      "INFO:tensorflow:Restoring parameters from models/CNN/pretrained_model.ckpt-17931\n",
      "0 : loss:  1.1363177 \t acc:  0.26\n",
      "2 : loss:  1.0871075 \t acc:  0.37\n",
      "4 : loss:  1.0795463 \t acc:  0.4\n",
      "6 : loss:  1.0948797 \t acc:  0.4\n",
      "8 : loss:  1.0927284 \t acc:  0.38\n",
      "10 : loss:  1.102207 \t acc:  0.31\n",
      "12 : loss:  1.0905193 \t acc:  0.38\n",
      "14 : loss:  1.0901093 \t acc:  0.4\n",
      "16 : loss:  1.0888169 \t acc:  0.39\n",
      "18 : loss:  1.0891314 \t acc:  0.41\n",
      "20 : loss:  1.0836968 \t acc:  0.42\n",
      "22 : loss:  1.0677435 \t acc:  0.41\n",
      "24 : loss:  1.0901747 \t acc:  0.34\n",
      "26 : loss:  1.0902495 \t acc:  0.34\n",
      "28 : loss:  1.071701 \t acc:  0.45\n",
      "30 : loss:  1.0718687 \t acc:  0.39\n",
      "32 : loss:  1.0953387 \t acc:  0.36\n",
      "34 : loss:  1.090693 \t acc:  0.37\n",
      "36 : loss:  1.071589 \t acc:  0.44\n",
      "38 : loss:  1.0695238 \t acc:  0.43\n",
      "40 : loss:  1.0573338 \t acc:  0.44\n",
      "42 : loss:  1.0666649 \t acc:  0.42\n",
      "44 : loss:  1.0586047 \t acc:  0.43\n",
      "46 : loss:  1.0863813 \t acc:  0.34\n",
      "48 : loss:  1.0947304 \t acc:  0.33\n",
      "50 : loss:  1.0631117 \t acc:  0.38\n",
      "52 : loss:  1.0538865 \t acc:  0.43\n",
      "54 : loss:  1.0625167 \t acc:  0.41\n",
      "56 : loss:  1.0774671 \t acc:  0.37\n",
      "58 : loss:  1.0456043 \t acc:  0.46\n",
      "60 : loss:  1.0674487 \t acc:  0.41\n",
      "62 : loss:  1.0566611 \t acc:  0.46\n",
      "64 : loss:  1.103161 \t acc:  0.32\n",
      "66 : loss:  1.0594136 \t acc:  0.42\n",
      "68 : loss:  1.0897261 \t acc:  0.32\n",
      "70 : loss:  1.0938317 \t acc:  0.38\n",
      "72 : loss:  1.0535207 \t acc:  0.54\n",
      "74 : loss:  1.0604631 \t acc:  0.39\n",
      "76 : loss:  1.1195854 \t acc:  0.38\n",
      "78 : loss:  1.0524304 \t acc:  0.42\n",
      "80 : loss:  1.1304566 \t acc:  0.29\n",
      "82 : loss:  1.0466999 \t acc:  0.43\n",
      "84 : loss:  1.0801558 \t acc:  0.39\n",
      "86 : loss:  1.0707971 \t acc:  0.39\n",
      "88 : loss:  1.0575083 \t acc:  0.39\n",
      "90 : loss:  1.0436646 \t acc:  0.43\n",
      "92 : loss:  1.0355139 \t acc:  0.44\n",
      "94 : loss:  1.0480666 \t acc:  0.4\n",
      "96 : loss:  1.0917982 \t acc:  0.35\n",
      "98 : loss:  1.0315642 \t acc:  0.47\n",
      "100 : loss:  1.0699252 \t acc:  0.44\n",
      "102 : loss:  1.0674796 \t acc:  0.42\n",
      "104 : loss:  1.0428298 \t acc:  0.47\n",
      "106 : loss:  1.075255 \t acc:  0.42\n",
      "108 : loss:  1.0739708 \t acc:  0.43\n",
      "110 : loss:  1.0407075 \t acc:  0.46\n",
      "112 : loss:  1.0543689 \t acc:  0.37\n",
      "114 : loss:  1.0096827 \t acc:  0.51\n",
      "116 : loss:  1.0468446 \t acc:  0.4\n",
      "118 : loss:  1.0847435 \t acc:  0.35\n",
      "120 : loss:  1.040186 \t acc:  0.44\n",
      "122 : loss:  1.077386 \t acc:  0.4\n",
      "124 : loss:  1.0061518 \t acc:  0.54\n",
      "126 : loss:  1.0164548 \t acc:  0.53\n",
      "128 : loss:  1.0683215 \t acc:  0.4\n",
      "130 : loss:  1.0598178 \t acc:  0.41\n",
      "132 : loss:  1.055165 \t acc:  0.41\n",
      "134 : loss:  1.0623208 \t acc:  0.43\n",
      "136 : loss:  1.0716702 \t acc:  0.4\n",
      "138 : loss:  1.0815171 \t acc:  0.4\n",
      "140 : loss:  1.0794058 \t acc:  0.4\n",
      "142 : loss:  1.0700139 \t acc:  0.38\n",
      "144 : loss:  1.0341669 \t acc:  0.49\n",
      "146 : loss:  1.0303547 \t acc:  0.45\n",
      "148 : loss:  1.0513276 \t acc:  0.41\n",
      "150 : loss:  1.0466495 \t acc:  0.44\n",
      "152 : loss:  1.0570021 \t acc:  0.42\n",
      "154 : loss:  1.0488188 \t acc:  0.43\n",
      "156 : loss:  1.024341 \t acc:  0.46\n",
      "158 : loss:  1.0556934 \t acc:  0.47\n",
      "160 : loss:  1.050462 \t acc:  0.39\n",
      "162 : loss:  1.0556726 \t acc:  0.43\n",
      "164 : loss:  1.0648823 \t acc:  0.44\n",
      "166 : loss:  1.0539738 \t acc:  0.42\n",
      "168 : loss:  1.0227957 \t acc:  0.47\n",
      "170 : loss:  1.0892076 \t acc:  0.35\n",
      "172 : loss:  1.0355777 \t acc:  0.46\n",
      "174 : loss:  1.0597634 \t acc:  0.44\n",
      "176 : loss:  1.0889115 \t acc:  0.39\n",
      "178 : loss:  1.074041 \t acc:  0.39\n",
      "180 : loss:  1.0391217 \t acc:  0.42\n",
      "182 : loss:  1.0989965 \t acc:  0.41\n",
      "184 : loss:  1.074101 \t acc:  0.43\n",
      "186 : loss:  1.0912609 \t acc:  0.33\n",
      "188 : loss:  1.0769162 \t acc:  0.36\n",
      "190 : loss:  1.0450013 \t acc:  0.42\n",
      "192 : loss:  1.1088237 \t acc:  0.28\n",
      "194 : loss:  1.066642 \t acc:  0.44\n",
      "196 : loss:  1.0354968 \t acc:  0.42\n",
      "198 : loss:  1.0351719 \t acc:  0.49\n",
      "200 : loss:  1.0553365 \t acc:  0.45\n",
      "202 : loss:  1.0447106 \t acc:  0.42\n",
      "204 : loss:  1.0642624 \t acc:  0.47\n",
      "206 : loss:  1.0480618 \t acc:  0.46\n",
      "208 : loss:  1.064172 \t acc:  0.45\n",
      "210 : loss:  1.0309805 \t acc:  0.43\n",
      "212 : loss:  1.0556244 \t acc:  0.37\n",
      "214 : loss:  1.0621452 \t acc:  0.4\n",
      "216 : loss:  1.0486305 \t acc:  0.42\n",
      "218 : loss:  1.0176489 \t acc:  0.48\n",
      "220 : loss:  1.0467913 \t acc:  0.44\n",
      "222 : loss:  1.0952897 \t acc:  0.34\n",
      "224 : loss:  1.059874 \t acc:  0.46\n",
      "226 : loss:  1.0611789 \t acc:  0.4\n",
      "228 : loss:  1.0897883 \t acc:  0.36\n",
      "230 : loss:  1.0662999 \t acc:  0.47\n",
      "232 : loss:  0.9807987 \t acc:  0.5\n",
      "234 : loss:  1.0108099 \t acc:  0.48\n",
      "236 : loss:  1.067791 \t acc:  0.43\n",
      "238 : loss:  1.063361 \t acc:  0.47\n",
      "240 : loss:  1.0239136 \t acc:  0.46\n",
      "242 : loss:  1.0512959 \t acc:  0.44\n",
      "244 : loss:  1.0334287 \t acc:  0.44\n",
      "246 : loss:  1.0661334 \t acc:  0.46\n",
      "248 : loss:  1.0572146 \t acc:  0.4\n",
      "250 : loss:  1.081482 \t acc:  0.42\n",
      "252 : loss:  1.0722954 \t acc:  0.41\n",
      "254 : loss:  1.0437338 \t acc:  0.46\n",
      "256 : loss:  1.0452294 \t acc:  0.38\n",
      "258 : loss:  1.0493126 \t acc:  0.43\n",
      "260 : loss:  1.0715704 \t acc:  0.41\n",
      "262 : loss:  1.0665857 \t acc:  0.39\n",
      "264 : loss:  1.0278735 \t acc:  0.5\n",
      "266 : loss:  1.074774 \t acc:  0.4\n",
      "268 : loss:  1.0837874 \t acc:  0.39\n",
      "270 : loss:  1.0768692 \t acc:  0.36\n",
      "272 : loss:  1.0747881 \t acc:  0.36\n",
      "274 : loss:  1.0720568 \t acc:  0.32\n",
      "276 : loss:  1.0206507 \t acc:  0.48\n",
      "278 : loss:  1.0452809 \t acc:  0.43\n",
      "280 : loss:  1.0397923 \t acc:  0.42\n",
      "282 : loss:  1.0449816 \t acc:  0.37\n",
      "284 : loss:  1.0538774 \t acc:  0.45\n",
      "286 : loss:  1.0307258 \t acc:  0.47\n",
      "288 : loss:  1.0469974 \t acc:  0.39\n",
      "290 : loss:  1.0994014 \t acc:  0.37\n",
      "292 : loss:  1.0707227 \t acc:  0.45\n",
      "294 : loss:  1.013973 \t acc:  0.49\n",
      "296 : loss:  1.0354092 \t acc:  0.43\n",
      "298 : loss:  1.0668207 \t acc:  0.32\n",
      "300 : loss:  1.0732162 \t acc:  0.5\n",
      "302 : loss:  1.0390253 \t acc:  0.39\n",
      "304 : loss:  1.0894643 \t acc:  0.4\n",
      "306 : loss:  1.086836 \t acc:  0.39\n",
      "308 : loss:  0.99961716 \t acc:  0.48\n",
      "310 : loss:  1.0447686 \t acc:  0.41\n",
      "312 : loss:  1.0792004 \t acc:  0.36\n",
      "314 : loss:  1.0397654 \t acc:  0.39\n",
      "316 : loss:  1.0766209 \t acc:  0.37\n",
      "318 : loss:  1.0881016 \t acc:  0.39\n",
      "320 : loss:  1.1033076 \t acc:  0.34\n",
      "322 : loss:  1.0640492 \t acc:  0.41\n",
      "324 : loss:  1.0324521 \t acc:  0.47\n",
      "326 : loss:  1.0789154 \t acc:  0.38\n",
      "328 : loss:  1.0670991 \t acc:  0.4\n",
      "330 : loss:  0.9902009 \t acc:  0.46\n",
      "332 : loss:  1.0403373 \t acc:  0.48\n",
      "334 : loss:  1.032487 \t acc:  0.48\n",
      "336 : loss:  1.0300415 \t acc:  0.42\n",
      "338 : loss:  1.0050263 \t acc:  0.44\n",
      "340 : loss:  1.0818092 \t acc:  0.35\n",
      "342 : loss:  1.0716331 \t acc:  0.42\n",
      "344 : loss:  1.0340321 \t acc:  0.46\n",
      "346 : loss:  0.9783731 \t acc:  0.52\n",
      "348 : loss:  1.1026201 \t acc:  0.41\n",
      "350 : loss:  1.1168865 \t acc:  0.45\n",
      "352 : loss:  1.0204746 \t acc:  0.41\n",
      "354 : loss:  1.0642196 \t acc:  0.44\n",
      "356 : loss:  1.0701209 \t acc:  0.39\n",
      "358 : loss:  1.0634097 \t acc:  0.35\n",
      "360 : loss:  1.0503255 \t acc:  0.4\n",
      "362 : loss:  0.9898543 \t acc:  0.49\n",
      "364 : loss:  1.0298772 \t acc:  0.44\n",
      "366 : loss:  1.0755091 \t acc:  0.29\n",
      "368 : loss:  1.0531658 \t acc:  0.49\n",
      "370 : loss:  1.0434736 \t acc:  0.43\n",
      "acc:  0.43\n",
      "acc:  0.44\n",
      "acc:  0.37\n",
      "acc:  0.5\n",
      "acc:  0.47\n",
      "acc:  0.45\n",
      "acc:  0.43\n",
      "acc:  0.35\n",
      "acc:  0.42\n",
      "acc:  0.45\n",
      "acc:  0.36\n",
      "acc:  0.47\n",
      "acc:  0.48\n",
      "acc:  0.46\n",
      "acc:  0.47\n",
      "acc:  0.38\n",
      "acc:  0.49\n",
      "acc:  0.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.45\n",
      "acc:  0.37\n",
      "acc:  0.47\n",
      "acc:  0.34\n",
      "acc:  0.47\n",
      "acc:  0.51\n",
      "acc:  0.47\n",
      "acc:  0.37\n",
      "acc:  0.42\n",
      "acc:  0.42\n",
      "acc:  0.39\n",
      "acc:  0.44\n",
      "acc:  0.48\n",
      "acc:  0.25\n",
      "acc:  0.46\n",
      "acc:  0.37\n",
      "acc:  0.5\n",
      "acc:  0.44\n",
      "acc:  0.5\n",
      "acc:  0.33\n",
      "acc:  0.4\n",
      "acc:  0.44\n",
      "acc:  0.35\n",
      "acc:  0.36\n",
      "acc:  0.42\n",
      "acc:  0.38\n",
      "acc:  0.4\n",
      "acc:  0.4\n",
      "acc:  0.42\n",
      "acc:  0.4\n",
      "acc:  0.42\n",
      "acc:  0.47\n",
      "acc:  0.42\n",
      "acc:  0.41\n",
      "acc:  0.45\n",
      "acc:  0.48\n",
      "acc:  0.42\n",
      "acc:  0.47\n",
      "acc:  0.47\n",
      "acc:  0.46\n",
      "acc:  0.41\n",
      "acc:  0.42\n",
      "acc:  0.47\n",
      "acc:  0.37\n",
      "acc:  0.41\n",
      "acc:  0.49\n",
      "acc:  0.53\n",
      "acc:  0.51\n",
      "acc:  0.49\n",
      "acc:  0.35\n",
      "acc:  0.3\n",
      "acc:  0.44\n",
      "acc:  0.33\n",
      "acc:  0.38\n",
      "acc:  0.33\n",
      "acc:  0.39\n",
      "acc:  0.52\n",
      "acc:  0.52\n",
      "acc:  0.47\n",
      "acc:  0.41\n",
      "acc:  0.41\n",
      "acc:  0.44\n",
      "acc:  0.42\n",
      "acc:  0.44\n",
      "acc:  0.4\n",
      "acc:  0.43\n",
      "acc:  0.4\n",
      "acc:  0.4\n",
      "acc:  0.45\n",
      "acc:  0.48\n",
      "acc:  0.48\n",
      "acc:  0.43\n",
      "acc:  0.31\n",
      "acc:  0.44\n",
      "acc:  0.48\n",
      "acc:  0.44\n",
      "acc:  0.43\n",
      "acc:  0.4\n",
      "acc:  0.4\n",
      "acc:  0.5\n",
      "acc:  0.37\n",
      "acc:  0.39\n",
      "\n",
      "100 \t [50.29594668 50.92468509  0.88422258]\n",
      "0 \tval accuracy:  0.42520002 \t f_! score:  [0.50295947 0.50924685 0.00884223]\n",
      "\n",
      "100 \t [50.29594668 50.92468509  0.88422258]\n",
      "100 \t [48.47964357 39.06225037 13.83333333]\n",
      "100 \t [53.06485382 74.43189408  0.45883981]\n",
      "100 \t [3296. 3333. 3371.]\n",
      "---train_last_layer Test  Twitter ---\n",
      "0.42520002\n",
      "f1:  [0.50295947 0.50924685 0.00884223]\n",
      "--- 29.847007751464844 seconds ---\n",
      "\n",
      "\n",
      "  Twitter\n",
      "65730\n",
      "\n",
      "Restoring...\n",
      "models/CNN/pretrained_model.ckpt-17931\n",
      "INFO:tensorflow:Restoring parameters from models/CNN/pretrained_model.ckpt-17931\n",
      "---init ready   CNN ---\n",
      "\n",
      "Saving...\n",
      "saved to models/CNN/pretrained_model.ckpt-0\n",
      "0 : loss:  1.823013 \t acc:  0.28\n",
      "2 : loss:  1.1230767 \t acc:  0.33\n",
      "4 : loss:  1.1102923 \t acc:  0.25\n",
      "6 : loss:  1.098885 \t acc:  0.31\n",
      "8 : loss:  1.0984883 \t acc:  0.28\n",
      "10 : loss:  1.0772027 \t acc:  0.47\n",
      "12 : loss:  1.0856456 \t acc:  0.45\n",
      "14 : loss:  1.0862688 \t acc:  0.33\n",
      "16 : loss:  1.0827261 \t acc:  0.37\n",
      "18 : loss:  1.0391577 \t acc:  0.52\n",
      "20 : loss:  1.0257076 \t acc:  0.55\n",
      "22 : loss:  1.0520902 \t acc:  0.45\n",
      "24 : loss:  0.9986509 \t acc:  0.5\n",
      "26 : loss:  0.99773943 \t acc:  0.55\n",
      "28 : loss:  0.9807488 \t acc:  0.5\n",
      "30 : loss:  0.9951447 \t acc:  0.5\n",
      "32 : loss:  0.9871148 \t acc:  0.5\n",
      "34 : loss:  0.98543626 \t acc:  0.55\n",
      "36 : loss:  1.1132046 \t acc:  0.37\n",
      "38 : loss:  1.032706 \t acc:  0.45\n",
      "40 : loss:  1.0215985 \t acc:  0.52\n",
      "42 : loss:  0.96368897 \t acc:  0.57\n",
      "44 : loss:  1.0540209 \t acc:  0.41\n",
      "46 : loss:  1.0091941 \t acc:  0.51\n",
      "48 : loss:  0.99531955 \t acc:  0.55\n",
      "50 : loss:  1.140557 \t acc:  0.31\n",
      "52 : loss:  0.98026407 \t acc:  0.51\n",
      "54 : loss:  0.9987872 \t acc:  0.47\n",
      "56 : loss:  1.0677912 \t acc:  0.37\n",
      "58 : loss:  1.009635 \t acc:  0.51\n",
      "60 : loss:  1.033522 \t acc:  0.49\n",
      "62 : loss:  1.0591353 \t acc:  0.36\n",
      "64 : loss:  1.0943067 \t acc:  0.38\n",
      "66 : loss:  0.996353 \t acc:  0.54\n",
      "68 : loss:  0.9528414 \t acc:  0.49\n",
      "70 : loss:  1.0199913 \t acc:  0.46\n",
      "72 : loss:  1.0235271 \t acc:  0.48\n",
      "74 : loss:  1.0592368 \t acc:  0.41\n",
      "76 : loss:  0.98840284 \t acc:  0.47\n",
      "78 : loss:  0.99523103 \t acc:  0.49\n",
      "80 : loss:  0.9482677 \t acc:  0.52\n",
      "82 : loss:  0.947606 \t acc:  0.52\n",
      "84 : loss:  0.96540886 \t acc:  0.51\n",
      "86 : loss:  1.0179094 \t acc:  0.5\n",
      "88 : loss:  0.89045316 \t acc:  0.58\n",
      "90 : loss:  0.9303518 \t acc:  0.53\n",
      "92 : loss:  1.0027531 \t acc:  0.49\n",
      "94 : loss:  0.9631016 \t acc:  0.41\n",
      "96 : loss:  0.95028013 \t acc:  0.52\n",
      "98 : loss:  0.9428133 \t acc:  0.58\n",
      "100 : loss:  1.0009102 \t acc:  0.48\n",
      "102 : loss:  0.9695597 \t acc:  0.44\n",
      "104 : loss:  1.012758 \t acc:  0.52\n",
      "106 : loss:  0.9237679 \t acc:  0.52\n",
      "108 : loss:  0.9793584 \t acc:  0.42\n",
      "110 : loss:  0.93527985 \t acc:  0.51\n",
      "112 : loss:  0.9702398 \t acc:  0.45\n",
      "114 : loss:  1.0428401 \t acc:  0.41\n",
      "116 : loss:  0.9429607 \t acc:  0.41\n",
      "118 : loss:  1.0993453 \t acc:  0.45\n",
      "120 : loss:  0.9850289 \t acc:  0.51\n",
      "122 : loss:  0.9606948 \t acc:  0.52\n",
      "124 : loss:  0.98371243 \t acc:  0.53\n",
      "126 : loss:  0.9924433 \t acc:  0.49\n",
      "128 : loss:  0.94155395 \t acc:  0.52\n",
      "130 : loss:  0.9083451 \t acc:  0.57\n",
      "132 : loss:  0.98628885 \t acc:  0.51\n",
      "134 : loss:  0.9759138 \t acc:  0.47\n",
      "136 : loss:  0.9977573 \t acc:  0.46\n",
      "138 : loss:  0.96362436 \t acc:  0.53\n",
      "140 : loss:  0.93354094 \t acc:  0.53\n",
      "142 : loss:  0.90713227 \t acc:  0.53\n",
      "144 : loss:  0.91069794 \t acc:  0.53\n",
      "146 : loss:  0.95588547 \t acc:  0.59\n",
      "148 : loss:  0.95009637 \t acc:  0.47\n",
      "150 : loss:  0.9312185 \t acc:  0.47\n",
      "152 : loss:  0.97577035 \t acc:  0.47\n",
      "154 : loss:  0.916781 \t acc:  0.6\n",
      "156 : loss:  0.86304414 \t acc:  0.51\n",
      "158 : loss:  0.91518515 \t acc:  0.56\n",
      "160 : loss:  0.93072975 \t acc:  0.45\n",
      "162 : loss:  0.9198336 \t acc:  0.52\n",
      "164 : loss:  0.9429387 \t acc:  0.55\n",
      "166 : loss:  0.9725097 \t acc:  0.48\n",
      "168 : loss:  0.9270997 \t acc:  0.56\n",
      "170 : loss:  0.88309294 \t acc:  0.58\n",
      "172 : loss:  0.8714723 \t acc:  0.59\n",
      "174 : loss:  0.9526269 \t acc:  0.5\n",
      "176 : loss:  0.9909682 \t acc:  0.51\n",
      "178 : loss:  0.988456 \t acc:  0.47\n",
      "180 : loss:  0.87638545 \t acc:  0.57\n",
      "182 : loss:  0.9335114 \t acc:  0.49\n",
      "184 : loss:  0.99721503 \t acc:  0.43\n",
      "186 : loss:  0.8527345 \t acc:  0.58\n",
      "188 : loss:  0.9163502 \t acc:  0.55\n",
      "190 : loss:  1.0064496 \t acc:  0.49\n",
      "192 : loss:  0.96074414 \t acc:  0.41\n",
      "194 : loss:  0.9576015 \t acc:  0.52\n",
      "196 : loss:  0.9185097 \t acc:  0.59\n",
      "198 : loss:  0.8797847 \t acc:  0.58\n",
      "200 : loss:  0.9413142 \t acc:  0.51\n",
      "202 : loss:  0.9504831 \t acc:  0.54\n",
      "204 : loss:  0.926844 \t acc:  0.58\n",
      "206 : loss:  0.99555075 \t acc:  0.51\n",
      "208 : loss:  0.89203244 \t acc:  0.51\n",
      "210 : loss:  0.8481592 \t acc:  0.59\n",
      "212 : loss:  0.9121326 \t acc:  0.56\n",
      "214 : loss:  0.97295487 \t acc:  0.51\n",
      "216 : loss:  0.9713591 \t acc:  0.46\n",
      "218 : loss:  0.98623025 \t acc:  0.45\n",
      "220 : loss:  0.9313358 \t acc:  0.54\n",
      "222 : loss:  0.8922467 \t acc:  0.57\n",
      "224 : loss:  0.9088499 \t acc:  0.55\n",
      "226 : loss:  0.9083127 \t acc:  0.55\n",
      "228 : loss:  0.9717972 \t acc:  0.53\n",
      "230 : loss:  0.9689943 \t acc:  0.44\n",
      "232 : loss:  0.91074866 \t acc:  0.58\n",
      "234 : loss:  0.9384676 \t acc:  0.57\n",
      "236 : loss:  0.9480648 \t acc:  0.45\n",
      "238 : loss:  0.90962327 \t acc:  0.55\n",
      "240 : loss:  0.89628357 \t acc:  0.57\n",
      "242 : loss:  0.9998567 \t acc:  0.47\n",
      "244 : loss:  0.9717645 \t acc:  0.46\n",
      "246 : loss:  0.9433075 \t acc:  0.51\n",
      "248 : loss:  0.91974103 \t acc:  0.57\n",
      "250 : loss:  0.93412966 \t acc:  0.58\n",
      "252 : loss:  1.0631139 \t acc:  0.46\n",
      "254 : loss:  0.8783406 \t acc:  0.58\n",
      "256 : loss:  0.94947845 \t acc:  0.43\n",
      "258 : loss:  1.0251125 \t acc:  0.48\n",
      "260 : loss:  0.90590453 \t acc:  0.59\n",
      "262 : loss:  0.9861001 \t acc:  0.5\n",
      "264 : loss:  0.92705286 \t acc:  0.53\n",
      "266 : loss:  0.9647531 \t acc:  0.54\n",
      "268 : loss:  0.914281 \t acc:  0.59\n",
      "270 : loss:  1.0031234 \t acc:  0.4\n",
      "272 : loss:  0.913747 \t acc:  0.56\n",
      "274 : loss:  0.99353963 \t acc:  0.43\n",
      "276 : loss:  0.92391837 \t acc:  0.55\n",
      "278 : loss:  0.95935607 \t acc:  0.52\n",
      "280 : loss:  0.98941827 \t acc:  0.44\n",
      "282 : loss:  0.8949904 \t acc:  0.48\n",
      "284 : loss:  0.8822214 \t acc:  0.56\n",
      "286 : loss:  0.97547716 \t acc:  0.52\n",
      "288 : loss:  0.9341957 \t acc:  0.6\n",
      "290 : loss:  0.8873111 \t acc:  0.52\n",
      "292 : loss:  0.9385505 \t acc:  0.54\n",
      "294 : loss:  0.9193937 \t acc:  0.54\n",
      "296 : loss:  0.87260556 \t acc:  0.58\n",
      "298 : loss:  0.96551305 \t acc:  0.48\n",
      "300 : loss:  0.93946165 \t acc:  0.43\n",
      "302 : loss:  0.88751274 \t acc:  0.61\n",
      "304 : loss:  0.90150976 \t acc:  0.56\n",
      "306 : loss:  0.97915214 \t acc:  0.54\n",
      "308 : loss:  1.0713878 \t acc:  0.47\n",
      "310 : loss:  0.9054261 \t acc:  0.6\n",
      "312 : loss:  0.9964177 \t acc:  0.47\n",
      "314 : loss:  0.963367 \t acc:  0.52\n",
      "316 : loss:  0.9556117 \t acc:  0.54\n",
      "318 : loss:  0.9930319 \t acc:  0.42\n",
      "320 : loss:  0.8924092 \t acc:  0.59\n",
      "322 : loss:  0.89941186 \t acc:  0.58\n",
      "324 : loss:  0.98444444 \t acc:  0.53\n",
      "326 : loss:  0.90332544 \t acc:  0.62\n",
      "328 : loss:  0.9202857 \t acc:  0.53\n",
      "330 : loss:  0.86911196 \t acc:  0.61\n",
      "332 : loss:  0.9858816 \t acc:  0.49\n",
      "334 : loss:  0.96073365 \t acc:  0.51\n",
      "336 : loss:  0.8983637 \t acc:  0.61\n",
      "338 : loss:  0.87136626 \t acc:  0.53\n",
      "340 : loss:  0.8756987 \t acc:  0.53\n",
      "342 : loss:  0.9806317 \t acc:  0.58\n",
      "344 : loss:  0.9656247 \t acc:  0.53\n",
      "346 : loss:  0.9279333 \t acc:  0.62\n",
      "348 : loss:  0.8519023 \t acc:  0.56\n",
      "350 : loss:  0.9664026 \t acc:  0.56\n",
      "352 : loss:  0.95974386 \t acc:  0.56\n",
      "354 : loss:  0.8337816 \t acc:  0.63\n",
      "356 : loss:  0.9258932 \t acc:  0.54\n",
      "358 : loss:  0.8499304 \t acc:  0.64\n",
      "360 : loss:  0.8525845 \t acc:  0.59\n",
      "362 : loss:  0.93783724 \t acc:  0.51\n",
      "364 : loss:  0.9154541 \t acc:  0.54\n",
      "366 : loss:  0.8673183 \t acc:  0.6\n",
      "368 : loss:  0.95100844 \t acc:  0.5\n",
      "370 : loss:  0.9035237 \t acc:  0.54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving...\n",
      "saved to models/CNN/pretrained_model.ckpt-372\n",
      "\n",
      "100 \t [65.16130369 58.41038108 29.66664391]\n",
      "0 \tval accuracy:  0.5455 \t f_! score:  [0.65161304 0.58410381 0.29666644]\n",
      "\n",
      "100 \t [65.16130369 58.41038108 29.66664391]\n",
      "100 \t [62.07753959 48.76944714 55.20502341]\n",
      "100 \t [69.15717411 73.47898553 20.71147382]\n",
      "100 \t [3361. 3302. 3337.]\n",
      "--- Test   Twitter ---\n",
      "0.5455\n",
      "f1:  [0.65161304 0.58410381 0.29666644]\n",
      "--- 40.56085133552551 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1.823013,\n",
       "  1.1230767,\n",
       "  1.1102923,\n",
       "  1.098885,\n",
       "  1.0984883,\n",
       "  1.0772027,\n",
       "  1.0856456,\n",
       "  1.0862688,\n",
       "  1.0827261,\n",
       "  1.0391577,\n",
       "  1.0257076,\n",
       "  1.0520902,\n",
       "  0.9986509,\n",
       "  0.99773943,\n",
       "  0.9807488,\n",
       "  0.9951447,\n",
       "  0.9871148,\n",
       "  0.98543626,\n",
       "  1.1132046,\n",
       "  1.032706,\n",
       "  1.0215985,\n",
       "  0.96368897,\n",
       "  1.0540209,\n",
       "  1.0091941,\n",
       "  0.99531955,\n",
       "  1.140557,\n",
       "  0.98026407,\n",
       "  0.9987872,\n",
       "  1.0677912,\n",
       "  1.009635,\n",
       "  1.033522,\n",
       "  1.0591353,\n",
       "  1.0943067,\n",
       "  0.996353,\n",
       "  0.9528414,\n",
       "  1.0199913,\n",
       "  1.0235271,\n",
       "  1.0592368,\n",
       "  0.98840284,\n",
       "  0.99523103,\n",
       "  0.9482677,\n",
       "  0.947606,\n",
       "  0.96540886,\n",
       "  1.0179094,\n",
       "  0.89045316,\n",
       "  0.9303518,\n",
       "  1.0027531,\n",
       "  0.9631016,\n",
       "  0.95028013,\n",
       "  0.9428133,\n",
       "  1.0009102,\n",
       "  0.9695597,\n",
       "  1.012758,\n",
       "  0.9237679,\n",
       "  0.9793584,\n",
       "  0.93527985,\n",
       "  0.9702398,\n",
       "  1.0428401,\n",
       "  0.9429607,\n",
       "  1.0993453,\n",
       "  0.9850289,\n",
       "  0.9606948,\n",
       "  0.98371243,\n",
       "  0.9924433,\n",
       "  0.94155395,\n",
       "  0.9083451,\n",
       "  0.98628885,\n",
       "  0.9759138,\n",
       "  0.9977573,\n",
       "  0.96362436,\n",
       "  0.93354094,\n",
       "  0.90713227,\n",
       "  0.91069794,\n",
       "  0.95588547,\n",
       "  0.95009637,\n",
       "  0.9312185,\n",
       "  0.97577035,\n",
       "  0.916781,\n",
       "  0.86304414,\n",
       "  0.91518515,\n",
       "  0.93072975,\n",
       "  0.9198336,\n",
       "  0.9429387,\n",
       "  0.9725097,\n",
       "  0.9270997,\n",
       "  0.88309294,\n",
       "  0.8714723,\n",
       "  0.9526269,\n",
       "  0.9909682,\n",
       "  0.988456,\n",
       "  0.87638545,\n",
       "  0.9335114,\n",
       "  0.99721503,\n",
       "  0.8527345,\n",
       "  0.9163502,\n",
       "  1.0064496,\n",
       "  0.96074414,\n",
       "  0.9576015,\n",
       "  0.9185097,\n",
       "  0.8797847,\n",
       "  0.9413142,\n",
       "  0.9504831,\n",
       "  0.926844,\n",
       "  0.99555075,\n",
       "  0.89203244,\n",
       "  0.8481592,\n",
       "  0.9121326,\n",
       "  0.97295487,\n",
       "  0.9713591,\n",
       "  0.98623025,\n",
       "  0.9313358,\n",
       "  0.8922467,\n",
       "  0.9088499,\n",
       "  0.9083127,\n",
       "  0.9717972,\n",
       "  0.9689943,\n",
       "  0.91074866,\n",
       "  0.9384676,\n",
       "  0.9480648,\n",
       "  0.90962327,\n",
       "  0.89628357,\n",
       "  0.9998567,\n",
       "  0.9717645,\n",
       "  0.9433075,\n",
       "  0.91974103,\n",
       "  0.93412966,\n",
       "  1.0631139,\n",
       "  0.8783406,\n",
       "  0.94947845,\n",
       "  1.0251125,\n",
       "  0.90590453,\n",
       "  0.9861001,\n",
       "  0.92705286,\n",
       "  0.9647531,\n",
       "  0.914281,\n",
       "  1.0031234,\n",
       "  0.913747,\n",
       "  0.99353963,\n",
       "  0.92391837,\n",
       "  0.95935607,\n",
       "  0.98941827,\n",
       "  0.8949904,\n",
       "  0.8822214,\n",
       "  0.97547716,\n",
       "  0.9341957,\n",
       "  0.8873111,\n",
       "  0.9385505,\n",
       "  0.9193937,\n",
       "  0.87260556,\n",
       "  0.96551305,\n",
       "  0.93946165,\n",
       "  0.88751274,\n",
       "  0.90150976,\n",
       "  0.97915214,\n",
       "  1.0713878,\n",
       "  0.9054261,\n",
       "  0.9964177,\n",
       "  0.963367,\n",
       "  0.9556117,\n",
       "  0.9930319,\n",
       "  0.8924092,\n",
       "  0.89941186,\n",
       "  0.98444444,\n",
       "  0.90332544,\n",
       "  0.9202857,\n",
       "  0.86911196,\n",
       "  0.9858816,\n",
       "  0.96073365,\n",
       "  0.8983637,\n",
       "  0.87136626,\n",
       "  0.8756987,\n",
       "  0.9806317,\n",
       "  0.9656247,\n",
       "  0.9279333,\n",
       "  0.8519023,\n",
       "  0.9664026,\n",
       "  0.95974386,\n",
       "  0.8337816,\n",
       "  0.9258932,\n",
       "  0.8499304,\n",
       "  0.8525845,\n",
       "  0.93783724,\n",
       "  0.9154541,\n",
       "  0.8673183,\n",
       "  0.95100844,\n",
       "  0.9035237],\n",
       " [0.28,\n",
       "  0.33,\n",
       "  0.25,\n",
       "  0.31,\n",
       "  0.28,\n",
       "  0.47,\n",
       "  0.45,\n",
       "  0.33,\n",
       "  0.37,\n",
       "  0.52,\n",
       "  0.55,\n",
       "  0.45,\n",
       "  0.5,\n",
       "  0.55,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.55,\n",
       "  0.37,\n",
       "  0.45,\n",
       "  0.52,\n",
       "  0.57,\n",
       "  0.41,\n",
       "  0.51,\n",
       "  0.55,\n",
       "  0.31,\n",
       "  0.51,\n",
       "  0.47,\n",
       "  0.37,\n",
       "  0.51,\n",
       "  0.49,\n",
       "  0.36,\n",
       "  0.38,\n",
       "  0.54,\n",
       "  0.49,\n",
       "  0.46,\n",
       "  0.48,\n",
       "  0.41,\n",
       "  0.47,\n",
       "  0.49,\n",
       "  0.52,\n",
       "  0.52,\n",
       "  0.51,\n",
       "  0.5,\n",
       "  0.58,\n",
       "  0.53,\n",
       "  0.49,\n",
       "  0.41,\n",
       "  0.52,\n",
       "  0.58,\n",
       "  0.48,\n",
       "  0.44,\n",
       "  0.52,\n",
       "  0.52,\n",
       "  0.42,\n",
       "  0.51,\n",
       "  0.45,\n",
       "  0.41,\n",
       "  0.41,\n",
       "  0.45,\n",
       "  0.51,\n",
       "  0.52,\n",
       "  0.53,\n",
       "  0.49,\n",
       "  0.52,\n",
       "  0.57,\n",
       "  0.51,\n",
       "  0.47,\n",
       "  0.46,\n",
       "  0.53,\n",
       "  0.53,\n",
       "  0.53,\n",
       "  0.53,\n",
       "  0.59,\n",
       "  0.47,\n",
       "  0.47,\n",
       "  0.47,\n",
       "  0.6,\n",
       "  0.51,\n",
       "  0.56,\n",
       "  0.45,\n",
       "  0.52,\n",
       "  0.55,\n",
       "  0.48,\n",
       "  0.56,\n",
       "  0.58,\n",
       "  0.59,\n",
       "  0.5,\n",
       "  0.51,\n",
       "  0.47,\n",
       "  0.57,\n",
       "  0.49,\n",
       "  0.43,\n",
       "  0.58,\n",
       "  0.55,\n",
       "  0.49,\n",
       "  0.41,\n",
       "  0.52,\n",
       "  0.59,\n",
       "  0.58,\n",
       "  0.51,\n",
       "  0.54,\n",
       "  0.58,\n",
       "  0.51,\n",
       "  0.51,\n",
       "  0.59,\n",
       "  0.56,\n",
       "  0.51,\n",
       "  0.46,\n",
       "  0.45,\n",
       "  0.54,\n",
       "  0.57,\n",
       "  0.55,\n",
       "  0.55,\n",
       "  0.53,\n",
       "  0.44,\n",
       "  0.58,\n",
       "  0.57,\n",
       "  0.45,\n",
       "  0.55,\n",
       "  0.57,\n",
       "  0.47,\n",
       "  0.46,\n",
       "  0.51,\n",
       "  0.57,\n",
       "  0.58,\n",
       "  0.46,\n",
       "  0.58,\n",
       "  0.43,\n",
       "  0.48,\n",
       "  0.59,\n",
       "  0.5,\n",
       "  0.53,\n",
       "  0.54,\n",
       "  0.59,\n",
       "  0.4,\n",
       "  0.56,\n",
       "  0.43,\n",
       "  0.55,\n",
       "  0.52,\n",
       "  0.44,\n",
       "  0.48,\n",
       "  0.56,\n",
       "  0.52,\n",
       "  0.6,\n",
       "  0.52,\n",
       "  0.54,\n",
       "  0.54,\n",
       "  0.58,\n",
       "  0.48,\n",
       "  0.43,\n",
       "  0.61,\n",
       "  0.56,\n",
       "  0.54,\n",
       "  0.47,\n",
       "  0.6,\n",
       "  0.47,\n",
       "  0.52,\n",
       "  0.54,\n",
       "  0.42,\n",
       "  0.59,\n",
       "  0.58,\n",
       "  0.53,\n",
       "  0.62,\n",
       "  0.53,\n",
       "  0.61,\n",
       "  0.49,\n",
       "  0.51,\n",
       "  0.61,\n",
       "  0.53,\n",
       "  0.53,\n",
       "  0.58,\n",
       "  0.53,\n",
       "  0.62,\n",
       "  0.56,\n",
       "  0.56,\n",
       "  0.56,\n",
       "  0.63,\n",
       "  0.54,\n",
       "  0.64,\n",
       "  0.59,\n",
       "  0.51,\n",
       "  0.54,\n",
       "  0.6,\n",
       "  0.5,\n",
       "  0.54])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testhelper = TestHelper()\n",
    "net_name=\"CNN\"\n",
    "review_loss, review_acc = testhelper.train_input(\"Review\", net_name, epochs=4)\n",
    "\n",
    "testhelper.just_test(\"Twitter\", net_name, print_time=True)\n",
    "testhelper.train_last_layer(\"Twitter\", net_name, epochs=1, print_time=True)\n",
    "testhelper.train_input(\"Twitter\", net_name, epochs=1, print_time=True, restore=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  Review\n",
      "delete old models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yannik/ba/TestHelper.py:64: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  input_data = self.decide_which_input(input_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509411\n",
      "Tensor(\"lstm_net/dense/BiasAdd:0\", shape=(?, 3), dtype=float32)\n",
      "Tensor(\"lstm_net_1/dense/BiasAdd:0\", shape=(?, 3), dtype=float32)\n",
      "---init ready   LSTM ---\n",
      "\n",
      "Saving...\n",
      "saved to models/LSTM/pretrained_model.ckpt-0\n",
      "0 : loss:  2.4231784 \t acc:  0.34\n",
      "40 : loss:  1.0940621 \t acc:  0.38\n",
      "80 : loss:  1.1451602 \t acc:  0.38\n",
      "120 : loss:  1.0299308 \t acc:  0.47\n",
      "160 : loss:  1.0414269 \t acc:  0.45\n",
      "200 : loss:  1.0078896 \t acc:  0.49\n",
      "240 : loss:  1.1335462 \t acc:  0.38\n",
      "280 : loss:  0.9776029 \t acc:  0.55\n",
      "320 : loss:  1.002204 \t acc:  0.49\n",
      "360 : loss:  1.012602 \t acc:  0.5\n",
      "400 : loss:  0.9683745 \t acc:  0.52\n",
      "440 : loss:  1.0554317 \t acc:  0.48\n",
      "480 : loss:  1.0167933 \t acc:  0.52\n",
      "520 : loss:  0.93092245 \t acc:  0.52\n",
      "560 : loss:  1.0369481 \t acc:  0.46\n",
      "600 : loss:  0.9543356 \t acc:  0.51\n",
      "640 : loss:  0.9150612 \t acc:  0.57\n",
      "680 : loss:  0.94175744 \t acc:  0.55\n",
      "720 : loss:  0.90480876 \t acc:  0.56\n",
      "760 : loss:  0.9238782 \t acc:  0.54\n",
      "800 : loss:  1.0219539 \t acc:  0.48\n",
      "840 : loss:  0.93907934 \t acc:  0.54\n",
      "880 : loss:  0.9061407 \t acc:  0.56\n",
      "920 : loss:  0.9253857 \t acc:  0.55\n",
      "960 : loss:  0.86143774 \t acc:  0.61\n",
      "1000 : loss:  0.90930116 \t acc:  0.5\n",
      "1040 : loss:  0.9123388 \t acc:  0.52\n",
      "1080 : loss:  1.0319108 \t acc:  0.53\n",
      "1120 : loss:  1.0256687 \t acc:  0.48\n",
      "1160 : loss:  0.9927786 \t acc:  0.54\n",
      "1200 : loss:  0.96442014 \t acc:  0.5\n",
      "1240 : loss:  0.91108644 \t acc:  0.6\n",
      "1280 : loss:  0.98851484 \t acc:  0.52\n",
      "1320 : loss:  0.8801701 \t acc:  0.58\n",
      "1360 : loss:  0.82895833 \t acc:  0.64\n",
      "1400 : loss:  0.89671135 \t acc:  0.52\n",
      "1440 : loss:  0.970039 \t acc:  0.51\n",
      "1480 : loss:  0.9237737 \t acc:  0.51\n",
      "1520 : loss:  0.8743158 \t acc:  0.58\n",
      "1560 : loss:  0.8745407 \t acc:  0.62\n",
      "1600 : loss:  0.7612397 \t acc:  0.7\n",
      "1640 : loss:  0.8630793 \t acc:  0.61\n",
      "1680 : loss:  0.8241207 \t acc:  0.63\n",
      "1720 : loss:  0.98745966 \t acc:  0.53\n",
      "1760 : loss:  0.8974816 \t acc:  0.59\n",
      "1800 : loss:  0.966265 \t acc:  0.57\n",
      "1840 : loss:  0.7987828 \t acc:  0.66\n",
      "1880 : loss:  0.830473 \t acc:  0.66\n",
      "1920 : loss:  0.81050324 \t acc:  0.62\n",
      "1960 : loss:  0.87987924 \t acc:  0.57\n",
      "2000 : loss:  0.8584553 \t acc:  0.55\n",
      "2040 : loss:  0.94362557 \t acc:  0.52\n",
      "2080 : loss:  0.79307985 \t acc:  0.61\n",
      "2120 : loss:  0.9232679 \t acc:  0.52\n",
      "2160 : loss:  0.8539764 \t acc:  0.65\n",
      "2200 : loss:  0.7850947 \t acc:  0.64\n",
      "2240 : loss:  0.8391111 \t acc:  0.63\n",
      "2280 : loss:  0.8657869 \t acc:  0.67\n",
      "2320 : loss:  0.8743293 \t acc:  0.63\n",
      "2360 : loss:  0.9913524 \t acc:  0.54\n",
      "2400 : loss:  0.9351856 \t acc:  0.62\n",
      "2440 : loss:  0.817919 \t acc:  0.63\n",
      "2480 : loss:  0.8247748 \t acc:  0.64\n",
      "2520 : loss:  0.8222087 \t acc:  0.6\n",
      "2560 : loss:  0.8022001 \t acc:  0.61\n",
      "2600 : loss:  0.8822683 \t acc:  0.51\n",
      "2640 : loss:  0.821051 \t acc:  0.65\n",
      "2680 : loss:  0.89724684 \t acc:  0.52\n",
      "2720 : loss:  0.77791154 \t acc:  0.65\n",
      "2760 : loss:  0.795872 \t acc:  0.64\n",
      "2800 : loss:  0.7039462 \t acc:  0.72\n",
      "2840 : loss:  0.88014865 \t acc:  0.54\n",
      "2880 : loss:  0.9515686 \t acc:  0.54\n",
      "2920 : loss:  0.84373915 \t acc:  0.59\n",
      "2960 : loss:  0.9764073 \t acc:  0.55\n",
      "3000 : loss:  0.6967186 \t acc:  0.74\n",
      "3040 : loss:  0.8861908 \t acc:  0.59\n",
      "3080 : loss:  0.7079948 \t acc:  0.65\n",
      "3120 : loss:  0.76532334 \t acc:  0.6\n",
      "3160 : loss:  0.75045156 \t acc:  0.68\n",
      "3200 : loss:  1.0046891 \t acc:  0.56\n",
      "3240 : loss:  0.82581484 \t acc:  0.6\n",
      "3280 : loss:  0.8244973 \t acc:  0.61\n",
      "3320 : loss:  0.8074114 \t acc:  0.61\n",
      "3360 : loss:  0.7104892 \t acc:  0.68\n",
      "3400 : loss:  0.75290453 \t acc:  0.64\n",
      "3440 : loss:  0.89948225 \t acc:  0.58\n",
      "3480 : loss:  0.7420675 \t acc:  0.71\n",
      "3520 : loss:  0.7648109 \t acc:  0.68\n",
      "3560 : loss:  0.7602334 \t acc:  0.63\n",
      "3600 : loss:  0.7690884 \t acc:  0.61\n",
      "3640 : loss:  0.72839034 \t acc:  0.63\n",
      "3680 : loss:  0.8053998 \t acc:  0.62\n",
      "3720 : loss:  0.7880935 \t acc:  0.66\n",
      "3760 : loss:  0.79119 \t acc:  0.68\n",
      "3800 : loss:  0.72291815 \t acc:  0.64\n",
      "3840 : loss:  0.7340521 \t acc:  0.67\n",
      "3880 : loss:  0.70861465 \t acc:  0.65\n",
      "3920 : loss:  0.89342827 \t acc:  0.55\n",
      "3960 : loss:  0.79160684 \t acc:  0.67\n",
      "4000 : loss:  0.82963526 \t acc:  0.6\n",
      "4040 : loss:  0.8334534 \t acc:  0.6\n",
      "4080 : loss:  0.8087122 \t acc:  0.6\n",
      "4120 : loss:  0.7367905 \t acc:  0.69\n",
      "4160 : loss:  0.7674282 \t acc:  0.61\n",
      "4200 : loss:  0.834355 \t acc:  0.62\n",
      "4240 : loss:  0.79624474 \t acc:  0.65\n",
      "4280 : loss:  0.7423181 \t acc:  0.72\n",
      "4320 : loss:  0.8529398 \t acc:  0.61\n",
      "4360 : loss:  0.6649251 \t acc:  0.71\n",
      "4400 : loss:  0.77243316 \t acc:  0.64\n",
      "4440 : loss:  0.83343905 \t acc:  0.56\n",
      "4480 : loss:  0.78934044 \t acc:  0.71\n",
      "\n",
      "Saving...\n",
      "saved to models/LSTM/pretrained_model.ckpt-4482\n",
      "\n",
      "500 \t [339.83739606 241.41017647 357.32891321]\n",
      "0 \tval accuracy:  0.63886 \t f_! score:  [0.67967479 0.48282035 0.71465783]\n",
      "\n",
      "4520 : loss:  0.7751971 \t acc:  0.63\n",
      "4560 : loss:  0.67695403 \t acc:  0.73\n",
      "4600 : loss:  0.72376424 \t acc:  0.65\n",
      "4640 : loss:  0.69391304 \t acc:  0.68\n",
      "4680 : loss:  0.8560241 \t acc:  0.55\n",
      "4720 : loss:  0.78815275 \t acc:  0.63\n",
      "4760 : loss:  0.84039956 \t acc:  0.61\n",
      "4800 : loss:  0.7468187 \t acc:  0.64\n",
      "4840 : loss:  0.69522893 \t acc:  0.68\n",
      "4880 : loss:  0.804379 \t acc:  0.61\n",
      "4920 : loss:  0.8468921 \t acc:  0.58\n",
      "4960 : loss:  0.7943156 \t acc:  0.59\n",
      "5000 : loss:  0.7994439 \t acc:  0.62\n",
      "5040 : loss:  0.9038843 \t acc:  0.57\n",
      "5080 : loss:  0.8189472 \t acc:  0.62\n",
      "5120 : loss:  0.79439574 \t acc:  0.66\n",
      "5160 : loss:  0.9487193 \t acc:  0.51\n",
      "5200 : loss:  0.82530624 \t acc:  0.66\n",
      "5240 : loss:  0.70015466 \t acc:  0.72\n",
      "5280 : loss:  0.6853769 \t acc:  0.7\n",
      "5320 : loss:  0.71791327 \t acc:  0.66\n",
      "5360 : loss:  0.6914718 \t acc:  0.7\n",
      "5400 : loss:  0.6355282 \t acc:  0.74\n",
      "5440 : loss:  0.7610417 \t acc:  0.66\n",
      "5480 : loss:  0.6468669 \t acc:  0.72\n",
      "5520 : loss:  0.84758943 \t acc:  0.63\n",
      "5560 : loss:  0.8513522 \t acc:  0.59\n",
      "5600 : loss:  0.78342956 \t acc:  0.64\n",
      "5640 : loss:  0.7933036 \t acc:  0.64\n",
      "5680 : loss:  0.68472993 \t acc:  0.73\n",
      "5720 : loss:  0.8750818 \t acc:  0.6\n",
      "5760 : loss:  0.8218046 \t acc:  0.65\n",
      "5800 : loss:  0.6677123 \t acc:  0.64\n",
      "5840 : loss:  0.71512383 \t acc:  0.65\n",
      "5880 : loss:  0.75133467 \t acc:  0.65\n",
      "5920 : loss:  0.6449811 \t acc:  0.7\n",
      "5960 : loss:  0.92166555 \t acc:  0.58\n",
      "6000 : loss:  0.77654916 \t acc:  0.64\n",
      "6040 : loss:  0.73380196 \t acc:  0.63\n",
      "6080 : loss:  0.70958465 \t acc:  0.67\n",
      "6120 : loss:  0.82542443 \t acc:  0.63\n",
      "6160 : loss:  0.7587092 \t acc:  0.62\n",
      "6200 : loss:  0.7023975 \t acc:  0.69\n",
      "6240 : loss:  0.76695806 \t acc:  0.62\n",
      "6280 : loss:  0.67516243 \t acc:  0.76\n",
      "6320 : loss:  0.7953769 \t acc:  0.63\n",
      "6360 : loss:  0.7179299 \t acc:  0.69\n",
      "6400 : loss:  0.76267797 \t acc:  0.6\n",
      "6440 : loss:  0.78543466 \t acc:  0.61\n",
      "6480 : loss:  0.72987 \t acc:  0.69\n",
      "6520 : loss:  0.8208532 \t acc:  0.65\n",
      "6560 : loss:  0.77642614 \t acc:  0.62\n",
      "6600 : loss:  0.66997194 \t acc:  0.68\n",
      "6640 : loss:  0.6907349 \t acc:  0.7\n",
      "6680 : loss:  0.79869723 \t acc:  0.61\n",
      "6720 : loss:  0.6640614 \t acc:  0.72\n",
      "6760 : loss:  0.7692512 \t acc:  0.68\n",
      "6800 : loss:  0.87982744 \t acc:  0.62\n",
      "6840 : loss:  0.7779882 \t acc:  0.66\n",
      "6880 : loss:  0.81188345 \t acc:  0.58\n",
      "6920 : loss:  0.8111038 \t acc:  0.65\n",
      "6960 : loss:  0.7901603 \t acc:  0.63\n",
      "7000 : loss:  0.7359038 \t acc:  0.66\n",
      "7040 : loss:  0.8209079 \t acc:  0.63\n",
      "7080 : loss:  0.81501096 \t acc:  0.62\n",
      "7120 : loss:  0.74338716 \t acc:  0.65\n",
      "7160 : loss:  0.7003921 \t acc:  0.7\n",
      "7200 : loss:  0.72527885 \t acc:  0.65\n",
      "7240 : loss:  0.7532864 \t acc:  0.66\n",
      "7280 : loss:  0.73096657 \t acc:  0.69\n",
      "7320 : loss:  0.782769 \t acc:  0.63\n",
      "7360 : loss:  0.6716865 \t acc:  0.68\n",
      "7400 : loss:  0.68956435 \t acc:  0.71\n",
      "7440 : loss:  0.8409015 \t acc:  0.66\n",
      "7480 : loss:  0.7925478 \t acc:  0.61\n",
      "7520 : loss:  0.70172554 \t acc:  0.63\n",
      "7560 : loss:  0.77903414 \t acc:  0.62\n",
      "7600 : loss:  0.6491637 \t acc:  0.71\n",
      "7640 : loss:  0.7376189 \t acc:  0.63\n",
      "7680 : loss:  0.71333647 \t acc:  0.72\n",
      "7720 : loss:  0.7217549 \t acc:  0.68\n",
      "7760 : loss:  0.74542236 \t acc:  0.62\n",
      "7800 : loss:  0.7518142 \t acc:  0.66\n",
      "7840 : loss:  0.740173 \t acc:  0.69\n",
      "7880 : loss:  0.8812909 \t acc:  0.63\n",
      "7920 : loss:  0.5957322 \t acc:  0.71\n",
      "7960 : loss:  0.75058085 \t acc:  0.7\n",
      "8000 : loss:  0.8597823 \t acc:  0.59\n",
      "8040 : loss:  0.77699137 \t acc:  0.63\n",
      "8080 : loss:  0.69666594 \t acc:  0.64\n",
      "8120 : loss:  0.6904559 \t acc:  0.68\n",
      "8160 : loss:  0.88406587 \t acc:  0.56\n",
      "8200 : loss:  0.7188614 \t acc:  0.66\n",
      "8240 : loss:  0.6917735 \t acc:  0.69\n",
      "8280 : loss:  0.72020125 \t acc:  0.67\n",
      "8320 : loss:  0.656322 \t acc:  0.72\n",
      "8360 : loss:  0.7919651 \t acc:  0.7\n",
      "8400 : loss:  0.7958372 \t acc:  0.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8440 : loss:  0.813051 \t acc:  0.64\n",
      "8480 : loss:  0.71444017 \t acc:  0.66\n",
      "8520 : loss:  0.7615327 \t acc:  0.64\n",
      "8560 : loss:  0.82328606 \t acc:  0.65\n",
      "8600 : loss:  0.77892977 \t acc:  0.68\n",
      "8640 : loss:  0.714553 \t acc:  0.67\n",
      "8680 : loss:  0.80244565 \t acc:  0.61\n",
      "8720 : loss:  0.77315557 \t acc:  0.6\n",
      "8760 : loss:  0.7716475 \t acc:  0.7\n",
      "8800 : loss:  0.71563727 \t acc:  0.66\n",
      "8840 : loss:  0.84886557 \t acc:  0.58\n",
      "8880 : loss:  0.60760134 \t acc:  0.74\n",
      "8920 : loss:  0.6393628 \t acc:  0.75\n",
      "8960 : loss:  0.75458544 \t acc:  0.63\n",
      "\n",
      "Saving...\n",
      "saved to models/LSTM/pretrained_model.ckpt-8965\n",
      "\n",
      "500 \t [359.42452023 231.03837743 373.47361597]\n",
      "1 \tval accuracy:  0.666 \t f_! score:  [0.71884904 0.46207675 0.74694723]\n",
      "\n",
      "9000 : loss:  0.72435415 \t acc:  0.66\n",
      "9040 : loss:  0.70616263 \t acc:  0.65\n",
      "9080 : loss:  0.7829689 \t acc:  0.64\n",
      "9120 : loss:  0.7562641 \t acc:  0.7\n",
      "9160 : loss:  0.6445525 \t acc:  0.76\n",
      "9200 : loss:  0.7200043 \t acc:  0.65\n",
      "9240 : loss:  0.7726386 \t acc:  0.63\n",
      "9280 : loss:  0.67279786 \t acc:  0.71\n",
      "9320 : loss:  0.70227486 \t acc:  0.69\n",
      "9360 : loss:  0.65300673 \t acc:  0.71\n",
      "9400 : loss:  0.70543253 \t acc:  0.72\n",
      "9440 : loss:  0.6515154 \t acc:  0.7\n",
      "9480 : loss:  0.732951 \t acc:  0.69\n",
      "9520 : loss:  0.75051385 \t acc:  0.69\n",
      "9560 : loss:  0.64310545 \t acc:  0.72\n",
      "9600 : loss:  0.7194209 \t acc:  0.63\n",
      "9640 : loss:  0.77936083 \t acc:  0.68\n",
      "9680 : loss:  0.7389192 \t acc:  0.7\n",
      "9720 : loss:  0.7197271 \t acc:  0.64\n",
      "9760 : loss:  0.6599352 \t acc:  0.72\n",
      "9800 : loss:  0.7015721 \t acc:  0.64\n",
      "9840 : loss:  0.80666685 \t acc:  0.64\n",
      "9880 : loss:  0.59854865 \t acc:  0.74\n",
      "9920 : loss:  0.7858432 \t acc:  0.62\n",
      "9960 : loss:  0.7443287 \t acc:  0.66\n",
      "10000 : loss:  0.8117002 \t acc:  0.63\n",
      "10040 : loss:  0.70003676 \t acc:  0.72\n",
      "10080 : loss:  0.72257966 \t acc:  0.73\n",
      "10120 : loss:  0.7766153 \t acc:  0.68\n",
      "10160 : loss:  0.6596969 \t acc:  0.75\n",
      "10200 : loss:  0.90126884 \t acc:  0.58\n",
      "10240 : loss:  0.6495581 \t acc:  0.68\n",
      "10280 : loss:  0.74690163 \t acc:  0.62\n",
      "10320 : loss:  0.72074014 \t acc:  0.72\n",
      "10360 : loss:  0.75870454 \t acc:  0.76\n",
      "10400 : loss:  0.66378814 \t acc:  0.72\n",
      "10440 : loss:  0.72733706 \t acc:  0.7\n",
      "10480 : loss:  0.677119 \t acc:  0.73\n",
      "10520 : loss:  0.6651018 \t acc:  0.77\n",
      "10560 : loss:  0.661132 \t acc:  0.71\n",
      "10600 : loss:  0.76252437 \t acc:  0.7\n",
      "10640 : loss:  0.75735 \t acc:  0.61\n",
      "10680 : loss:  0.7091195 \t acc:  0.69\n",
      "10720 : loss:  0.64795107 \t acc:  0.68\n",
      "10760 : loss:  0.6578057 \t acc:  0.72\n",
      "10800 : loss:  0.6197133 \t acc:  0.71\n",
      "10840 : loss:  0.8614082 \t acc:  0.62\n",
      "10880 : loss:  0.808818 \t acc:  0.65\n",
      "10920 : loss:  0.76110286 \t acc:  0.7\n",
      "10960 : loss:  0.8761197 \t acc:  0.63\n",
      "11000 : loss:  0.7921268 \t acc:  0.6\n",
      "11040 : loss:  0.71808696 \t acc:  0.68\n",
      "11080 : loss:  0.7127086 \t acc:  0.68\n",
      "11120 : loss:  0.70606667 \t acc:  0.66\n",
      "11160 : loss:  0.6173115 \t acc:  0.73\n",
      "11200 : loss:  0.7287912 \t acc:  0.74\n",
      "11240 : loss:  0.67543036 \t acc:  0.7\n",
      "11280 : loss:  0.8449427 \t acc:  0.61\n",
      "11320 : loss:  0.8823036 \t acc:  0.55\n",
      "11360 : loss:  0.6620042 \t acc:  0.71\n",
      "11400 : loss:  0.6873354 \t acc:  0.68\n",
      "11440 : loss:  0.7598083 \t acc:  0.69\n",
      "11480 : loss:  0.7115024 \t acc:  0.71\n",
      "11520 : loss:  0.8149847 \t acc:  0.62\n",
      "11560 : loss:  0.7257837 \t acc:  0.66\n",
      "11600 : loss:  0.7593642 \t acc:  0.68\n",
      "11640 : loss:  0.7919233 \t acc:  0.68\n",
      "11680 : loss:  0.6891458 \t acc:  0.73\n",
      "11720 : loss:  0.7726028 \t acc:  0.65\n",
      "11760 : loss:  0.6009052 \t acc:  0.71\n",
      "11800 : loss:  0.6355759 \t acc:  0.76\n",
      "11840 : loss:  0.6351427 \t acc:  0.71\n",
      "11880 : loss:  0.683381 \t acc:  0.7\n",
      "11920 : loss:  0.79628253 \t acc:  0.66\n",
      "11960 : loss:  0.68767685 \t acc:  0.71\n",
      "12000 : loss:  0.6814225 \t acc:  0.68\n",
      "12040 : loss:  0.7040528 \t acc:  0.68\n",
      "12080 : loss:  0.64704084 \t acc:  0.73\n",
      "12120 : loss:  0.7026198 \t acc:  0.74\n",
      "12160 : loss:  0.5731466 \t acc:  0.73\n",
      "12200 : loss:  0.77875704 \t acc:  0.61\n",
      "12240 : loss:  0.7640552 \t acc:  0.65\n",
      "12280 : loss:  0.59200835 \t acc:  0.74\n",
      "12320 : loss:  0.79020935 \t acc:  0.66\n",
      "12360 : loss:  0.65145576 \t acc:  0.73\n",
      "12400 : loss:  0.7234069 \t acc:  0.69\n",
      "12440 : loss:  0.78236496 \t acc:  0.61\n",
      "12480 : loss:  0.64832336 \t acc:  0.74\n",
      "12520 : loss:  0.9531959 \t acc:  0.57\n",
      "12560 : loss:  0.7708979 \t acc:  0.64\n",
      "12600 : loss:  0.77983916 \t acc:  0.65\n",
      "12640 : loss:  0.72141266 \t acc:  0.7\n",
      "12680 : loss:  0.8667196 \t acc:  0.59\n",
      "12720 : loss:  0.71404326 \t acc:  0.73\n",
      "12760 : loss:  0.68847525 \t acc:  0.73\n",
      "12800 : loss:  0.75370157 \t acc:  0.69\n",
      "12840 : loss:  0.7323124 \t acc:  0.65\n",
      "12880 : loss:  0.71243346 \t acc:  0.69\n",
      "12920 : loss:  0.7884147 \t acc:  0.63\n",
      "12960 : loss:  0.6814349 \t acc:  0.66\n",
      "13000 : loss:  0.7825728 \t acc:  0.63\n",
      "13040 : loss:  0.7471168 \t acc:  0.64\n",
      "13080 : loss:  0.7618196 \t acc:  0.66\n",
      "13120 : loss:  0.77913266 \t acc:  0.62\n",
      "13160 : loss:  0.52523595 \t acc:  0.79\n",
      "13200 : loss:  0.7444145 \t acc:  0.65\n",
      "13240 : loss:  0.6940033 \t acc:  0.63\n",
      "13280 : loss:  0.6890075 \t acc:  0.68\n",
      "13320 : loss:  0.8310362 \t acc:  0.62\n",
      "13360 : loss:  0.79817283 \t acc:  0.63\n",
      "13400 : loss:  0.7257318 \t acc:  0.69\n",
      "13440 : loss:  0.6603567 \t acc:  0.76\n",
      "\n",
      "Saving...\n",
      "saved to models/LSTM/pretrained_model.ckpt-13448\n",
      "\n",
      "500 \t [359.71496833 245.90209001 373.30710418]\n",
      "2 \tval accuracy:  0.66818 \t f_! score:  [0.71942994 0.49180418 0.74661421]\n",
      "\n",
      "13480 : loss:  0.74762374 \t acc:  0.61\n",
      "13520 : loss:  0.6703744 \t acc:  0.66\n",
      "13560 : loss:  0.6192066 \t acc:  0.72\n",
      "13600 : loss:  0.6889702 \t acc:  0.67\n",
      "13640 : loss:  0.6688499 \t acc:  0.7\n",
      "13680 : loss:  0.67361236 \t acc:  0.69\n",
      "13720 : loss:  0.5765235 \t acc:  0.76\n",
      "13760 : loss:  0.694755 \t acc:  0.7\n",
      "13800 : loss:  0.7142332 \t acc:  0.65\n",
      "13840 : loss:  0.7353554 \t acc:  0.66\n",
      "13880 : loss:  0.6218715 \t acc:  0.75\n",
      "13920 : loss:  0.7669173 \t acc:  0.67\n",
      "13960 : loss:  0.79588664 \t acc:  0.73\n",
      "14000 : loss:  0.85904175 \t acc:  0.65\n",
      "14040 : loss:  0.72274965 \t acc:  0.66\n",
      "14080 : loss:  0.6853382 \t acc:  0.73\n",
      "14120 : loss:  0.65472734 \t acc:  0.71\n",
      "14160 : loss:  0.6098184 \t acc:  0.69\n",
      "14200 : loss:  0.61766493 \t acc:  0.77\n",
      "14240 : loss:  0.814915 \t acc:  0.67\n",
      "14280 : loss:  0.61419433 \t acc:  0.76\n",
      "14320 : loss:  0.80747586 \t acc:  0.65\n",
      "14360 : loss:  0.676693 \t acc:  0.7\n",
      "14400 : loss:  0.6655732 \t acc:  0.72\n",
      "14440 : loss:  0.693387 \t acc:  0.75\n",
      "14480 : loss:  0.6970774 \t acc:  0.67\n",
      "14520 : loss:  0.7872096 \t acc:  0.63\n",
      "14560 : loss:  0.6215065 \t acc:  0.7\n",
      "14600 : loss:  0.6699688 \t acc:  0.66\n",
      "14640 : loss:  0.73262876 \t acc:  0.72\n",
      "14680 : loss:  0.76129067 \t acc:  0.66\n",
      "14720 : loss:  0.7881153 \t acc:  0.63\n",
      "14760 : loss:  0.6972409 \t acc:  0.66\n",
      "14800 : loss:  0.78021204 \t acc:  0.72\n",
      "14840 : loss:  0.67387396 \t acc:  0.69\n",
      "14880 : loss:  0.79631424 \t acc:  0.67\n",
      "14920 : loss:  0.75346625 \t acc:  0.64\n",
      "14960 : loss:  0.5726587 \t acc:  0.77\n",
      "15000 : loss:  0.8522049 \t acc:  0.61\n",
      "15040 : loss:  0.79319525 \t acc:  0.62\n",
      "15080 : loss:  0.5991281 \t acc:  0.74\n",
      "15120 : loss:  0.691725 \t acc:  0.69\n",
      "15160 : loss:  0.81526166 \t acc:  0.64\n",
      "15200 : loss:  0.7735042 \t acc:  0.62\n",
      "15240 : loss:  0.7232952 \t acc:  0.62\n",
      "15280 : loss:  0.74855906 \t acc:  0.66\n",
      "15320 : loss:  0.6771805 \t acc:  0.65\n",
      "15360 : loss:  0.7973279 \t acc:  0.67\n",
      "15400 : loss:  0.69206786 \t acc:  0.67\n",
      "15440 : loss:  0.6519953 \t acc:  0.67\n",
      "15480 : loss:  0.6232083 \t acc:  0.72\n",
      "15520 : loss:  0.7423521 \t acc:  0.64\n",
      "15560 : loss:  0.784477 \t acc:  0.62\n",
      "15600 : loss:  0.79654574 \t acc:  0.67\n",
      "15640 : loss:  0.66758937 \t acc:  0.71\n",
      "15680 : loss:  0.65728563 \t acc:  0.74\n",
      "15720 : loss:  0.6570214 \t acc:  0.71\n",
      "15760 : loss:  0.7710176 \t acc:  0.66\n",
      "15800 : loss:  0.82968765 \t acc:  0.68\n",
      "15840 : loss:  0.6817796 \t acc:  0.67\n",
      "15880 : loss:  0.66200805 \t acc:  0.7\n",
      "15920 : loss:  0.70696336 \t acc:  0.69\n",
      "15960 : loss:  0.7459633 \t acc:  0.65\n",
      "16000 : loss:  0.66022843 \t acc:  0.73\n",
      "16040 : loss:  0.688952 \t acc:  0.7\n",
      "16080 : loss:  0.82950217 \t acc:  0.58\n",
      "16120 : loss:  0.5896234 \t acc:  0.74\n",
      "16160 : loss:  0.80909276 \t acc:  0.59\n",
      "16200 : loss:  0.65499365 \t acc:  0.75\n",
      "16240 : loss:  0.7184519 \t acc:  0.69\n",
      "16280 : loss:  0.6977465 \t acc:  0.68\n",
      "16320 : loss:  0.670573 \t acc:  0.71\n",
      "16360 : loss:  0.7886972 \t acc:  0.6\n",
      "16400 : loss:  0.76249045 \t acc:  0.66\n",
      "16440 : loss:  0.7698089 \t acc:  0.64\n",
      "16480 : loss:  0.7492197 \t acc:  0.65\n",
      "16520 : loss:  0.6406173 \t acc:  0.68\n",
      "16560 : loss:  0.6926305 \t acc:  0.7\n",
      "16600 : loss:  0.62759364 \t acc:  0.72\n",
      "16640 : loss:  0.7993133 \t acc:  0.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16680 : loss:  0.8098502 \t acc:  0.62\n",
      "16720 : loss:  0.6302205 \t acc:  0.67\n",
      "16760 : loss:  0.6700049 \t acc:  0.67\n",
      "16800 : loss:  0.61106485 \t acc:  0.71\n",
      "16840 : loss:  0.8074692 \t acc:  0.64\n",
      "16880 : loss:  0.50269586 \t acc:  0.81\n",
      "16920 : loss:  0.74592865 \t acc:  0.6\n",
      "16960 : loss:  0.6956534 \t acc:  0.7\n",
      "17000 : loss:  0.56016785 \t acc:  0.75\n",
      "17040 : loss:  0.6812143 \t acc:  0.7\n",
      "17080 : loss:  0.6847325 \t acc:  0.69\n",
      "17120 : loss:  0.7188933 \t acc:  0.7\n",
      "17160 : loss:  0.7352941 \t acc:  0.65\n",
      "17200 : loss:  0.77711785 \t acc:  0.69\n",
      "17240 : loss:  0.6083536 \t acc:  0.78\n",
      "17280 : loss:  0.7842056 \t acc:  0.61\n",
      "17320 : loss:  0.64266694 \t acc:  0.73\n",
      "17360 : loss:  0.639391 \t acc:  0.72\n",
      "17400 : loss:  0.741584 \t acc:  0.64\n",
      "17440 : loss:  0.7258455 \t acc:  0.65\n",
      "17480 : loss:  0.7295664 \t acc:  0.72\n",
      "17520 : loss:  0.6083281 \t acc:  0.73\n",
      "17560 : loss:  0.6672686 \t acc:  0.7\n",
      "17600 : loss:  0.7168007 \t acc:  0.68\n",
      "17640 : loss:  0.7175062 \t acc:  0.63\n",
      "17680 : loss:  0.5582262 \t acc:  0.74\n",
      "17720 : loss:  0.67812103 \t acc:  0.71\n",
      "17760 : loss:  0.6300329 \t acc:  0.69\n",
      "17800 : loss:  0.586543 \t acc:  0.77\n",
      "17840 : loss:  0.62844443 \t acc:  0.74\n",
      "17880 : loss:  0.76749504 \t acc:  0.65\n",
      "17920 : loss:  0.63556147 \t acc:  0.73\n",
      "\n",
      "Saving...\n",
      "saved to models/LSTM/pretrained_model.ckpt-17931\n",
      "\n",
      "500 \t [358.20700911 248.14721423 382.9026122 ]\n",
      "3 \tval accuracy:  0.6773 \t f_! score:  [0.71641402 0.49629443 0.76580522]\n",
      "\n",
      "500 \t [358.20700911 248.14721423 382.9026122 ]\n",
      "500 \t [371.53337481 254.44945768 368.4518643 ]\n",
      "500 \t [348.66709767 246.58026413 401.39140086]\n",
      "500 \t [18794. 13823. 17383.]\n",
      "--- Test   Review ---\n",
      "0.6773\n",
      "f1:  [0.71641402 0.49629443 0.76580522]\n",
      "65730\n",
      "Vorsicht! es muss vorher ein  LSTM  Netz abgespeichert worden sein\n",
      "\n",
      "Restoring...\n",
      "models/LSTM/pretrained_model.ckpt-17931\n",
      "INFO:tensorflow:Restoring parameters from models/LSTM/pretrained_model.ckpt-17931\n",
      "acc:  0.3\n",
      "acc:  0.31\n",
      "acc:  0.31\n",
      "acc:  0.27\n",
      "acc:  0.44\n",
      "acc:  0.34\n",
      "acc:  0.38\n",
      "acc:  0.27\n",
      "acc:  0.29\n",
      "acc:  0.35\n",
      "acc:  0.31\n",
      "acc:  0.36\n",
      "acc:  0.33\n",
      "acc:  0.29\n",
      "acc:  0.36\n",
      "acc:  0.25\n",
      "acc:  0.31\n",
      "acc:  0.33\n",
      "acc:  0.33\n",
      "acc:  0.27\n",
      "acc:  0.24\n",
      "acc:  0.38\n",
      "acc:  0.3\n",
      "acc:  0.35\n",
      "acc:  0.28\n",
      "acc:  0.3\n",
      "acc:  0.33\n",
      "acc:  0.26\n",
      "acc:  0.31\n",
      "acc:  0.31\n",
      "acc:  0.36\n",
      "acc:  0.3\n",
      "acc:  0.29\n",
      "acc:  0.32\n",
      "acc:  0.35\n",
      "acc:  0.44\n",
      "acc:  0.25\n",
      "acc:  0.26\n",
      "acc:  0.35\n",
      "acc:  0.24\n",
      "acc:  0.29\n",
      "acc:  0.36\n",
      "acc:  0.35\n",
      "acc:  0.34\n",
      "acc:  0.25\n",
      "acc:  0.36\n",
      "acc:  0.22\n",
      "acc:  0.27\n",
      "acc:  0.36\n",
      "acc:  0.39\n",
      "acc:  0.28\n",
      "acc:  0.33\n",
      "acc:  0.37\n",
      "acc:  0.27\n",
      "acc:  0.35\n",
      "acc:  0.25\n",
      "acc:  0.32\n",
      "acc:  0.33\n",
      "acc:  0.35\n",
      "acc:  0.27\n",
      "acc:  0.31\n",
      "acc:  0.3\n",
      "acc:  0.34\n",
      "acc:  0.34\n",
      "acc:  0.25\n",
      "acc:  0.29\n",
      "acc:  0.39\n",
      "acc:  0.33\n",
      "acc:  0.33\n",
      "acc:  0.33\n",
      "acc:  0.35\n",
      "acc:  0.29\n",
      "acc:  0.32\n",
      "acc:  0.23\n",
      "acc:  0.34\n",
      "acc:  0.33\n",
      "acc:  0.31\n",
      "acc:  0.35\n",
      "acc:  0.33\n",
      "acc:  0.39\n",
      "acc:  0.4\n",
      "acc:  0.29\n",
      "acc:  0.26\n",
      "acc:  0.28\n",
      "acc:  0.27\n",
      "acc:  0.42\n",
      "acc:  0.32\n",
      "acc:  0.32\n",
      "acc:  0.35\n",
      "acc:  0.33\n",
      "acc:  0.36\n",
      "acc:  0.24\n",
      "acc:  0.3\n",
      "acc:  0.33\n",
      "acc:  0.35\n",
      "acc:  0.37\n",
      "acc:  0.23\n",
      "acc:  0.32\n",
      "acc:  0.3\n",
      "acc:  0.35\n",
      "\n",
      "100 \t [42.56994442 16.85279119 22.02326361]\n",
      "val accuracy:  0.3177 \t f_! score:  [0.42569944 0.16852791 0.22023264]\n",
      "\n",
      "100 \t [42.56994442 16.85279119 22.02326361]\n",
      "100 \t [31.21973127 34.15243183 32.96889421]\n",
      "100 \t [67.93983726 11.40353922 16.97228126]\n",
      "100 \t [3277. 3406. 3317.]\n",
      "---just Test  Twitter ---\n",
      "0.3177\n",
      "f1:  [0.42569944 0.16852791 0.22023264]\n",
      "--- 8.473462343215942 seconds ---\n",
      "65730\n",
      "Vorsicht! es muss vorher ein  LSTM  Netz abgespeichert worden sein\n",
      "\n",
      "Restoring...\n",
      "models/LSTM/pretrained_model.ckpt-17931\n",
      "INFO:tensorflow:Restoring parameters from models/LSTM/pretrained_model.ckpt-17931\n",
      "0 : loss:  2.9393833 \t acc:  0.29\n",
      "2 : loss:  1.2477514 \t acc:  0.4\n",
      "4 : loss:  1.3991218 \t acc:  0.38\n",
      "6 : loss:  1.2937877 \t acc:  0.39\n",
      "8 : loss:  1.290649 \t acc:  0.36\n",
      "10 : loss:  1.0633457 \t acc:  0.44\n",
      "12 : loss:  1.1641855 \t acc:  0.37\n",
      "14 : loss:  1.0620451 \t acc:  0.42\n",
      "16 : loss:  1.0671343 \t acc:  0.43\n",
      "18 : loss:  1.0691127 \t acc:  0.48\n",
      "20 : loss:  1.1281437 \t acc:  0.44\n",
      "22 : loss:  1.0810076 \t acc:  0.42\n",
      "24 : loss:  1.070703 \t acc:  0.42\n",
      "26 : loss:  1.0619121 \t acc:  0.49\n",
      "28 : loss:  1.0315735 \t acc:  0.47\n",
      "30 : loss:  1.1024159 \t acc:  0.4\n",
      "32 : loss:  1.1121444 \t acc:  0.33\n",
      "34 : loss:  1.0408071 \t acc:  0.42\n",
      "36 : loss:  1.0851278 \t acc:  0.36\n",
      "38 : loss:  1.0326291 \t acc:  0.46\n",
      "40 : loss:  1.0711211 \t acc:  0.4\n",
      "42 : loss:  1.0165163 \t acc:  0.46\n",
      "44 : loss:  1.1378334 \t acc:  0.32\n",
      "46 : loss:  1.0590067 \t acc:  0.42\n",
      "48 : loss:  1.0645976 \t acc:  0.47\n",
      "50 : loss:  1.0552047 \t acc:  0.46\n",
      "52 : loss:  0.9503575 \t acc:  0.51\n",
      "54 : loss:  1.1371254 \t acc:  0.39\n",
      "56 : loss:  1.0652488 \t acc:  0.43\n",
      "58 : loss:  1.1628766 \t acc:  0.39\n",
      "60 : loss:  1.1073596 \t acc:  0.51\n",
      "62 : loss:  1.0847969 \t acc:  0.43\n",
      "64 : loss:  1.1513515 \t acc:  0.39\n",
      "66 : loss:  1.041317 \t acc:  0.44\n",
      "68 : loss:  1.1743828 \t acc:  0.48\n",
      "70 : loss:  1.051566 \t acc:  0.44\n",
      "72 : loss:  1.0468146 \t acc:  0.37\n",
      "74 : loss:  1.0511581 \t acc:  0.42\n",
      "76 : loss:  1.0472571 \t acc:  0.44\n",
      "78 : loss:  1.1270095 \t acc:  0.44\n",
      "80 : loss:  0.9873163 \t acc:  0.54\n",
      "82 : loss:  1.0880123 \t acc:  0.37\n",
      "84 : loss:  1.0908072 \t acc:  0.4\n",
      "86 : loss:  1.1181858 \t acc:  0.37\n",
      "88 : loss:  0.9877976 \t acc:  0.47\n",
      "90 : loss:  1.0004916 \t acc:  0.53\n",
      "92 : loss:  0.99848855 \t acc:  0.51\n",
      "94 : loss:  1.0724277 \t acc:  0.47\n",
      "96 : loss:  0.98121595 \t acc:  0.44\n",
      "98 : loss:  1.1557865 \t acc:  0.32\n",
      "100 : loss:  0.9776437 \t acc:  0.43\n",
      "102 : loss:  1.1092014 \t acc:  0.5\n",
      "104 : loss:  1.0115526 \t acc:  0.47\n",
      "106 : loss:  1.070742 \t acc:  0.45\n",
      "108 : loss:  0.9650692 \t acc:  0.52\n",
      "110 : loss:  0.991821 \t acc:  0.47\n",
      "112 : loss:  0.9416798 \t acc:  0.56\n",
      "114 : loss:  1.0615646 \t acc:  0.48\n",
      "116 : loss:  0.9527318 \t acc:  0.52\n",
      "118 : loss:  0.97756463 \t acc:  0.49\n",
      "120 : loss:  1.1060328 \t acc:  0.41\n",
      "122 : loss:  1.0558743 \t acc:  0.45\n",
      "124 : loss:  1.0261161 \t acc:  0.41\n",
      "126 : loss:  1.1153644 \t acc:  0.44\n",
      "128 : loss:  0.96292084 \t acc:  0.55\n",
      "130 : loss:  1.0504141 \t acc:  0.43\n",
      "132 : loss:  1.0429877 \t acc:  0.43\n",
      "134 : loss:  1.053104 \t acc:  0.42\n",
      "136 : loss:  1.0284553 \t acc:  0.48\n",
      "138 : loss:  0.94849443 \t acc:  0.56\n",
      "140 : loss:  1.0107307 \t acc:  0.49\n",
      "142 : loss:  1.0366783 \t acc:  0.46\n",
      "144 : loss:  1.0796813 \t acc:  0.42\n",
      "146 : loss:  0.9683705 \t acc:  0.55\n",
      "148 : loss:  0.9524337 \t acc:  0.56\n",
      "150 : loss:  1.0375447 \t acc:  0.43\n",
      "152 : loss:  1.0225348 \t acc:  0.47\n",
      "154 : loss:  0.9757454 \t acc:  0.49\n",
      "156 : loss:  1.0476149 \t acc:  0.47\n",
      "158 : loss:  1.0657055 \t acc:  0.46\n",
      "160 : loss:  0.9361355 \t acc:  0.51\n",
      "162 : loss:  0.9697541 \t acc:  0.49\n",
      "164 : loss:  1.0511087 \t acc:  0.4\n",
      "166 : loss:  1.0681913 \t acc:  0.42\n",
      "168 : loss:  0.99042207 \t acc:  0.5\n",
      "170 : loss:  1.0417452 \t acc:  0.47\n",
      "172 : loss:  0.9503386 \t acc:  0.6\n",
      "174 : loss:  1.159431 \t acc:  0.38\n",
      "176 : loss:  1.0685186 \t acc:  0.39\n",
      "178 : loss:  1.0214891 \t acc:  0.47\n",
      "180 : loss:  1.108449 \t acc:  0.47\n",
      "182 : loss:  1.0211389 \t acc:  0.46\n",
      "184 : loss:  1.0663058 \t acc:  0.44\n",
      "186 : loss:  1.0569372 \t acc:  0.49\n",
      "188 : loss:  1.1223798 \t acc:  0.37\n",
      "190 : loss:  0.9600738 \t acc:  0.53\n",
      "192 : loss:  0.9961637 \t acc:  0.5\n",
      "194 : loss:  1.029824 \t acc:  0.45\n",
      "196 : loss:  1.0413272 \t acc:  0.42\n",
      "198 : loss:  1.1173187 \t acc:  0.38\n",
      "200 : loss:  0.99242175 \t acc:  0.51\n",
      "202 : loss:  0.9991337 \t acc:  0.46\n",
      "204 : loss:  1.0002006 \t acc:  0.44\n",
      "206 : loss:  0.9780862 \t acc:  0.51\n",
      "208 : loss:  0.95388377 \t acc:  0.54\n",
      "210 : loss:  0.92788905 \t acc:  0.55\n",
      "212 : loss:  0.8827861 \t acc:  0.6\n",
      "214 : loss:  0.9934903 \t acc:  0.51\n",
      "216 : loss:  0.91190547 \t acc:  0.56\n",
      "218 : loss:  1.0836638 \t acc:  0.4\n",
      "220 : loss:  0.97162336 \t acc:  0.52\n",
      "222 : loss:  1.0188161 \t acc:  0.36\n",
      "224 : loss:  0.86302376 \t acc:  0.56\n",
      "226 : loss:  0.97925264 \t acc:  0.5\n",
      "228 : loss:  0.98303914 \t acc:  0.49\n",
      "230 : loss:  0.9824917 \t acc:  0.52\n",
      "232 : loss:  1.0288898 \t acc:  0.47\n",
      "234 : loss:  1.0389113 \t acc:  0.41\n",
      "236 : loss:  1.1197373 \t acc:  0.46\n",
      "238 : loss:  0.9877965 \t acc:  0.46\n",
      "240 : loss:  1.00108 \t acc:  0.48\n",
      "242 : loss:  1.0441822 \t acc:  0.49\n",
      "244 : loss:  1.085188 \t acc:  0.38\n",
      "246 : loss:  1.1102251 \t acc:  0.44\n",
      "248 : loss:  1.1038758 \t acc:  0.48\n",
      "250 : loss:  1.0062821 \t acc:  0.48\n",
      "252 : loss:  0.9026772 \t acc:  0.56\n",
      "254 : loss:  0.9017842 \t acc:  0.57\n",
      "256 : loss:  1.0050505 \t acc:  0.53\n",
      "258 : loss:  1.0174155 \t acc:  0.51\n",
      "260 : loss:  1.021934 \t acc:  0.49\n",
      "262 : loss:  1.0267266 \t acc:  0.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264 : loss:  0.94292605 \t acc:  0.56\n",
      "266 : loss:  0.9882885 \t acc:  0.51\n",
      "268 : loss:  0.9752312 \t acc:  0.49\n",
      "270 : loss:  0.98917466 \t acc:  0.47\n",
      "272 : loss:  1.1303511 \t acc:  0.44\n",
      "274 : loss:  0.94451416 \t acc:  0.53\n",
      "276 : loss:  0.97833335 \t acc:  0.57\n",
      "278 : loss:  0.99547386 \t acc:  0.51\n",
      "280 : loss:  1.234004 \t acc:  0.37\n",
      "282 : loss:  1.0640261 \t acc:  0.48\n",
      "284 : loss:  1.0721151 \t acc:  0.41\n",
      "286 : loss:  0.92787623 \t acc:  0.54\n",
      "288 : loss:  1.0860611 \t acc:  0.45\n",
      "290 : loss:  0.9510665 \t acc:  0.49\n",
      "292 : loss:  1.0049347 \t acc:  0.53\n",
      "294 : loss:  1.0385232 \t acc:  0.46\n",
      "296 : loss:  0.99661225 \t acc:  0.51\n",
      "298 : loss:  1.0461285 \t acc:  0.52\n",
      "300 : loss:  0.95419496 \t acc:  0.51\n",
      "302 : loss:  0.96795446 \t acc:  0.46\n",
      "304 : loss:  1.1343881 \t acc:  0.46\n",
      "306 : loss:  0.962255 \t acc:  0.5\n",
      "308 : loss:  1.0240172 \t acc:  0.45\n",
      "310 : loss:  1.0223472 \t acc:  0.51\n",
      "312 : loss:  0.9712879 \t acc:  0.51\n",
      "314 : loss:  0.9379371 \t acc:  0.52\n",
      "316 : loss:  0.98055196 \t acc:  0.47\n",
      "318 : loss:  0.93368894 \t acc:  0.52\n",
      "320 : loss:  0.9580419 \t acc:  0.5\n",
      "322 : loss:  0.8910313 \t acc:  0.51\n",
      "324 : loss:  1.0336345 \t acc:  0.46\n",
      "326 : loss:  0.94105536 \t acc:  0.55\n",
      "328 : loss:  1.0287197 \t acc:  0.45\n",
      "330 : loss:  0.94593567 \t acc:  0.55\n",
      "332 : loss:  0.9896261 \t acc:  0.5\n",
      "334 : loss:  1.0371597 \t acc:  0.47\n",
      "336 : loss:  1.0049554 \t acc:  0.49\n",
      "338 : loss:  1.0023985 \t acc:  0.45\n",
      "340 : loss:  0.9891712 \t acc:  0.53\n",
      "342 : loss:  1.0529059 \t acc:  0.46\n",
      "344 : loss:  0.9649895 \t acc:  0.55\n",
      "346 : loss:  1.0760913 \t acc:  0.43\n",
      "348 : loss:  1.1016427 \t acc:  0.41\n",
      "350 : loss:  0.9535163 \t acc:  0.53\n",
      "352 : loss:  0.94899195 \t acc:  0.57\n",
      "354 : loss:  0.9837243 \t acc:  0.5\n",
      "356 : loss:  1.0080342 \t acc:  0.47\n",
      "358 : loss:  1.1630092 \t acc:  0.4\n",
      "360 : loss:  1.0136875 \t acc:  0.46\n",
      "362 : loss:  1.050559 \t acc:  0.49\n",
      "364 : loss:  0.98576623 \t acc:  0.5\n",
      "366 : loss:  1.0870106 \t acc:  0.5\n",
      "368 : loss:  1.044012 \t acc:  0.52\n",
      "370 : loss:  1.100807 \t acc:  0.4\n",
      "acc:  0.45\n",
      "acc:  0.61\n",
      "acc:  0.5\n",
      "acc:  0.62\n",
      "acc:  0.55\n",
      "acc:  0.56\n",
      "acc:  0.49\n",
      "acc:  0.48\n",
      "acc:  0.43\n",
      "acc:  0.48\n",
      "acc:  0.57\n",
      "acc:  0.49\n",
      "acc:  0.5\n",
      "acc:  0.53\n",
      "acc:  0.52\n",
      "acc:  0.55\n",
      "acc:  0.56\n",
      "acc:  0.45\n",
      "acc:  0.48\n",
      "acc:  0.54\n",
      "acc:  0.48\n",
      "acc:  0.48\n",
      "acc:  0.53\n",
      "acc:  0.5\n",
      "acc:  0.53\n",
      "acc:  0.38\n",
      "acc:  0.51\n",
      "acc:  0.53\n",
      "acc:  0.46\n",
      "acc:  0.57\n",
      "acc:  0.5\n",
      "acc:  0.49\n",
      "acc:  0.46\n",
      "acc:  0.46\n",
      "acc:  0.45\n",
      "acc:  0.53\n",
      "acc:  0.55\n",
      "acc:  0.49\n",
      "acc:  0.49\n",
      "acc:  0.48\n",
      "acc:  0.48\n",
      "acc:  0.47\n",
      "acc:  0.61\n",
      "acc:  0.48\n",
      "acc:  0.5\n",
      "acc:  0.46\n",
      "acc:  0.53\n",
      "acc:  0.52\n",
      "acc:  0.55\n",
      "acc:  0.52\n",
      "acc:  0.56\n",
      "acc:  0.51\n",
      "acc:  0.54\n",
      "acc:  0.46\n",
      "acc:  0.51\n",
      "acc:  0.54\n",
      "acc:  0.51\n",
      "acc:  0.43\n",
      "acc:  0.44\n",
      "acc:  0.45\n",
      "acc:  0.5\n",
      "acc:  0.51\n",
      "acc:  0.52\n",
      "acc:  0.46\n",
      "acc:  0.48\n",
      "acc:  0.49\n",
      "acc:  0.57\n",
      "acc:  0.51\n",
      "acc:  0.56\n",
      "acc:  0.54\n",
      "acc:  0.58\n",
      "acc:  0.46\n",
      "acc:  0.51\n",
      "acc:  0.57\n",
      "acc:  0.48\n",
      "acc:  0.55\n",
      "acc:  0.56\n",
      "acc:  0.55\n",
      "acc:  0.54\n",
      "acc:  0.47\n",
      "acc:  0.55\n",
      "acc:  0.51\n",
      "acc:  0.43\n",
      "acc:  0.54\n",
      "acc:  0.49\n",
      "acc:  0.54\n",
      "acc:  0.53\n",
      "acc:  0.5\n",
      "acc:  0.45\n",
      "acc:  0.53\n",
      "acc:  0.57\n",
      "acc:  0.46\n",
      "acc:  0.59\n",
      "acc:  0.5\n",
      "acc:  0.51\n",
      "acc:  0.54\n",
      "acc:  0.5\n",
      "acc:  0.53\n",
      "acc:  0.56\n",
      "acc:  0.55\n",
      "\n",
      "100 \t [60.83478075 42.26368657 44.6748819 ]\n",
      "0 \tval accuracy:  0.51089996 \t f_! score:  [0.60834781 0.42263687 0.44674882]\n",
      "\n",
      "100 \t [60.83478075 42.26368657 44.6748819 ]\n",
      "100 \t [51.54185763 54.79525253 47.9712717 ]\n",
      "100 \t [75.35767125 35.05375535 42.62961175]\n",
      "100 \t [3348. 3219. 3433.]\n",
      "---train_last_layer Test  Twitter ---\n",
      "0.51089996\n",
      "f1:  [0.60834781 0.42263687 0.44674882]\n",
      "--- 59.59993267059326 seconds ---\n",
      "\n",
      "\n",
      "  Twitter\n",
      "65730\n",
      "\n",
      "Restoring...\n",
      "models/LSTM/pretrained_model.ckpt-17931\n",
      "INFO:tensorflow:Restoring parameters from models/LSTM/pretrained_model.ckpt-17931\n",
      "---init ready   LSTM ---\n",
      "\n",
      "Saving...\n",
      "saved to models/LSTM/pretrained_model.ckpt-0\n",
      "0 : loss:  2.169774 \t acc:  0.37\n",
      "2 : loss:  1.1045249 \t acc:  0.33\n",
      "4 : loss:  1.1502464 \t acc:  0.37\n",
      "6 : loss:  1.1887939 \t acc:  0.33\n",
      "8 : loss:  1.1994232 \t acc:  0.33\n",
      "10 : loss:  1.0487086 \t acc:  0.54\n",
      "12 : loss:  1.1942582 \t acc:  0.39\n",
      "14 : loss:  1.1378673 \t acc:  0.38\n",
      "16 : loss:  1.05157 \t acc:  0.46\n",
      "18 : loss:  1.1037327 \t acc:  0.43\n",
      "20 : loss:  1.128677 \t acc:  0.39\n",
      "22 : loss:  1.1052917 \t acc:  0.35\n",
      "24 : loss:  1.0395069 \t acc:  0.44\n",
      "26 : loss:  1.0461084 \t acc:  0.49\n",
      "28 : loss:  1.0414921 \t acc:  0.53\n",
      "30 : loss:  1.1581801 \t acc:  0.41\n",
      "32 : loss:  1.0531293 \t acc:  0.43\n",
      "34 : loss:  1.1005253 \t acc:  0.35\n",
      "36 : loss:  0.99714243 \t acc:  0.45\n",
      "38 : loss:  1.0502 \t acc:  0.39\n",
      "40 : loss:  0.9797996 \t acc:  0.49\n",
      "42 : loss:  1.0245061 \t acc:  0.44\n",
      "44 : loss:  0.988331 \t acc:  0.51\n",
      "46 : loss:  0.97446066 \t acc:  0.49\n",
      "48 : loss:  1.0273932 \t acc:  0.45\n",
      "50 : loss:  1.016955 \t acc:  0.48\n",
      "52 : loss:  1.000825 \t acc:  0.46\n",
      "54 : loss:  0.96786773 \t acc:  0.51\n",
      "56 : loss:  1.0348899 \t acc:  0.44\n",
      "58 : loss:  1.0765455 \t acc:  0.4\n",
      "60 : loss:  0.97814345 \t acc:  0.48\n",
      "62 : loss:  1.0106907 \t acc:  0.53\n",
      "64 : loss:  0.99695563 \t acc:  0.51\n",
      "66 : loss:  0.98126924 \t acc:  0.5\n",
      "68 : loss:  1.0210506 \t acc:  0.42\n",
      "70 : loss:  0.9795341 \t acc:  0.49\n",
      "72 : loss:  0.9551143 \t acc:  0.63\n",
      "74 : loss:  0.93036824 \t acc:  0.5\n",
      "76 : loss:  1.0511876 \t acc:  0.52\n",
      "78 : loss:  0.8945536 \t acc:  0.59\n",
      "80 : loss:  0.85714906 \t acc:  0.6\n",
      "82 : loss:  0.94520414 \t acc:  0.54\n",
      "84 : loss:  0.959525 \t acc:  0.48\n",
      "86 : loss:  0.8930619 \t acc:  0.53\n",
      "88 : loss:  1.1739672 \t acc:  0.35\n",
      "90 : loss:  1.0058655 \t acc:  0.47\n",
      "92 : loss:  1.0790604 \t acc:  0.33\n",
      "94 : loss:  1.0869063 \t acc:  0.43\n",
      "96 : loss:  0.9672551 \t acc:  0.51\n",
      "98 : loss:  0.92147464 \t acc:  0.58\n",
      "100 : loss:  0.91126937 \t acc:  0.48\n",
      "102 : loss:  0.8988098 \t acc:  0.62\n",
      "104 : loss:  0.9102565 \t acc:  0.56\n",
      "106 : loss:  0.93569374 \t acc:  0.53\n",
      "108 : loss:  0.92158836 \t acc:  0.5\n",
      "110 : loss:  0.8821482 \t acc:  0.57\n",
      "112 : loss:  0.88831526 \t acc:  0.61\n",
      "114 : loss:  0.9637688 \t acc:  0.55\n",
      "116 : loss:  0.9351185 \t acc:  0.54\n",
      "118 : loss:  0.9213714 \t acc:  0.57\n",
      "120 : loss:  1.0016911 \t acc:  0.48\n",
      "122 : loss:  0.9291335 \t acc:  0.56\n",
      "124 : loss:  0.90383184 \t acc:  0.54\n",
      "126 : loss:  0.94619936 \t acc:  0.54\n",
      "128 : loss:  0.88942426 \t acc:  0.59\n",
      "130 : loss:  0.9219666 \t acc:  0.56\n",
      "132 : loss:  0.90864265 \t acc:  0.53\n",
      "134 : loss:  0.96296525 \t acc:  0.48\n",
      "136 : loss:  0.88211894 \t acc:  0.6\n",
      "138 : loss:  0.9268771 \t acc:  0.52\n",
      "140 : loss:  0.98519194 \t acc:  0.42\n",
      "142 : loss:  0.89509416 \t acc:  0.58\n",
      "144 : loss:  0.96215093 \t acc:  0.5\n",
      "146 : loss:  0.9020865 \t acc:  0.64\n",
      "148 : loss:  0.917699 \t acc:  0.54\n",
      "150 : loss:  0.96674806 \t acc:  0.5\n",
      "152 : loss:  0.86723167 \t acc:  0.61\n",
      "154 : loss:  0.94987977 \t acc:  0.51\n",
      "156 : loss:  0.930302 \t acc:  0.54\n",
      "158 : loss:  0.9747279 \t acc:  0.47\n",
      "160 : loss:  0.9346433 \t acc:  0.56\n",
      "162 : loss:  0.87892365 \t acc:  0.6\n",
      "164 : loss:  0.9543613 \t acc:  0.56\n",
      "166 : loss:  0.8368084 \t acc:  0.64\n",
      "168 : loss:  0.94234526 \t acc:  0.57\n",
      "170 : loss:  0.95483726 \t acc:  0.49\n",
      "172 : loss:  0.9743349 \t acc:  0.52\n",
      "174 : loss:  0.8610178 \t acc:  0.57\n",
      "176 : loss:  0.87777436 \t acc:  0.58\n",
      "178 : loss:  0.9265622 \t acc:  0.53\n",
      "180 : loss:  0.86436594 \t acc:  0.62\n",
      "182 : loss:  0.88571525 \t acc:  0.55\n",
      "184 : loss:  0.96950126 \t acc:  0.5\n",
      "186 : loss:  0.9965584 \t acc:  0.5\n",
      "188 : loss:  0.9545684 \t acc:  0.5\n",
      "190 : loss:  0.9647483 \t acc:  0.48\n",
      "192 : loss:  0.8406443 \t acc:  0.65\n",
      "194 : loss:  0.987885 \t acc:  0.48\n",
      "196 : loss:  0.89762336 \t acc:  0.56\n",
      "198 : loss:  1.0186101 \t acc:  0.49\n",
      "200 : loss:  1.0144011 \t acc:  0.51\n",
      "202 : loss:  0.8803981 \t acc:  0.58\n",
      "204 : loss:  0.85381097 \t acc:  0.67\n",
      "206 : loss:  0.9079924 \t acc:  0.57\n",
      "208 : loss:  0.90648806 \t acc:  0.55\n",
      "210 : loss:  0.91079134 \t acc:  0.54\n",
      "212 : loss:  0.88544565 \t acc:  0.65\n",
      "214 : loss:  1.0291052 \t acc:  0.47\n",
      "216 : loss:  0.8814664 \t acc:  0.56\n",
      "218 : loss:  1.0038152 \t acc:  0.51\n",
      "220 : loss:  0.95663613 \t acc:  0.55\n",
      "222 : loss:  0.8732374 \t acc:  0.6\n",
      "224 : loss:  0.89469534 \t acc:  0.57\n",
      "226 : loss:  0.90511286 \t acc:  0.62\n",
      "228 : loss:  0.9280594 \t acc:  0.5\n",
      "230 : loss:  0.9284948 \t acc:  0.56\n",
      "232 : loss:  0.94886696 \t acc:  0.52\n",
      "234 : loss:  0.89195234 \t acc:  0.55\n",
      "236 : loss:  0.8780233 \t acc:  0.56\n",
      "238 : loss:  0.9672058 \t acc:  0.5\n",
      "240 : loss:  0.87148863 \t acc:  0.61\n",
      "242 : loss:  0.843088 \t acc:  0.57\n",
      "244 : loss:  0.9223357 \t acc:  0.55\n",
      "246 : loss:  0.965483 \t acc:  0.55\n",
      "248 : loss:  0.9353374 \t acc:  0.47\n",
      "250 : loss:  0.98321635 \t acc:  0.52\n",
      "252 : loss:  0.8791984 \t acc:  0.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254 : loss:  0.79032636 \t acc:  0.62\n",
      "256 : loss:  0.9499983 \t acc:  0.54\n",
      "258 : loss:  1.0306144 \t acc:  0.46\n",
      "260 : loss:  0.8868401 \t acc:  0.62\n",
      "262 : loss:  0.86444503 \t acc:  0.63\n",
      "264 : loss:  0.89560837 \t acc:  0.53\n",
      "266 : loss:  0.96528715 \t acc:  0.49\n",
      "268 : loss:  0.8517289 \t acc:  0.56\n",
      "270 : loss:  0.92493635 \t acc:  0.57\n",
      "272 : loss:  1.0169477 \t acc:  0.52\n",
      "274 : loss:  0.95121264 \t acc:  0.5\n",
      "276 : loss:  0.94595754 \t acc:  0.56\n",
      "278 : loss:  0.8663441 \t acc:  0.61\n",
      "280 : loss:  0.93387574 \t acc:  0.5\n",
      "282 : loss:  0.9329169 \t acc:  0.56\n",
      "284 : loss:  0.8583034 \t acc:  0.58\n",
      "286 : loss:  0.8869163 \t acc:  0.6\n",
      "288 : loss:  0.8142943 \t acc:  0.57\n",
      "290 : loss:  0.90697294 \t acc:  0.59\n",
      "292 : loss:  0.85511535 \t acc:  0.63\n",
      "294 : loss:  0.73591673 \t acc:  0.72\n",
      "296 : loss:  0.89452225 \t acc:  0.55\n",
      "298 : loss:  0.9038576 \t acc:  0.53\n",
      "300 : loss:  1.0736462 \t acc:  0.43\n",
      "302 : loss:  0.87538147 \t acc:  0.65\n",
      "304 : loss:  0.8592978 \t acc:  0.62\n",
      "306 : loss:  0.8449219 \t acc:  0.59\n",
      "308 : loss:  0.9441926 \t acc:  0.51\n",
      "310 : loss:  0.9659016 \t acc:  0.51\n",
      "312 : loss:  0.88324636 \t acc:  0.54\n",
      "314 : loss:  0.97921526 \t acc:  0.51\n",
      "316 : loss:  0.9551782 \t acc:  0.5\n",
      "318 : loss:  0.8537197 \t acc:  0.64\n",
      "320 : loss:  0.9091183 \t acc:  0.57\n",
      "322 : loss:  0.8920744 \t acc:  0.51\n",
      "324 : loss:  0.8640313 \t acc:  0.55\n",
      "326 : loss:  0.84332305 \t acc:  0.57\n",
      "328 : loss:  0.7788698 \t acc:  0.71\n",
      "330 : loss:  0.9114413 \t acc:  0.57\n",
      "332 : loss:  0.92092085 \t acc:  0.6\n",
      "334 : loss:  0.9259737 \t acc:  0.53\n",
      "336 : loss:  0.82507396 \t acc:  0.59\n",
      "338 : loss:  0.91924345 \t acc:  0.56\n",
      "340 : loss:  0.86743295 \t acc:  0.55\n",
      "342 : loss:  1.0285428 \t acc:  0.45\n",
      "344 : loss:  0.84984976 \t acc:  0.64\n",
      "346 : loss:  0.87067294 \t acc:  0.56\n",
      "348 : loss:  0.9438651 \t acc:  0.49\n",
      "350 : loss:  0.8373103 \t acc:  0.63\n",
      "352 : loss:  0.9749769 \t acc:  0.54\n",
      "354 : loss:  0.9125371 \t acc:  0.63\n",
      "356 : loss:  0.90674645 \t acc:  0.55\n",
      "358 : loss:  0.8867269 \t acc:  0.6\n",
      "360 : loss:  0.9914827 \t acc:  0.49\n",
      "362 : loss:  0.87848115 \t acc:  0.56\n",
      "364 : loss:  0.93784773 \t acc:  0.5\n",
      "366 : loss:  0.93961424 \t acc:  0.56\n",
      "368 : loss:  0.90664315 \t acc:  0.59\n",
      "370 : loss:  0.8891239 \t acc:  0.62\n",
      "\n",
      "Saving...\n",
      "saved to models/LSTM/pretrained_model.ckpt-372\n",
      "\n",
      "100 \t [64.19358995 51.17025035 53.65551507]\n",
      "0 \tval accuracy:  0.5707 \t f_! score:  [0.6419359  0.5117025  0.53655515]\n",
      "\n",
      "100 \t [64.19358995 51.17025035 53.65551507]\n",
      "100 \t [60.05064864 62.21246111 50.88128119]\n",
      "100 \t [69.82960633 43.96439825 57.77185267]\n",
      "100 \t [3290. 3354. 3356.]\n",
      "--- Test   Twitter ---\n",
      "0.5707\n",
      "f1:  [0.6419359  0.5117025  0.53655515]\n",
      "--- 71.14059400558472 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([2.169774,\n",
       "  1.1045249,\n",
       "  1.1502464,\n",
       "  1.1887939,\n",
       "  1.1994232,\n",
       "  1.0487086,\n",
       "  1.1942582,\n",
       "  1.1378673,\n",
       "  1.05157,\n",
       "  1.1037327,\n",
       "  1.128677,\n",
       "  1.1052917,\n",
       "  1.0395069,\n",
       "  1.0461084,\n",
       "  1.0414921,\n",
       "  1.1581801,\n",
       "  1.0531293,\n",
       "  1.1005253,\n",
       "  0.99714243,\n",
       "  1.0502,\n",
       "  0.9797996,\n",
       "  1.0245061,\n",
       "  0.988331,\n",
       "  0.97446066,\n",
       "  1.0273932,\n",
       "  1.016955,\n",
       "  1.000825,\n",
       "  0.96786773,\n",
       "  1.0348899,\n",
       "  1.0765455,\n",
       "  0.97814345,\n",
       "  1.0106907,\n",
       "  0.99695563,\n",
       "  0.98126924,\n",
       "  1.0210506,\n",
       "  0.9795341,\n",
       "  0.9551143,\n",
       "  0.93036824,\n",
       "  1.0511876,\n",
       "  0.8945536,\n",
       "  0.85714906,\n",
       "  0.94520414,\n",
       "  0.959525,\n",
       "  0.8930619,\n",
       "  1.1739672,\n",
       "  1.0058655,\n",
       "  1.0790604,\n",
       "  1.0869063,\n",
       "  0.9672551,\n",
       "  0.92147464,\n",
       "  0.91126937,\n",
       "  0.8988098,\n",
       "  0.9102565,\n",
       "  0.93569374,\n",
       "  0.92158836,\n",
       "  0.8821482,\n",
       "  0.88831526,\n",
       "  0.9637688,\n",
       "  0.9351185,\n",
       "  0.9213714,\n",
       "  1.0016911,\n",
       "  0.9291335,\n",
       "  0.90383184,\n",
       "  0.94619936,\n",
       "  0.88942426,\n",
       "  0.9219666,\n",
       "  0.90864265,\n",
       "  0.96296525,\n",
       "  0.88211894,\n",
       "  0.9268771,\n",
       "  0.98519194,\n",
       "  0.89509416,\n",
       "  0.96215093,\n",
       "  0.9020865,\n",
       "  0.917699,\n",
       "  0.96674806,\n",
       "  0.86723167,\n",
       "  0.94987977,\n",
       "  0.930302,\n",
       "  0.9747279,\n",
       "  0.9346433,\n",
       "  0.87892365,\n",
       "  0.9543613,\n",
       "  0.8368084,\n",
       "  0.94234526,\n",
       "  0.95483726,\n",
       "  0.9743349,\n",
       "  0.8610178,\n",
       "  0.87777436,\n",
       "  0.9265622,\n",
       "  0.86436594,\n",
       "  0.88571525,\n",
       "  0.96950126,\n",
       "  0.9965584,\n",
       "  0.9545684,\n",
       "  0.9647483,\n",
       "  0.8406443,\n",
       "  0.987885,\n",
       "  0.89762336,\n",
       "  1.0186101,\n",
       "  1.0144011,\n",
       "  0.8803981,\n",
       "  0.85381097,\n",
       "  0.9079924,\n",
       "  0.90648806,\n",
       "  0.91079134,\n",
       "  0.88544565,\n",
       "  1.0291052,\n",
       "  0.8814664,\n",
       "  1.0038152,\n",
       "  0.95663613,\n",
       "  0.8732374,\n",
       "  0.89469534,\n",
       "  0.90511286,\n",
       "  0.9280594,\n",
       "  0.9284948,\n",
       "  0.94886696,\n",
       "  0.89195234,\n",
       "  0.8780233,\n",
       "  0.9672058,\n",
       "  0.87148863,\n",
       "  0.843088,\n",
       "  0.9223357,\n",
       "  0.965483,\n",
       "  0.9353374,\n",
       "  0.98321635,\n",
       "  0.8791984,\n",
       "  0.79032636,\n",
       "  0.9499983,\n",
       "  1.0306144,\n",
       "  0.8868401,\n",
       "  0.86444503,\n",
       "  0.89560837,\n",
       "  0.96528715,\n",
       "  0.8517289,\n",
       "  0.92493635,\n",
       "  1.0169477,\n",
       "  0.95121264,\n",
       "  0.94595754,\n",
       "  0.8663441,\n",
       "  0.93387574,\n",
       "  0.9329169,\n",
       "  0.8583034,\n",
       "  0.8869163,\n",
       "  0.8142943,\n",
       "  0.90697294,\n",
       "  0.85511535,\n",
       "  0.73591673,\n",
       "  0.89452225,\n",
       "  0.9038576,\n",
       "  1.0736462,\n",
       "  0.87538147,\n",
       "  0.8592978,\n",
       "  0.8449219,\n",
       "  0.9441926,\n",
       "  0.9659016,\n",
       "  0.88324636,\n",
       "  0.97921526,\n",
       "  0.9551782,\n",
       "  0.8537197,\n",
       "  0.9091183,\n",
       "  0.8920744,\n",
       "  0.8640313,\n",
       "  0.84332305,\n",
       "  0.7788698,\n",
       "  0.9114413,\n",
       "  0.92092085,\n",
       "  0.9259737,\n",
       "  0.82507396,\n",
       "  0.91924345,\n",
       "  0.86743295,\n",
       "  1.0285428,\n",
       "  0.84984976,\n",
       "  0.87067294,\n",
       "  0.9438651,\n",
       "  0.8373103,\n",
       "  0.9749769,\n",
       "  0.9125371,\n",
       "  0.90674645,\n",
       "  0.8867269,\n",
       "  0.9914827,\n",
       "  0.87848115,\n",
       "  0.93784773,\n",
       "  0.93961424,\n",
       "  0.90664315,\n",
       "  0.8891239],\n",
       " [0.37,\n",
       "  0.33,\n",
       "  0.37,\n",
       "  0.33,\n",
       "  0.33,\n",
       "  0.54,\n",
       "  0.39,\n",
       "  0.38,\n",
       "  0.46,\n",
       "  0.43,\n",
       "  0.39,\n",
       "  0.35,\n",
       "  0.44,\n",
       "  0.49,\n",
       "  0.53,\n",
       "  0.41,\n",
       "  0.43,\n",
       "  0.35,\n",
       "  0.45,\n",
       "  0.39,\n",
       "  0.49,\n",
       "  0.44,\n",
       "  0.51,\n",
       "  0.49,\n",
       "  0.45,\n",
       "  0.48,\n",
       "  0.46,\n",
       "  0.51,\n",
       "  0.44,\n",
       "  0.4,\n",
       "  0.48,\n",
       "  0.53,\n",
       "  0.51,\n",
       "  0.5,\n",
       "  0.42,\n",
       "  0.49,\n",
       "  0.63,\n",
       "  0.5,\n",
       "  0.52,\n",
       "  0.59,\n",
       "  0.6,\n",
       "  0.54,\n",
       "  0.48,\n",
       "  0.53,\n",
       "  0.35,\n",
       "  0.47,\n",
       "  0.33,\n",
       "  0.43,\n",
       "  0.51,\n",
       "  0.58,\n",
       "  0.48,\n",
       "  0.62,\n",
       "  0.56,\n",
       "  0.53,\n",
       "  0.5,\n",
       "  0.57,\n",
       "  0.61,\n",
       "  0.55,\n",
       "  0.54,\n",
       "  0.57,\n",
       "  0.48,\n",
       "  0.56,\n",
       "  0.54,\n",
       "  0.54,\n",
       "  0.59,\n",
       "  0.56,\n",
       "  0.53,\n",
       "  0.48,\n",
       "  0.6,\n",
       "  0.52,\n",
       "  0.42,\n",
       "  0.58,\n",
       "  0.5,\n",
       "  0.64,\n",
       "  0.54,\n",
       "  0.5,\n",
       "  0.61,\n",
       "  0.51,\n",
       "  0.54,\n",
       "  0.47,\n",
       "  0.56,\n",
       "  0.6,\n",
       "  0.56,\n",
       "  0.64,\n",
       "  0.57,\n",
       "  0.49,\n",
       "  0.52,\n",
       "  0.57,\n",
       "  0.58,\n",
       "  0.53,\n",
       "  0.62,\n",
       "  0.55,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.48,\n",
       "  0.65,\n",
       "  0.48,\n",
       "  0.56,\n",
       "  0.49,\n",
       "  0.51,\n",
       "  0.58,\n",
       "  0.67,\n",
       "  0.57,\n",
       "  0.55,\n",
       "  0.54,\n",
       "  0.65,\n",
       "  0.47,\n",
       "  0.56,\n",
       "  0.51,\n",
       "  0.55,\n",
       "  0.6,\n",
       "  0.57,\n",
       "  0.62,\n",
       "  0.5,\n",
       "  0.56,\n",
       "  0.52,\n",
       "  0.55,\n",
       "  0.56,\n",
       "  0.5,\n",
       "  0.61,\n",
       "  0.57,\n",
       "  0.55,\n",
       "  0.55,\n",
       "  0.47,\n",
       "  0.52,\n",
       "  0.6,\n",
       "  0.62,\n",
       "  0.54,\n",
       "  0.46,\n",
       "  0.62,\n",
       "  0.63,\n",
       "  0.53,\n",
       "  0.49,\n",
       "  0.56,\n",
       "  0.57,\n",
       "  0.52,\n",
       "  0.5,\n",
       "  0.56,\n",
       "  0.61,\n",
       "  0.5,\n",
       "  0.56,\n",
       "  0.58,\n",
       "  0.6,\n",
       "  0.57,\n",
       "  0.59,\n",
       "  0.63,\n",
       "  0.72,\n",
       "  0.55,\n",
       "  0.53,\n",
       "  0.43,\n",
       "  0.65,\n",
       "  0.62,\n",
       "  0.59,\n",
       "  0.51,\n",
       "  0.51,\n",
       "  0.54,\n",
       "  0.51,\n",
       "  0.5,\n",
       "  0.64,\n",
       "  0.57,\n",
       "  0.51,\n",
       "  0.55,\n",
       "  0.57,\n",
       "  0.71,\n",
       "  0.57,\n",
       "  0.6,\n",
       "  0.53,\n",
       "  0.59,\n",
       "  0.56,\n",
       "  0.55,\n",
       "  0.45,\n",
       "  0.64,\n",
       "  0.56,\n",
       "  0.49,\n",
       "  0.63,\n",
       "  0.54,\n",
       "  0.63,\n",
       "  0.55,\n",
       "  0.6,\n",
       "  0.49,\n",
       "  0.56,\n",
       "  0.5,\n",
       "  0.56,\n",
       "  0.59,\n",
       "  0.62])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_name=\"LSTM\"\n",
    "review_loss, review_acc = testhelper.train_input(\"Review\", net_name, epochs=4)\n",
    "\n",
    "testhelper.just_test(\"Twitter\", net_name, print_time=True)\n",
    "testhelper.train_last_layer(\"Twitter\", net_name, epochs=1, print_time=True)\n",
    "testhelper.train_input(\"Twitter\", net_name, epochs=1, print_time=True, restore=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
