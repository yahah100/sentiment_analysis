{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yannik/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from TestHelper import TestHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  Review\n",
      "delete old models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yannik/ba/TestHelper.py:64: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  input_data = self.decide_which_input(input_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509411\n",
      "Tensor(\"ConvNet/dense/BiasAdd:0\", shape=(?, 3), dtype=float32)\n",
      "Tensor(\"ConvNet_1/dense/BiasAdd:0\", shape=(?, 3), dtype=float32)\n",
      "---init ready   CNN ---\n",
      "\n",
      "Saving...\n",
      "saved to models/CNN/pretrained_model.ckpt-0\n",
      "0 : loss:  2.6452563 \t acc:  0.37\n",
      "40 : loss:  1.088457 \t acc:  0.4\n",
      "80 : loss:  1.0956731 \t acc:  0.33\n",
      "120 : loss:  1.1076199 \t acc:  0.4\n",
      "160 : loss:  1.0824168 \t acc:  0.41\n",
      "200 : loss:  1.0706793 \t acc:  0.42\n",
      "240 : loss:  1.0819454 \t acc:  0.5\n",
      "280 : loss:  1.1111405 \t acc:  0.31\n",
      "320 : loss:  1.0689805 \t acc:  0.44\n",
      "360 : loss:  1.0724491 \t acc:  0.41\n",
      "400 : loss:  0.995045 \t acc:  0.51\n",
      "440 : loss:  0.9874031 \t acc:  0.5\n",
      "480 : loss:  1.059461 \t acc:  0.45\n",
      "520 : loss:  0.94260734 \t acc:  0.52\n",
      "560 : loss:  0.9680862 \t acc:  0.53\n",
      "600 : loss:  0.95310247 \t acc:  0.54\n",
      "640 : loss:  0.99682176 \t acc:  0.46\n",
      "680 : loss:  1.0508763 \t acc:  0.45\n",
      "720 : loss:  0.8909299 \t acc:  0.63\n",
      "760 : loss:  0.91548944 \t acc:  0.6\n",
      "800 : loss:  0.93060684 \t acc:  0.54\n",
      "840 : loss:  0.87830824 \t acc:  0.58\n",
      "880 : loss:  0.81946313 \t acc:  0.59\n",
      "920 : loss:  0.8463608 \t acc:  0.63\n",
      "960 : loss:  0.93532175 \t acc:  0.51\n",
      "1000 : loss:  0.8643407 \t acc:  0.62\n",
      "1040 : loss:  0.8485814 \t acc:  0.66\n",
      "1080 : loss:  0.8312763 \t acc:  0.58\n",
      "1120 : loss:  0.7189798 \t acc:  0.7\n",
      "1160 : loss:  0.76797754 \t acc:  0.63\n",
      "1200 : loss:  0.91621584 \t acc:  0.52\n",
      "1240 : loss:  0.8913866 \t acc:  0.61\n",
      "1280 : loss:  0.8383925 \t acc:  0.62\n",
      "1320 : loss:  0.8849172 \t acc:  0.6\n",
      "1360 : loss:  0.8773067 \t acc:  0.61\n",
      "1400 : loss:  0.94831663 \t acc:  0.56\n",
      "1440 : loss:  0.8631851 \t acc:  0.61\n",
      "1480 : loss:  0.81151795 \t acc:  0.63\n",
      "1520 : loss:  0.73599434 \t acc:  0.7\n",
      "1560 : loss:  0.9096811 \t acc:  0.63\n",
      "1600 : loss:  0.8588684 \t acc:  0.59\n",
      "1640 : loss:  0.75930786 \t acc:  0.61\n",
      "1680 : loss:  0.9338248 \t acc:  0.52\n",
      "1720 : loss:  0.8485222 \t acc:  0.59\n",
      "1760 : loss:  0.8678437 \t acc:  0.62\n",
      "1800 : loss:  0.87514436 \t acc:  0.58\n",
      "1840 : loss:  0.85372454 \t acc:  0.58\n",
      "1880 : loss:  0.84450287 \t acc:  0.62\n",
      "1920 : loss:  0.84864306 \t acc:  0.57\n",
      "1960 : loss:  0.88286895 \t acc:  0.58\n",
      "2000 : loss:  0.7286734 \t acc:  0.68\n",
      "2040 : loss:  0.75107956 \t acc:  0.68\n",
      "2080 : loss:  0.80948454 \t acc:  0.64\n",
      "2120 : loss:  0.9275835 \t acc:  0.58\n",
      "2160 : loss:  0.7831704 \t acc:  0.62\n",
      "2200 : loss:  0.8102693 \t acc:  0.61\n",
      "2240 : loss:  0.7546044 \t acc:  0.7\n",
      "2280 : loss:  0.7873424 \t acc:  0.63\n",
      "2320 : loss:  0.7875652 \t acc:  0.63\n",
      "2360 : loss:  0.71925604 \t acc:  0.69\n",
      "2400 : loss:  0.7176153 \t acc:  0.7\n",
      "2440 : loss:  0.7905143 \t acc:  0.68\n",
      "2480 : loss:  0.83516955 \t acc:  0.63\n",
      "2520 : loss:  0.7934557 \t acc:  0.58\n",
      "2560 : loss:  1.0225309 \t acc:  0.5\n",
      "2600 : loss:  0.73286855 \t acc:  0.65\n",
      "2640 : loss:  0.62030524 \t acc:  0.77\n",
      "2680 : loss:  0.7186879 \t acc:  0.64\n",
      "2720 : loss:  0.8817079 \t acc:  0.59\n",
      "2760 : loss:  0.9679639 \t acc:  0.52\n",
      "2800 : loss:  0.71326333 \t acc:  0.69\n",
      "2840 : loss:  0.8141567 \t acc:  0.62\n",
      "2880 : loss:  0.8218488 \t acc:  0.62\n",
      "2920 : loss:  0.8368882 \t acc:  0.65\n",
      "2960 : loss:  0.81064063 \t acc:  0.65\n",
      "3000 : loss:  0.7698849 \t acc:  0.64\n",
      "3040 : loss:  0.8310293 \t acc:  0.64\n",
      "3080 : loss:  0.75490165 \t acc:  0.69\n",
      "3120 : loss:  0.7926719 \t acc:  0.64\n",
      "3160 : loss:  0.87994593 \t acc:  0.59\n",
      "3200 : loss:  0.79995406 \t acc:  0.65\n",
      "3240 : loss:  0.69895554 \t acc:  0.7\n",
      "3280 : loss:  0.7082254 \t acc:  0.67\n",
      "3320 : loss:  0.83691484 \t acc:  0.58\n",
      "3360 : loss:  0.78878456 \t acc:  0.67\n",
      "3400 : loss:  0.75382775 \t acc:  0.65\n",
      "3440 : loss:  0.84596515 \t acc:  0.61\n",
      "3480 : loss:  0.87756205 \t acc:  0.59\n",
      "3520 : loss:  0.70022506 \t acc:  0.71\n",
      "3560 : loss:  0.7705595 \t acc:  0.67\n",
      "3600 : loss:  0.8085763 \t acc:  0.63\n",
      "3640 : loss:  0.8637091 \t acc:  0.57\n",
      "3680 : loss:  0.94713557 \t acc:  0.54\n",
      "3720 : loss:  0.7405278 \t acc:  0.74\n",
      "3760 : loss:  0.7774838 \t acc:  0.63\n",
      "3800 : loss:  0.88450354 \t acc:  0.58\n",
      "3840 : loss:  0.7944483 \t acc:  0.61\n",
      "3880 : loss:  0.8001802 \t acc:  0.64\n",
      "3920 : loss:  0.6941367 \t acc:  0.69\n",
      "3960 : loss:  0.81899786 \t acc:  0.61\n",
      "4000 : loss:  0.70013064 \t acc:  0.67\n",
      "4040 : loss:  0.743059 \t acc:  0.62\n",
      "4080 : loss:  0.8452145 \t acc:  0.62\n",
      "4120 : loss:  0.7392459 \t acc:  0.69\n",
      "4160 : loss:  0.6856871 \t acc:  0.69\n",
      "4200 : loss:  0.67783433 \t acc:  0.64\n",
      "4240 : loss:  0.70064217 \t acc:  0.7\n",
      "4280 : loss:  0.6845502 \t acc:  0.67\n",
      "4320 : loss:  0.6861619 \t acc:  0.69\n",
      "4360 : loss:  0.7367723 \t acc:  0.63\n",
      "4400 : loss:  0.7730931 \t acc:  0.63\n",
      "4440 : loss:  0.7233924 \t acc:  0.7\n",
      "4480 : loss:  0.73039275 \t acc:  0.67\n",
      "\n",
      "Saving...\n",
      "saved to models/CNN/pretrained_model.ckpt-4482\n",
      "\n",
      "500 \t [361.75726987 246.11860523 363.6720097 ]\n",
      "0 \tval accuracy:  0.6638 \t f_! score:  [0.72351454 0.49223721 0.72734402]\n",
      "\n",
      "4520 : loss:  0.76298577 \t acc:  0.66\n",
      "4560 : loss:  0.7377101 \t acc:  0.7\n",
      "4600 : loss:  0.7187358 \t acc:  0.68\n",
      "4640 : loss:  0.76058424 \t acc:  0.71\n",
      "4680 : loss:  0.710198 \t acc:  0.71\n",
      "4720 : loss:  0.740275 \t acc:  0.66\n",
      "4760 : loss:  0.6573227 \t acc:  0.73\n",
      "4800 : loss:  0.74292654 \t acc:  0.67\n",
      "4840 : loss:  0.6357492 \t acc:  0.72\n",
      "4880 : loss:  0.8133659 \t acc:  0.53\n",
      "4920 : loss:  0.6041924 \t acc:  0.75\n",
      "4960 : loss:  0.7508992 \t acc:  0.62\n",
      "5000 : loss:  0.7895388 \t acc:  0.61\n",
      "5040 : loss:  0.8315144 \t acc:  0.58\n",
      "5080 : loss:  0.7404128 \t acc:  0.61\n",
      "5120 : loss:  0.73853874 \t acc:  0.64\n",
      "5160 : loss:  0.8527965 \t acc:  0.57\n",
      "5200 : loss:  0.6815659 \t acc:  0.72\n",
      "5240 : loss:  0.8187027 \t acc:  0.65\n",
      "5280 : loss:  0.74799407 \t acc:  0.66\n",
      "5320 : loss:  0.6769771 \t acc:  0.7\n",
      "5360 : loss:  0.83860826 \t acc:  0.62\n",
      "5400 : loss:  0.6473865 \t acc:  0.68\n",
      "5440 : loss:  0.664671 \t acc:  0.7\n",
      "5480 : loss:  0.71954286 \t acc:  0.63\n",
      "5520 : loss:  0.6328574 \t acc:  0.71\n",
      "5560 : loss:  0.73605245 \t acc:  0.69\n",
      "5600 : loss:  0.8435121 \t acc:  0.64\n",
      "5640 : loss:  0.7305372 \t acc:  0.67\n",
      "5680 : loss:  0.6593082 \t acc:  0.68\n",
      "5720 : loss:  0.76553065 \t acc:  0.64\n",
      "5760 : loss:  0.6928062 \t acc:  0.7\n",
      "5800 : loss:  0.694927 \t acc:  0.71\n",
      "5840 : loss:  0.8387639 \t acc:  0.68\n",
      "5880 : loss:  0.6568354 \t acc:  0.73\n",
      "5920 : loss:  0.56327134 \t acc:  0.74\n",
      "5960 : loss:  0.7719024 \t acc:  0.68\n",
      "6000 : loss:  0.7029924 \t acc:  0.67\n",
      "6040 : loss:  0.68897784 \t acc:  0.72\n",
      "6080 : loss:  0.6651633 \t acc:  0.73\n",
      "6120 : loss:  0.69414216 \t acc:  0.68\n",
      "6160 : loss:  0.6753067 \t acc:  0.67\n",
      "6200 : loss:  0.7160484 \t acc:  0.65\n",
      "6240 : loss:  0.8151964 \t acc:  0.61\n",
      "6280 : loss:  0.7828712 \t acc:  0.65\n",
      "6320 : loss:  0.72564775 \t acc:  0.67\n",
      "6360 : loss:  0.76135427 \t acc:  0.66\n",
      "6400 : loss:  0.8220435 \t acc:  0.69\n",
      "6440 : loss:  0.6676312 \t acc:  0.69\n",
      "6480 : loss:  0.7543161 \t acc:  0.65\n",
      "6520 : loss:  0.67657363 \t acc:  0.72\n",
      "6560 : loss:  0.70484644 \t acc:  0.71\n",
      "6600 : loss:  0.6180942 \t acc:  0.71\n",
      "6640 : loss:  0.76359713 \t acc:  0.68\n",
      "6680 : loss:  0.8160701 \t acc:  0.64\n",
      "6720 : loss:  0.6385916 \t acc:  0.68\n",
      "6760 : loss:  0.752684 \t acc:  0.68\n",
      "6800 : loss:  0.69046956 \t acc:  0.68\n",
      "6840 : loss:  0.6977213 \t acc:  0.69\n",
      "6880 : loss:  0.6445233 \t acc:  0.73\n",
      "6920 : loss:  0.6739842 \t acc:  0.69\n",
      "6960 : loss:  0.6446459 \t acc:  0.67\n",
      "7000 : loss:  0.6741944 \t acc:  0.7\n",
      "7040 : loss:  0.8051313 \t acc:  0.67\n",
      "7080 : loss:  0.69360834 \t acc:  0.66\n",
      "7120 : loss:  0.67147475 \t acc:  0.69\n",
      "7160 : loss:  0.7099808 \t acc:  0.68\n",
      "7200 : loss:  0.7087177 \t acc:  0.71\n",
      "7240 : loss:  0.7208056 \t acc:  0.68\n",
      "7280 : loss:  0.64307904 \t acc:  0.73\n",
      "7320 : loss:  0.71400225 \t acc:  0.67\n",
      "7360 : loss:  0.6502669 \t acc:  0.68\n",
      "7400 : loss:  0.7167579 \t acc:  0.63\n",
      "7440 : loss:  0.76084733 \t acc:  0.67\n",
      "7480 : loss:  0.7303428 \t acc:  0.72\n",
      "7520 : loss:  0.65166724 \t acc:  0.66\n",
      "7560 : loss:  0.81430745 \t acc:  0.63\n",
      "7600 : loss:  0.71411985 \t acc:  0.71\n",
      "7640 : loss:  0.749167 \t acc:  0.65\n",
      "7680 : loss:  0.87345046 \t acc:  0.56\n",
      "7720 : loss:  0.62835956 \t acc:  0.75\n",
      "7760 : loss:  0.7403884 \t acc:  0.63\n",
      "7800 : loss:  0.8282165 \t acc:  0.62\n",
      "7840 : loss:  0.71279716 \t acc:  0.7\n",
      "7880 : loss:  0.6623616 \t acc:  0.69\n",
      "7920 : loss:  0.7706449 \t acc:  0.64\n",
      "7960 : loss:  0.7669854 \t acc:  0.65\n",
      "8000 : loss:  0.831106 \t acc:  0.63\n",
      "8040 : loss:  0.7784854 \t acc:  0.64\n",
      "8080 : loss:  0.57450527 \t acc:  0.74\n",
      "8120 : loss:  0.78125054 \t acc:  0.66\n",
      "8160 : loss:  0.7202345 \t acc:  0.68\n",
      "8200 : loss:  0.698013 \t acc:  0.7\n",
      "8240 : loss:  0.8157616 \t acc:  0.64\n",
      "8280 : loss:  0.66764337 \t acc:  0.71\n",
      "8320 : loss:  0.5912223 \t acc:  0.77\n",
      "8360 : loss:  0.5930625 \t acc:  0.72\n",
      "8400 : loss:  0.72486055 \t acc:  0.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8440 : loss:  0.6571445 \t acc:  0.76\n",
      "8480 : loss:  0.69824487 \t acc:  0.67\n",
      "8520 : loss:  0.8568828 \t acc:  0.61\n",
      "8560 : loss:  0.8243735 \t acc:  0.59\n",
      "8600 : loss:  0.75207514 \t acc:  0.67\n",
      "8640 : loss:  0.7984459 \t acc:  0.66\n",
      "8680 : loss:  0.84120905 \t acc:  0.61\n",
      "8720 : loss:  0.7008446 \t acc:  0.64\n",
      "8760 : loss:  0.729886 \t acc:  0.64\n",
      "8800 : loss:  0.8205315 \t acc:  0.64\n",
      "8840 : loss:  0.73473203 \t acc:  0.63\n",
      "8880 : loss:  0.7075785 \t acc:  0.75\n",
      "8920 : loss:  0.5935524 \t acc:  0.74\n",
      "8960 : loss:  0.7545572 \t acc:  0.61\n",
      "\n",
      "Saving...\n",
      "saved to models/CNN/pretrained_model.ckpt-8965\n",
      "\n",
      "500 \t [364.32365109 229.14058994 369.70517538]\n",
      "1 \tval accuracy:  0.66940004 \t f_! score:  [0.7286473  0.45828118 0.73941035]\n",
      "\n",
      "9000 : loss:  0.5957317 \t acc:  0.7\n",
      "9040 : loss:  0.62176347 \t acc:  0.71\n",
      "9080 : loss:  0.6852441 \t acc:  0.67\n",
      "9120 : loss:  0.64732313 \t acc:  0.68\n",
      "9160 : loss:  0.6868683 \t acc:  0.66\n",
      "9200 : loss:  0.61826956 \t acc:  0.68\n",
      "9240 : loss:  0.8329335 \t acc:  0.62\n",
      "9280 : loss:  0.7583683 \t acc:  0.63\n",
      "9320 : loss:  0.65197885 \t acc:  0.75\n",
      "9360 : loss:  0.7609504 \t acc:  0.69\n",
      "9400 : loss:  0.691497 \t acc:  0.68\n",
      "9440 : loss:  0.81519806 \t acc:  0.63\n",
      "9480 : loss:  0.7244279 \t acc:  0.71\n",
      "9520 : loss:  0.68941545 \t acc:  0.68\n",
      "9560 : loss:  0.66195375 \t acc:  0.7\n",
      "9600 : loss:  0.7256182 \t acc:  0.66\n",
      "9640 : loss:  0.7779322 \t acc:  0.69\n",
      "9680 : loss:  0.75005203 \t acc:  0.69\n",
      "9720 : loss:  0.8299762 \t acc:  0.63\n",
      "9760 : loss:  0.8003267 \t acc:  0.57\n",
      "9800 : loss:  0.72277856 \t acc:  0.69\n",
      "9840 : loss:  0.71451217 \t acc:  0.64\n",
      "9880 : loss:  0.6260864 \t acc:  0.7\n",
      "9920 : loss:  0.56772894 \t acc:  0.78\n",
      "9960 : loss:  0.61642504 \t acc:  0.77\n",
      "10000 : loss:  0.8732447 \t acc:  0.59\n",
      "10040 : loss:  0.68211955 \t acc:  0.74\n",
      "10080 : loss:  0.72829324 \t acc:  0.66\n",
      "10120 : loss:  0.7147133 \t acc:  0.69\n",
      "10160 : loss:  0.7240285 \t acc:  0.63\n",
      "10200 : loss:  0.6829159 \t acc:  0.66\n",
      "10240 : loss:  0.75372505 \t acc:  0.68\n",
      "10280 : loss:  0.8062106 \t acc:  0.65\n",
      "10320 : loss:  0.6108492 \t acc:  0.75\n",
      "10360 : loss:  0.7902222 \t acc:  0.62\n",
      "10400 : loss:  0.79164094 \t acc:  0.69\n",
      "10440 : loss:  0.7635711 \t acc:  0.68\n",
      "10480 : loss:  0.84319425 \t acc:  0.64\n",
      "10520 : loss:  0.7167799 \t acc:  0.73\n",
      "10560 : loss:  0.6886659 \t acc:  0.72\n",
      "10600 : loss:  0.582669 \t acc:  0.73\n",
      "10640 : loss:  0.7643263 \t acc:  0.62\n",
      "10680 : loss:  0.8007303 \t acc:  0.64\n",
      "10720 : loss:  0.6870209 \t acc:  0.75\n",
      "10760 : loss:  0.773561 \t acc:  0.62\n",
      "10800 : loss:  0.6922245 \t acc:  0.62\n",
      "10840 : loss:  0.74733496 \t acc:  0.62\n",
      "10880 : loss:  0.77182233 \t acc:  0.63\n",
      "10920 : loss:  0.7173311 \t acc:  0.68\n",
      "10960 : loss:  0.59969634 \t acc:  0.74\n",
      "11000 : loss:  0.82014686 \t acc:  0.62\n",
      "11040 : loss:  0.788563 \t acc:  0.62\n",
      "11080 : loss:  0.8238501 \t acc:  0.63\n",
      "11120 : loss:  0.76159126 \t acc:  0.72\n",
      "11160 : loss:  0.6437515 \t acc:  0.71\n",
      "11200 : loss:  0.782125 \t acc:  0.6\n",
      "11240 : loss:  0.6641127 \t acc:  0.68\n",
      "11280 : loss:  0.84595054 \t acc:  0.59\n",
      "11320 : loss:  0.7987135 \t acc:  0.63\n",
      "11360 : loss:  0.76613533 \t acc:  0.6\n",
      "11400 : loss:  0.6424571 \t acc:  0.74\n",
      "11440 : loss:  0.71198684 \t acc:  0.68\n",
      "11480 : loss:  0.5210436 \t acc:  0.74\n",
      "11520 : loss:  0.6844973 \t acc:  0.72\n",
      "11560 : loss:  0.75336957 \t acc:  0.7\n",
      "11600 : loss:  0.7139598 \t acc:  0.65\n",
      "11640 : loss:  0.6949771 \t acc:  0.67\n",
      "11680 : loss:  0.66906554 \t acc:  0.66\n",
      "11720 : loss:  0.6637077 \t acc:  0.75\n",
      "11760 : loss:  0.641634 \t acc:  0.7\n",
      "11800 : loss:  0.7589233 \t acc:  0.65\n",
      "11840 : loss:  0.7541476 \t acc:  0.62\n",
      "11880 : loss:  0.62200516 \t acc:  0.7\n",
      "11920 : loss:  0.7581438 \t acc:  0.64\n",
      "11960 : loss:  0.8733359 \t acc:  0.64\n",
      "12000 : loss:  0.6471615 \t acc:  0.65\n",
      "12040 : loss:  0.74322677 \t acc:  0.67\n",
      "12080 : loss:  0.8104029 \t acc:  0.69\n",
      "12120 : loss:  0.6863754 \t acc:  0.67\n",
      "12160 : loss:  0.7256431 \t acc:  0.69\n",
      "12200 : loss:  0.6490331 \t acc:  0.68\n",
      "12240 : loss:  0.74136657 \t acc:  0.68\n",
      "12280 : loss:  0.6102233 \t acc:  0.69\n",
      "12320 : loss:  0.68306327 \t acc:  0.7\n",
      "12360 : loss:  0.6525566 \t acc:  0.73\n",
      "12400 : loss:  0.68984693 \t acc:  0.71\n",
      "12440 : loss:  0.68637556 \t acc:  0.71\n",
      "12480 : loss:  0.6759093 \t acc:  0.69\n",
      "12520 : loss:  0.6746719 \t acc:  0.69\n",
      "12560 : loss:  0.71815115 \t acc:  0.71\n",
      "12600 : loss:  0.6614678 \t acc:  0.72\n",
      "12640 : loss:  0.7028958 \t acc:  0.67\n",
      "12680 : loss:  0.64295304 \t acc:  0.74\n",
      "12720 : loss:  0.7009201 \t acc:  0.71\n",
      "12760 : loss:  0.7162189 \t acc:  0.62\n",
      "12800 : loss:  0.73612857 \t acc:  0.63\n",
      "12840 : loss:  0.77792567 \t acc:  0.62\n",
      "12880 : loss:  0.7570656 \t acc:  0.67\n",
      "12920 : loss:  0.67698914 \t acc:  0.71\n",
      "12960 : loss:  0.75564384 \t acc:  0.67\n",
      "13000 : loss:  0.6885324 \t acc:  0.67\n",
      "13040 : loss:  0.8387985 \t acc:  0.58\n",
      "13080 : loss:  0.6209427 \t acc:  0.74\n",
      "13120 : loss:  0.6090687 \t acc:  0.72\n",
      "13160 : loss:  0.6218032 \t acc:  0.72\n",
      "13200 : loss:  0.6531165 \t acc:  0.7\n",
      "13240 : loss:  0.7039437 \t acc:  0.7\n",
      "13280 : loss:  0.6414673 \t acc:  0.72\n",
      "13320 : loss:  0.73713714 \t acc:  0.69\n",
      "13360 : loss:  0.75675637 \t acc:  0.66\n",
      "13400 : loss:  0.7580465 \t acc:  0.66\n",
      "13440 : loss:  0.6980513 \t acc:  0.73\n",
      "\n",
      "Saving...\n",
      "saved to models/CNN/pretrained_model.ckpt-13448\n",
      "\n",
      "500 \t [369.86465596 236.6890035  375.67553741]\n",
      "2 \tval accuracy:  0.67978 \t f_! score:  [0.73972931 0.47337801 0.75135107]\n",
      "\n",
      "13480 : loss:  0.6970243 \t acc:  0.66\n",
      "13520 : loss:  0.5093291 \t acc:  0.8\n",
      "13560 : loss:  0.54947376 \t acc:  0.8\n",
      "13600 : loss:  0.64009005 \t acc:  0.69\n",
      "13640 : loss:  0.8018823 \t acc:  0.58\n",
      "13680 : loss:  0.51987475 \t acc:  0.78\n",
      "13720 : loss:  0.698185 \t acc:  0.69\n",
      "13760 : loss:  0.605291 \t acc:  0.78\n",
      "13800 : loss:  0.69107056 \t acc:  0.67\n",
      "13840 : loss:  0.7250017 \t acc:  0.72\n",
      "13880 : loss:  0.51437914 \t acc:  0.81\n",
      "13920 : loss:  0.60267603 \t acc:  0.7\n",
      "13960 : loss:  0.65415114 \t acc:  0.71\n",
      "14000 : loss:  0.63984466 \t acc:  0.72\n",
      "14040 : loss:  0.6179765 \t acc:  0.71\n",
      "14080 : loss:  0.7583691 \t acc:  0.68\n",
      "14120 : loss:  0.6689045 \t acc:  0.75\n",
      "14160 : loss:  0.71457726 \t acc:  0.65\n",
      "14200 : loss:  0.69657415 \t acc:  0.74\n",
      "14240 : loss:  0.7566234 \t acc:  0.7\n",
      "14280 : loss:  0.6646935 \t acc:  0.72\n",
      "14320 : loss:  0.7704279 \t acc:  0.69\n",
      "14360 : loss:  0.75585854 \t acc:  0.61\n",
      "14400 : loss:  0.68068093 \t acc:  0.7\n",
      "14440 : loss:  0.6552702 \t acc:  0.73\n",
      "14480 : loss:  0.757685 \t acc:  0.74\n",
      "14520 : loss:  0.5816396 \t acc:  0.72\n",
      "14560 : loss:  0.6184649 \t acc:  0.74\n",
      "14600 : loss:  0.72124934 \t acc:  0.69\n",
      "14640 : loss:  0.66981995 \t acc:  0.69\n",
      "14680 : loss:  0.64925927 \t acc:  0.73\n",
      "14720 : loss:  0.6388778 \t acc:  0.69\n",
      "14760 : loss:  0.6998368 \t acc:  0.71\n",
      "14800 : loss:  0.59585714 \t acc:  0.73\n",
      "14840 : loss:  0.75534606 \t acc:  0.68\n",
      "14880 : loss:  0.6248879 \t acc:  0.75\n",
      "14920 : loss:  0.5888331 \t acc:  0.77\n",
      "14960 : loss:  0.63518864 \t acc:  0.77\n",
      "15000 : loss:  0.6911825 \t acc:  0.67\n",
      "15040 : loss:  0.65055037 \t acc:  0.74\n",
      "15080 : loss:  0.59124523 \t acc:  0.78\n",
      "15120 : loss:  0.7193135 \t acc:  0.67\n",
      "15160 : loss:  0.595695 \t acc:  0.72\n",
      "15200 : loss:  0.6198305 \t acc:  0.73\n",
      "15240 : loss:  0.7251928 \t acc:  0.69\n",
      "15280 : loss:  0.6438805 \t acc:  0.68\n",
      "15320 : loss:  0.774049 \t acc:  0.62\n",
      "15360 : loss:  0.79585564 \t acc:  0.59\n",
      "15400 : loss:  0.8459377 \t acc:  0.64\n",
      "15440 : loss:  0.7246486 \t acc:  0.71\n",
      "15480 : loss:  0.760931 \t acc:  0.67\n",
      "15520 : loss:  0.7516409 \t acc:  0.66\n",
      "15560 : loss:  0.7185096 \t acc:  0.7\n",
      "15600 : loss:  0.66130674 \t acc:  0.73\n",
      "15640 : loss:  0.7766323 \t acc:  0.59\n",
      "15680 : loss:  0.65900713 \t acc:  0.69\n",
      "15720 : loss:  0.7374529 \t acc:  0.63\n",
      "15760 : loss:  0.64258856 \t acc:  0.73\n",
      "15800 : loss:  0.6184336 \t acc:  0.74\n",
      "15840 : loss:  0.76576287 \t acc:  0.68\n",
      "15880 : loss:  0.71025574 \t acc:  0.74\n",
      "15920 : loss:  0.6007398 \t acc:  0.76\n",
      "15960 : loss:  0.6594083 \t acc:  0.66\n",
      "16000 : loss:  0.6692592 \t acc:  0.76\n",
      "16040 : loss:  0.69297963 \t acc:  0.62\n",
      "16080 : loss:  0.5699127 \t acc:  0.77\n",
      "16120 : loss:  0.66037905 \t acc:  0.73\n",
      "16160 : loss:  0.6843704 \t acc:  0.69\n",
      "16200 : loss:  0.7338531 \t acc:  0.73\n",
      "16240 : loss:  0.61313075 \t acc:  0.74\n",
      "16280 : loss:  0.7634285 \t acc:  0.64\n",
      "16320 : loss:  0.85696065 \t acc:  0.57\n",
      "16360 : loss:  0.76311857 \t acc:  0.69\n",
      "16400 : loss:  0.7525545 \t acc:  0.61\n",
      "16440 : loss:  0.661583 \t acc:  0.71\n",
      "16480 : loss:  0.66190374 \t acc:  0.75\n",
      "16520 : loss:  0.6955803 \t acc:  0.67\n",
      "16560 : loss:  0.61898565 \t acc:  0.73\n",
      "16600 : loss:  0.6038068 \t acc:  0.76\n",
      "16640 : loss:  0.61104226 \t acc:  0.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16680 : loss:  0.6578091 \t acc:  0.68\n",
      "16720 : loss:  0.84246475 \t acc:  0.63\n",
      "16760 : loss:  0.6812934 \t acc:  0.66\n",
      "16800 : loss:  0.66778207 \t acc:  0.69\n",
      "16840 : loss:  0.5732059 \t acc:  0.72\n",
      "16880 : loss:  0.77523255 \t acc:  0.65\n",
      "16920 : loss:  0.7146015 \t acc:  0.64\n",
      "16960 : loss:  0.73153305 \t acc:  0.68\n",
      "17000 : loss:  0.61490625 \t acc:  0.7\n",
      "17040 : loss:  0.6222807 \t acc:  0.64\n",
      "17080 : loss:  0.6944443 \t acc:  0.65\n",
      "17120 : loss:  0.75549334 \t acc:  0.65\n",
      "17160 : loss:  0.611995 \t acc:  0.71\n",
      "17200 : loss:  0.6654438 \t acc:  0.72\n",
      "17240 : loss:  0.7670103 \t acc:  0.66\n",
      "17280 : loss:  0.6597748 \t acc:  0.7\n",
      "17320 : loss:  0.7653172 \t acc:  0.68\n",
      "17360 : loss:  0.62512416 \t acc:  0.76\n",
      "17400 : loss:  0.6631145 \t acc:  0.65\n",
      "17440 : loss:  0.7971385 \t acc:  0.64\n",
      "17480 : loss:  0.65089464 \t acc:  0.72\n",
      "17520 : loss:  0.73612094 \t acc:  0.66\n",
      "17560 : loss:  0.67037004 \t acc:  0.73\n",
      "17600 : loss:  0.69359434 \t acc:  0.68\n",
      "17640 : loss:  0.6756481 \t acc:  0.71\n",
      "17680 : loss:  0.77003026 \t acc:  0.64\n",
      "17720 : loss:  0.71328163 \t acc:  0.71\n",
      "17760 : loss:  0.66961265 \t acc:  0.67\n",
      "17800 : loss:  0.6188937 \t acc:  0.73\n",
      "17840 : loss:  0.73898005 \t acc:  0.65\n",
      "17880 : loss:  0.67048234 \t acc:  0.75\n",
      "17920 : loss:  0.531442 \t acc:  0.76\n",
      "\n",
      "Saving...\n",
      "saved to models/CNN/pretrained_model.ckpt-17931\n",
      "\n",
      "500 \t [368.11469382 262.17434736 380.75380475]\n",
      "3 \tval accuracy:  0.68632 \t f_! score:  [0.73622939 0.52434869 0.76150761]\n",
      "\n",
      "17960 : loss:  0.6065417 \t acc:  0.72\n",
      "18000 : loss:  0.7344913 \t acc:  0.68\n",
      "18040 : loss:  0.5732401 \t acc:  0.8\n",
      "18080 : loss:  0.6772071 \t acc:  0.67\n",
      "18120 : loss:  0.70199263 \t acc:  0.71\n",
      "18160 : loss:  0.7779143 \t acc:  0.63\n",
      "18200 : loss:  0.6747406 \t acc:  0.7\n",
      "18240 : loss:  0.64623404 \t acc:  0.67\n",
      "18280 : loss:  0.7461695 \t acc:  0.64\n",
      "18320 : loss:  0.5372868 \t acc:  0.77\n",
      "18360 : loss:  0.6844891 \t acc:  0.63\n",
      "18400 : loss:  0.65923727 \t acc:  0.72\n",
      "18440 : loss:  0.6677613 \t acc:  0.72\n",
      "18480 : loss:  0.7294116 \t acc:  0.62\n",
      "18520 : loss:  0.6102188 \t acc:  0.72\n",
      "18560 : loss:  0.7090865 \t acc:  0.68\n",
      "18600 : loss:  0.6359368 \t acc:  0.69\n",
      "18640 : loss:  0.705821 \t acc:  0.68\n",
      "18680 : loss:  0.6656584 \t acc:  0.72\n",
      "18720 : loss:  0.6217002 \t acc:  0.66\n",
      "18760 : loss:  0.74663794 \t acc:  0.69\n",
      "18800 : loss:  0.6724044 \t acc:  0.73\n",
      "18840 : loss:  0.5730458 \t acc:  0.71\n",
      "18880 : loss:  0.67867136 \t acc:  0.71\n",
      "18920 : loss:  0.74497116 \t acc:  0.72\n",
      "18960 : loss:  0.67624617 \t acc:  0.64\n",
      "19000 : loss:  0.5845074 \t acc:  0.76\n",
      "19040 : loss:  0.76008433 \t acc:  0.61\n",
      "19080 : loss:  0.6497116 \t acc:  0.77\n",
      "19120 : loss:  0.68481797 \t acc:  0.72\n",
      "19160 : loss:  0.6963737 \t acc:  0.67\n",
      "19200 : loss:  0.70002574 \t acc:  0.68\n",
      "19240 : loss:  0.69319826 \t acc:  0.71\n",
      "19280 : loss:  0.61784476 \t acc:  0.77\n",
      "19320 : loss:  0.6108154 \t acc:  0.71\n",
      "19360 : loss:  0.7307199 \t acc:  0.68\n",
      "19400 : loss:  0.588114 \t acc:  0.73\n",
      "19440 : loss:  0.6790041 \t acc:  0.73\n",
      "19480 : loss:  0.8450841 \t acc:  0.6\n",
      "19520 : loss:  0.7850775 \t acc:  0.63\n",
      "19560 : loss:  0.6953566 \t acc:  0.64\n",
      "19600 : loss:  0.6866242 \t acc:  0.73\n",
      "19640 : loss:  0.66641366 \t acc:  0.69\n",
      "19680 : loss:  0.6378919 \t acc:  0.69\n",
      "19720 : loss:  0.68840677 \t acc:  0.73\n",
      "19760 : loss:  0.62560296 \t acc:  0.72\n",
      "19800 : loss:  0.7626699 \t acc:  0.65\n",
      "19840 : loss:  0.5149107 \t acc:  0.79\n",
      "19880 : loss:  0.6835053 \t acc:  0.73\n",
      "19920 : loss:  0.70407045 \t acc:  0.69\n",
      "19960 : loss:  0.80644494 \t acc:  0.69\n",
      "20000 : loss:  0.722851 \t acc:  0.71\n",
      "20040 : loss:  0.7308037 \t acc:  0.65\n",
      "20080 : loss:  0.6261689 \t acc:  0.73\n",
      "20120 : loss:  0.6257273 \t acc:  0.78\n",
      "20160 : loss:  0.561642 \t acc:  0.76\n",
      "20200 : loss:  0.71006715 \t acc:  0.72\n",
      "20240 : loss:  0.6700826 \t acc:  0.7\n",
      "20280 : loss:  0.6787432 \t acc:  0.65\n",
      "20320 : loss:  0.60704714 \t acc:  0.67\n",
      "20360 : loss:  0.5776084 \t acc:  0.78\n",
      "20400 : loss:  0.6461617 \t acc:  0.74\n",
      "20440 : loss:  0.6750835 \t acc:  0.74\n",
      "20480 : loss:  0.7136734 \t acc:  0.69\n",
      "20520 : loss:  0.5964576 \t acc:  0.73\n",
      "20560 : loss:  0.6675759 \t acc:  0.75\n",
      "20600 : loss:  0.6451386 \t acc:  0.71\n",
      "20640 : loss:  0.70004237 \t acc:  0.71\n",
      "20680 : loss:  0.77376854 \t acc:  0.67\n",
      "20720 : loss:  0.7686173 \t acc:  0.71\n",
      "20760 : loss:  0.70607686 \t acc:  0.71\n",
      "20800 : loss:  0.7317387 \t acc:  0.72\n",
      "20840 : loss:  0.6167947 \t acc:  0.71\n",
      "20880 : loss:  0.7229054 \t acc:  0.69\n",
      "20920 : loss:  0.654798 \t acc:  0.71\n",
      "20960 : loss:  0.7483427 \t acc:  0.69\n",
      "21000 : loss:  0.6956934 \t acc:  0.73\n",
      "21040 : loss:  0.7649343 \t acc:  0.68\n",
      "21080 : loss:  0.5212108 \t acc:  0.79\n",
      "21120 : loss:  0.7567812 \t acc:  0.64\n",
      "21160 : loss:  0.55942184 \t acc:  0.72\n",
      "21200 : loss:  0.648808 \t acc:  0.7\n",
      "21240 : loss:  0.67946976 \t acc:  0.69\n",
      "21280 : loss:  0.5924971 \t acc:  0.74\n",
      "21320 : loss:  0.5470448 \t acc:  0.79\n",
      "21360 : loss:  0.7059514 \t acc:  0.7\n",
      "21400 : loss:  0.7800055 \t acc:  0.64\n",
      "21440 : loss:  0.62478805 \t acc:  0.71\n",
      "21480 : loss:  0.6686956 \t acc:  0.71\n",
      "21520 : loss:  0.62701833 \t acc:  0.73\n",
      "21560 : loss:  0.77264875 \t acc:  0.64\n",
      "21600 : loss:  0.8179885 \t acc:  0.6\n",
      "21640 : loss:  0.5668795 \t acc:  0.82\n",
      "21680 : loss:  0.7857368 \t acc:  0.67\n",
      "21720 : loss:  0.6586767 \t acc:  0.66\n",
      "21760 : loss:  0.61184645 \t acc:  0.76\n",
      "21800 : loss:  0.5907911 \t acc:  0.8\n",
      "21840 : loss:  0.7845369 \t acc:  0.62\n",
      "21880 : loss:  0.7238124 \t acc:  0.65\n",
      "21920 : loss:  0.5764191 \t acc:  0.78\n",
      "21960 : loss:  0.59590197 \t acc:  0.75\n",
      "22000 : loss:  0.73493767 \t acc:  0.67\n",
      "22040 : loss:  0.7152386 \t acc:  0.69\n",
      "22080 : loss:  0.63310766 \t acc:  0.75\n",
      "22120 : loss:  0.65176904 \t acc:  0.65\n",
      "22160 : loss:  0.67487466 \t acc:  0.66\n",
      "22200 : loss:  0.61959714 \t acc:  0.75\n",
      "22240 : loss:  0.8494278 \t acc:  0.6\n",
      "22280 : loss:  0.602079 \t acc:  0.71\n",
      "22320 : loss:  0.6861466 \t acc:  0.66\n",
      "22360 : loss:  0.625335 \t acc:  0.7\n",
      "22400 : loss:  0.70250547 \t acc:  0.67\n",
      "\n",
      "Saving...\n",
      "saved to models/CNN/pretrained_model.ckpt-22414\n",
      "\n",
      "500 \t [367.76495172 247.71238742 387.14758604]\n",
      "4 \tval accuracy:  0.6913 \t f_! score:  [0.7355299  0.49542477 0.77429517]\n",
      "\n",
      "22440 : loss:  0.5755075 \t acc:  0.71\n",
      "22480 : loss:  0.61301285 \t acc:  0.73\n",
      "22520 : loss:  0.5643949 \t acc:  0.78\n",
      "22560 : loss:  0.65926 \t acc:  0.7\n",
      "22600 : loss:  0.6724782 \t acc:  0.71\n",
      "22640 : loss:  0.6762354 \t acc:  0.74\n",
      "22680 : loss:  0.56456727 \t acc:  0.74\n",
      "22720 : loss:  0.6979965 \t acc:  0.68\n",
      "22760 : loss:  0.7392595 \t acc:  0.7\n",
      "22800 : loss:  0.7460573 \t acc:  0.66\n",
      "22840 : loss:  0.61192155 \t acc:  0.77\n",
      "22880 : loss:  0.76553774 \t acc:  0.63\n",
      "22920 : loss:  0.74015623 \t acc:  0.69\n",
      "22960 : loss:  0.70647067 \t acc:  0.67\n",
      "23000 : loss:  0.65138024 \t acc:  0.68\n",
      "23040 : loss:  0.61974 \t acc:  0.7\n",
      "23080 : loss:  0.638048 \t acc:  0.77\n",
      "23120 : loss:  0.64549965 \t acc:  0.72\n",
      "23160 : loss:  0.72100824 \t acc:  0.65\n",
      "23200 : loss:  0.6898333 \t acc:  0.7\n",
      "23240 : loss:  0.64705527 \t acc:  0.76\n",
      "23280 : loss:  0.6912907 \t acc:  0.7\n",
      "23320 : loss:  0.881541 \t acc:  0.57\n",
      "23360 : loss:  0.6993349 \t acc:  0.67\n",
      "23400 : loss:  0.69427323 \t acc:  0.68\n",
      "23440 : loss:  0.58069086 \t acc:  0.73\n",
      "23480 : loss:  0.60473174 \t acc:  0.79\n",
      "23520 : loss:  0.84241384 \t acc:  0.62\n",
      "23560 : loss:  0.83018637 \t acc:  0.66\n",
      "23600 : loss:  0.6015715 \t acc:  0.75\n",
      "23640 : loss:  0.71759313 \t acc:  0.72\n",
      "23680 : loss:  0.65169173 \t acc:  0.72\n",
      "23720 : loss:  0.6602738 \t acc:  0.73\n",
      "23760 : loss:  0.59936804 \t acc:  0.68\n",
      "23800 : loss:  0.7312532 \t acc:  0.65\n",
      "23840 : loss:  0.7288614 \t acc:  0.67\n",
      "23880 : loss:  0.7062994 \t acc:  0.63\n",
      "23920 : loss:  0.6023354 \t acc:  0.79\n",
      "23960 : loss:  0.78457266 \t acc:  0.66\n",
      "24000 : loss:  0.56326103 \t acc:  0.78\n",
      "24040 : loss:  0.8024106 \t acc:  0.6\n",
      "24080 : loss:  0.6823352 \t acc:  0.72\n",
      "24120 : loss:  0.6262834 \t acc:  0.75\n",
      "24160 : loss:  0.5980702 \t acc:  0.76\n",
      "24200 : loss:  0.69802994 \t acc:  0.67\n",
      "24240 : loss:  0.66667634 \t acc:  0.75\n",
      "24280 : loss:  0.6674052 \t acc:  0.74\n",
      "24320 : loss:  0.656986 \t acc:  0.72\n",
      "24360 : loss:  0.76568025 \t acc:  0.67\n",
      "24400 : loss:  0.6613521 \t acc:  0.69\n",
      "24440 : loss:  0.72701865 \t acc:  0.73\n",
      "24480 : loss:  0.72734404 \t acc:  0.67\n",
      "24520 : loss:  0.61895907 \t acc:  0.72\n",
      "24560 : loss:  0.72544044 \t acc:  0.7\n",
      "24600 : loss:  0.67153466 \t acc:  0.74\n",
      "24640 : loss:  0.5500611 \t acc:  0.78\n",
      "24680 : loss:  0.66916436 \t acc:  0.72\n",
      "24720 : loss:  0.6569655 \t acc:  0.72\n",
      "24760 : loss:  0.59326243 \t acc:  0.8\n",
      "24800 : loss:  0.6952622 \t acc:  0.69\n",
      "24840 : loss:  0.71954024 \t acc:  0.69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24880 : loss:  0.69388115 \t acc:  0.68\n",
      "24920 : loss:  0.73268473 \t acc:  0.64\n",
      "24960 : loss:  0.69931036 \t acc:  0.68\n",
      "25000 : loss:  0.51196074 \t acc:  0.79\n",
      "25040 : loss:  0.68541795 \t acc:  0.69\n",
      "25080 : loss:  0.80224437 \t acc:  0.69\n",
      "25120 : loss:  0.6629112 \t acc:  0.68\n",
      "25160 : loss:  0.66493714 \t acc:  0.67\n",
      "25200 : loss:  0.62524873 \t acc:  0.73\n",
      "25240 : loss:  0.7484828 \t acc:  0.74\n",
      "25280 : loss:  0.68570113 \t acc:  0.7\n",
      "25320 : loss:  0.63827366 \t acc:  0.75\n",
      "25360 : loss:  0.6655898 \t acc:  0.66\n",
      "25400 : loss:  0.7348764 \t acc:  0.66\n",
      "25440 : loss:  0.68049157 \t acc:  0.72\n",
      "25480 : loss:  0.5374735 \t acc:  0.78\n",
      "25520 : loss:  0.6041846 \t acc:  0.76\n",
      "25560 : loss:  0.66586626 \t acc:  0.75\n",
      "25600 : loss:  0.607752 \t acc:  0.66\n",
      "25640 : loss:  0.6123546 \t acc:  0.77\n",
      "25680 : loss:  0.70766646 \t acc:  0.73\n",
      "25720 : loss:  0.6338867 \t acc:  0.74\n",
      "25760 : loss:  0.60739076 \t acc:  0.76\n",
      "25800 : loss:  0.64194196 \t acc:  0.7\n",
      "25840 : loss:  0.77401197 \t acc:  0.67\n",
      "25880 : loss:  0.7248844 \t acc:  0.68\n",
      "25920 : loss:  0.60657984 \t acc:  0.74\n",
      "25960 : loss:  0.63497263 \t acc:  0.74\n",
      "26000 : loss:  0.62317294 \t acc:  0.7\n",
      "26040 : loss:  0.64329153 \t acc:  0.7\n",
      "26080 : loss:  0.7030399 \t acc:  0.7\n",
      "26120 : loss:  0.62926173 \t acc:  0.73\n",
      "26160 : loss:  0.6309074 \t acc:  0.72\n",
      "26200 : loss:  0.73412424 \t acc:  0.66\n",
      "26240 : loss:  0.5779834 \t acc:  0.73\n",
      "26280 : loss:  0.6676854 \t acc:  0.75\n",
      "26320 : loss:  0.64443517 \t acc:  0.68\n",
      "26360 : loss:  0.6550868 \t acc:  0.72\n",
      "26400 : loss:  0.55727965 \t acc:  0.79\n",
      "26440 : loss:  0.62407744 \t acc:  0.71\n",
      "26480 : loss:  0.69452876 \t acc:  0.68\n",
      "26520 : loss:  0.7051814 \t acc:  0.66\n",
      "26560 : loss:  0.6977665 \t acc:  0.64\n",
      "26600 : loss:  0.77553046 \t acc:  0.62\n",
      "26640 : loss:  0.57494706 \t acc:  0.77\n",
      "26680 : loss:  0.6545646 \t acc:  0.72\n",
      "26720 : loss:  0.6784638 \t acc:  0.68\n",
      "26760 : loss:  0.624957 \t acc:  0.73\n",
      "26800 : loss:  0.6452659 \t acc:  0.75\n",
      "26840 : loss:  0.6805713 \t acc:  0.69\n",
      "26880 : loss:  0.7537883 \t acc:  0.71\n",
      "\n",
      "Saving...\n",
      "saved to models/CNN/pretrained_model.ckpt-26897\n",
      "\n",
      "500 \t [371.0438912  263.85040972 383.49028741]\n",
      "5 \tval accuracy:  0.69216 \t f_! score:  [0.74208778 0.52770082 0.76698057]\n",
      "\n",
      "26920 : loss:  0.6906755 \t acc:  0.69\n",
      "26960 : loss:  0.50071055 \t acc:  0.79\n",
      "27000 : loss:  0.51453716 \t acc:  0.79\n",
      "27040 : loss:  0.67394555 \t acc:  0.64\n",
      "27080 : loss:  0.5926274 \t acc:  0.75\n",
      "27120 : loss:  0.7078852 \t acc:  0.65\n",
      "27160 : loss:  0.555864 \t acc:  0.76\n",
      "27200 : loss:  0.61056733 \t acc:  0.76\n",
      "27240 : loss:  0.68057615 \t acc:  0.68\n",
      "27280 : loss:  0.5651846 \t acc:  0.82\n",
      "27320 : loss:  0.5952845 \t acc:  0.75\n",
      "27360 : loss:  0.597477 \t acc:  0.75\n",
      "27400 : loss:  0.6238866 \t acc:  0.74\n",
      "27440 : loss:  0.6657249 \t acc:  0.72\n",
      "27480 : loss:  0.6240603 \t acc:  0.79\n",
      "27520 : loss:  0.50499284 \t acc:  0.77\n",
      "27560 : loss:  0.7461983 \t acc:  0.66\n",
      "27600 : loss:  0.7780048 \t acc:  0.67\n",
      "27640 : loss:  0.63071907 \t acc:  0.71\n",
      "27680 : loss:  0.75842756 \t acc:  0.64\n",
      "27720 : loss:  0.6793847 \t acc:  0.71\n",
      "27760 : loss:  0.60210234 \t acc:  0.76\n",
      "27800 : loss:  0.61197096 \t acc:  0.74\n",
      "27840 : loss:  0.63399386 \t acc:  0.71\n",
      "27880 : loss:  0.8593747 \t acc:  0.57\n",
      "27920 : loss:  0.6926086 \t acc:  0.71\n",
      "27960 : loss:  0.4627842 \t acc:  0.83\n",
      "28000 : loss:  0.7054184 \t acc:  0.65\n",
      "28040 : loss:  0.7046375 \t acc:  0.72\n",
      "28080 : loss:  0.72663844 \t acc:  0.65\n",
      "28120 : loss:  0.6749295 \t acc:  0.67\n",
      "28160 : loss:  0.6128583 \t acc:  0.71\n",
      "28200 : loss:  0.62306815 \t acc:  0.75\n",
      "28240 : loss:  0.671033 \t acc:  0.72\n",
      "28280 : loss:  0.47106117 \t acc:  0.78\n",
      "28320 : loss:  0.5480037 \t acc:  0.76\n",
      "28360 : loss:  0.6484259 \t acc:  0.65\n",
      "28400 : loss:  0.6292815 \t acc:  0.66\n",
      "28440 : loss:  0.57505167 \t acc:  0.73\n",
      "28480 : loss:  0.52640057 \t acc:  0.77\n",
      "28520 : loss:  0.61185944 \t acc:  0.75\n",
      "28560 : loss:  0.5803121 \t acc:  0.76\n",
      "28600 : loss:  0.75498545 \t acc:  0.66\n",
      "28640 : loss:  0.66227293 \t acc:  0.69\n",
      "28680 : loss:  0.53839177 \t acc:  0.83\n",
      "28720 : loss:  0.7898739 \t acc:  0.65\n",
      "28760 : loss:  0.5888641 \t acc:  0.77\n",
      "28800 : loss:  0.7621174 \t acc:  0.68\n",
      "28840 : loss:  0.57977176 \t acc:  0.7\n",
      "28880 : loss:  0.57514566 \t acc:  0.74\n",
      "28920 : loss:  0.67608297 \t acc:  0.67\n",
      "28960 : loss:  0.61448026 \t acc:  0.69\n",
      "29000 : loss:  0.63911116 \t acc:  0.79\n",
      "29040 : loss:  0.6329134 \t acc:  0.7\n",
      "29080 : loss:  0.6832884 \t acc:  0.72\n",
      "29120 : loss:  0.6458849 \t acc:  0.74\n",
      "29160 : loss:  0.57307184 \t acc:  0.76\n",
      "29200 : loss:  0.711288 \t acc:  0.67\n",
      "29240 : loss:  0.54024565 \t acc:  0.79\n",
      "29280 : loss:  0.8347209 \t acc:  0.58\n",
      "29320 : loss:  0.71158624 \t acc:  0.64\n",
      "29360 : loss:  0.58892643 \t acc:  0.72\n",
      "29400 : loss:  0.6123553 \t acc:  0.71\n",
      "29440 : loss:  0.6427451 \t acc:  0.73\n",
      "29480 : loss:  0.61409396 \t acc:  0.71\n",
      "29520 : loss:  0.5525133 \t acc:  0.79\n",
      "29560 : loss:  0.64549387 \t acc:  0.64\n",
      "29600 : loss:  0.53593385 \t acc:  0.74\n",
      "29640 : loss:  0.6930002 \t acc:  0.72\n",
      "29680 : loss:  0.7040515 \t acc:  0.7\n",
      "29720 : loss:  0.67007506 \t acc:  0.71\n",
      "29760 : loss:  0.6214193 \t acc:  0.72\n",
      "29800 : loss:  0.7041781 \t acc:  0.7\n",
      "29840 : loss:  0.51079607 \t acc:  0.79\n",
      "29880 : loss:  0.59331185 \t acc:  0.75\n",
      "29920 : loss:  0.6349491 \t acc:  0.79\n",
      "29960 : loss:  0.6402841 \t acc:  0.65\n",
      "30000 : loss:  0.8172038 \t acc:  0.65\n",
      "30040 : loss:  0.631203 \t acc:  0.78\n",
      "30080 : loss:  0.75972426 \t acc:  0.64\n",
      "30120 : loss:  0.5961649 \t acc:  0.76\n",
      "30160 : loss:  0.6599841 \t acc:  0.76\n",
      "30200 : loss:  0.6399506 \t acc:  0.73\n",
      "30240 : loss:  0.5070019 \t acc:  0.77\n",
      "30280 : loss:  0.6705613 \t acc:  0.69\n",
      "30320 : loss:  0.6066887 \t acc:  0.77\n",
      "30360 : loss:  0.6502667 \t acc:  0.78\n",
      "30400 : loss:  0.6955367 \t acc:  0.72\n",
      "30440 : loss:  0.6414512 \t acc:  0.71\n",
      "30480 : loss:  0.62639666 \t acc:  0.78\n",
      "30520 : loss:  0.5108876 \t acc:  0.79\n",
      "30560 : loss:  0.7369303 \t acc:  0.65\n",
      "30600 : loss:  0.6427544 \t acc:  0.69\n",
      "30640 : loss:  0.6167318 \t acc:  0.72\n",
      "30680 : loss:  0.70541567 \t acc:  0.62\n",
      "30720 : loss:  0.6641897 \t acc:  0.73\n",
      "30760 : loss:  0.5968816 \t acc:  0.76\n",
      "30800 : loss:  0.80059314 \t acc:  0.64\n",
      "30840 : loss:  0.5799013 \t acc:  0.77\n",
      "30880 : loss:  0.59013057 \t acc:  0.74\n",
      "30920 : loss:  0.6030488 \t acc:  0.74\n",
      "30960 : loss:  0.62607646 \t acc:  0.71\n",
      "31000 : loss:  0.6566352 \t acc:  0.78\n",
      "31040 : loss:  0.8213049 \t acc:  0.67\n",
      "31080 : loss:  0.6477063 \t acc:  0.72\n",
      "31120 : loss:  0.7694062 \t acc:  0.61\n",
      "31160 : loss:  0.6171399 \t acc:  0.75\n",
      "31200 : loss:  0.6838753 \t acc:  0.67\n",
      "31240 : loss:  0.7141048 \t acc:  0.68\n",
      "31280 : loss:  0.7440711 \t acc:  0.67\n",
      "31320 : loss:  0.6762426 \t acc:  0.71\n",
      "31360 : loss:  0.65845007 \t acc:  0.63\n",
      "\n",
      "Saving...\n",
      "saved to models/CNN/pretrained_model.ckpt-31380\n",
      "\n",
      "500 \t [370.05628506 250.80990862 388.8876695 ]\n",
      "6 \tval accuracy:  0.69516 \t f_! score:  [0.74011257 0.50161982 0.77777534]\n",
      "\n",
      "500 \t [370.05628506 250.80990862 388.8876695 ]\n",
      "500 \t [368.0768143  274.11887263 373.20582218]\n",
      "500 \t [374.72135415 235.22965488 408.33196655]\n",
      "500 \t [18794. 13823. 17383.]\n",
      "--- Test   Review ---\n",
      "0.69516\n",
      "f1:  [0.74011257 0.50161982 0.77777534]\n",
      "65730\n",
      "Vorsicht! es muss vorher ein  CNN  Netz abgespeichert worden sein\n",
      "\n",
      "Restoring...\n",
      "models/CNN/pretrained_model.ckpt-31380\n",
      "INFO:tensorflow:Restoring parameters from models/CNN/pretrained_model.ckpt-31380\n",
      "acc:  0.35\n",
      "acc:  0.41\n",
      "acc:  0.3\n",
      "acc:  0.19\n",
      "acc:  0.34\n",
      "acc:  0.3\n",
      "acc:  0.3\n",
      "acc:  0.35\n",
      "acc:  0.34\n",
      "acc:  0.28\n",
      "acc:  0.33\n",
      "acc:  0.3\n",
      "acc:  0.33\n",
      "acc:  0.26\n",
      "acc:  0.29\n",
      "acc:  0.29\n",
      "acc:  0.28\n",
      "acc:  0.35\n",
      "acc:  0.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yannik/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.36\n",
      "acc:  0.34\n",
      "acc:  0.34\n",
      "acc:  0.33\n",
      "acc:  0.36\n",
      "acc:  0.4\n",
      "acc:  0.27\n",
      "acc:  0.34\n",
      "acc:  0.35\n",
      "acc:  0.37\n",
      "acc:  0.28\n",
      "acc:  0.35\n",
      "acc:  0.27\n",
      "acc:  0.41\n",
      "acc:  0.42\n",
      "acc:  0.39\n",
      "acc:  0.32\n",
      "acc:  0.28\n",
      "acc:  0.34\n",
      "acc:  0.3\n",
      "acc:  0.28\n",
      "acc:  0.22\n",
      "acc:  0.41\n",
      "acc:  0.25\n",
      "acc:  0.33\n",
      "acc:  0.3\n",
      "acc:  0.33\n",
      "acc:  0.42\n",
      "acc:  0.39\n",
      "acc:  0.29\n",
      "acc:  0.43\n",
      "acc:  0.26\n",
      "acc:  0.37\n",
      "acc:  0.44\n",
      "acc:  0.28\n",
      "acc:  0.36\n",
      "acc:  0.38\n",
      "acc:  0.32\n",
      "acc:  0.33\n",
      "acc:  0.35\n",
      "acc:  0.29\n",
      "acc:  0.29\n",
      "acc:  0.33\n",
      "acc:  0.3\n",
      "acc:  0.37\n",
      "acc:  0.34\n",
      "acc:  0.26\n",
      "acc:  0.32\n",
      "acc:  0.28\n",
      "acc:  0.35\n",
      "acc:  0.38\n",
      "acc:  0.35\n",
      "acc:  0.31\n",
      "acc:  0.31\n",
      "acc:  0.29\n",
      "acc:  0.39\n",
      "acc:  0.29\n",
      "acc:  0.32\n",
      "acc:  0.23\n",
      "acc:  0.3\n",
      "acc:  0.27\n",
      "acc:  0.36\n",
      "acc:  0.41\n",
      "acc:  0.31\n",
      "acc:  0.32\n",
      "acc:  0.41\n",
      "acc:  0.31\n",
      "acc:  0.31\n",
      "acc:  0.31\n",
      "acc:  0.45\n",
      "acc:  0.37\n",
      "acc:  0.48\n",
      "acc:  0.33\n",
      "acc:  0.34\n",
      "acc:  0.35\n",
      "acc:  0.36\n",
      "acc:  0.36\n",
      "acc:  0.36\n",
      "acc:  0.32\n",
      "acc:  0.29\n",
      "acc:  0.27\n",
      "\n",
      "100 \t [ 0.          7.02584856 48.91733622]\n",
      "val accuracy:  0.33099997 \t f_! score:  [0.         0.07025849 0.48917336]\n",
      "\n",
      "100 \t [ 0.          7.02584856 48.91733622]\n",
      "100 \t [ 0.         34.7781746  33.05155134]\n",
      "100 \t [ 0.          4.01224958 95.98624686]\n",
      "100 \t [3341. 3345. 3314.]\n",
      "---just Test  Twitter ---\n",
      "0.33099997\n",
      "f1:  [0.         0.07025849 0.48917336]\n",
      "(7733, 3)\n",
      "7733\n",
      "Vorsicht! es muss vorher ein  CNN  Netz abgespeichert worden sein\n",
      "\n",
      "Restoring...\n",
      "models/CNN/pretrained_model.ckpt-31380\n",
      "INFO:tensorflow:Restoring parameters from models/CNN/pretrained_model.ckpt-31380\n",
      "acc:  0.08\n",
      "acc:  0.02\n",
      "acc:  0.12\n",
      "acc:  0.15\n",
      "acc:  0.13\n",
      "acc:  0.08\n",
      "acc:  0.02\n",
      "acc:  0.11\n",
      "acc:  0.08\n",
      "acc:  0.05\n",
      "acc:  0.04\n",
      "acc:  0.04\n",
      "acc:  0.11\n",
      "acc:  0.09\n",
      "\n",
      "14 \t [0.92929068 2.1044227  0.        ]\n",
      "val accuracy:  0.07999999 \t f_! score:  [0.06637791 0.15031591 0.        ]\n",
      "\n",
      "14 \t [0.92929068 2.1044227  0.        ]\n",
      "14 \t [0.76259062 1.17436406 0.        ]\n",
      "14 \t [ 1.27692308 11.64994172  0.        ]\n",
      "14 \t [ 105.  116. 1179.]\n",
      "---just Test  Medical ---\n",
      "0.07999999\n",
      "f1:  [0.06637791 0.15031591 0.        ]\n",
      "(37126,)\n",
      "(37126,)\n",
      "37126\n",
      "Vorsicht! es muss vorher ein  CNN  Netz abgespeichert worden sein\n",
      "\n",
      "Restoring...\n",
      "models/CNN/pretrained_model.ckpt-31380\n",
      "INFO:tensorflow:Restoring parameters from models/CNN/pretrained_model.ckpt-31380\n",
      "acc:  0.66\n",
      "acc:  0.85\n",
      "acc:  0.89\n",
      "acc:  0.91\n",
      "acc:  0.85\n",
      "acc:  0.83\n",
      "acc:  0.73\n",
      "acc:  0.92\n",
      "acc:  0.65\n",
      "acc:  0.92\n",
      "acc:  0.84\n",
      "acc:  0.92\n",
      "acc:  0.86\n",
      "acc:  0.91\n",
      "acc:  0.75\n",
      "acc:  0.96\n",
      "(2,)\n",
      "acc:  0.79\n",
      "acc:  0.86\n",
      "acc:  0.96\n",
      "acc:  0.87\n",
      "acc:  0.78\n",
      "acc:  0.78\n",
      "acc:  0.89\n",
      "acc:  0.85\n",
      "acc:  0.84\n",
      "acc:  0.95\n",
      "acc:  0.72\n",
      "acc:  0.77\n",
      "acc:  0.94\n",
      "acc:  0.89\n",
      "acc:  0.87\n",
      "acc:  0.84\n",
      "acc:  0.86\n",
      "acc:  0.74\n",
      "acc:  0.88\n",
      "acc:  0.78\n",
      "acc:  0.67\n",
      "acc:  0.72\n",
      "acc:  0.89\n",
      "acc:  0.84\n",
      "acc:  0.91\n",
      "acc:  0.91\n",
      "acc:  0.89\n",
      "acc:  0.92\n",
      "acc:  0.83\n",
      "acc:  0.58\n",
      "acc:  0.74\n",
      "acc:  0.89\n",
      "acc:  0.69\n",
      "acc:  0.91\n",
      "acc:  0.8\n",
      "acc:  0.97\n",
      "acc:  0.61\n",
      "acc:  0.75\n",
      "acc:  0.84\n",
      "acc:  0.93\n",
      "acc:  0.76\n",
      "acc:  0.76\n",
      "acc:  0.71\n",
      "acc:  0.97\n",
      "(2,)\n",
      "acc:  0.93\n",
      "acc:  0.8\n",
      "acc:  0.79\n",
      "acc:  0.82\n",
      "acc:  0.92\n",
      "acc:  0.81\n",
      "acc:  0.92\n",
      "acc:  0.85\n",
      "acc:  0.81\n",
      "acc:  0.77\n",
      "\n",
      "68 \t [ 0.          0.         61.41592625]\n",
      "val accuracy:  0.82978255 \t f_! score:  [0.         0.         0.90317539]\n",
      "\n",
      "68 \t [ 0.          0.         61.41592625]\n",
      "68 \t [ 0.          0.         56.39785611]\n",
      "68 \t [ 0.         0.        67.8428146]\n",
      "68 \t [ 501.  657. 5642.]\n",
      "---just Test  Prime ---\n",
      "0.82978255\n",
      "f1:  [0.         0.         0.90317539]\n"
     ]
    }
   ],
   "source": [
    "net_name = \"CNN\"\n",
    "\n",
    "testhelper = TestHelper()\n",
    "review_loss, review_acc = testhelper.train_input(\"Review\", net_name)\n",
    "\n",
    "testhelper.just_test(\"Twitter\", net_name)\n",
    "testhelper.just_test(\"Medical\", net_name)\n",
    "testhelper.just_test(\"Prime\", net_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  Twitter\n",
      "delete old models\n",
      "65730\n",
      "Tensor(\"ConvNet/dense/BiasAdd:0\", shape=(?, 3), dtype=float32)\n",
      "Tensor(\"ConvNet_1/dense/BiasAdd:0\", shape=(?, 3), dtype=float32)\n",
      "---init ready   CNN ---\n",
      "\n",
      "Saving...\n",
      "saved to models/CNN/pretrained_model.ckpt-0\n",
      "0 : loss:  3.004263 \t acc:  0.29\n",
      "2 : loss:  1.1138015 \t acc:  0.32\n",
      "4 : loss:  1.1004195 \t acc:  0.38\n",
      "6 : loss:  1.1143165 \t acc:  0.28\n",
      "8 : loss:  1.0778195 \t acc:  0.41\n",
      "10 : loss:  1.0992397 \t acc:  0.32\n",
      "12 : loss:  1.085969 \t acc:  0.38\n",
      "14 : loss:  1.0624435 \t acc:  0.51\n",
      "16 : loss:  1.0375304 \t acc:  0.41\n",
      "18 : loss:  1.0731149 \t acc:  0.39\n",
      "20 : loss:  1.0869447 \t acc:  0.41\n",
      "22 : loss:  1.099341 \t acc:  0.41\n",
      "24 : loss:  1.0397574 \t acc:  0.45\n",
      "26 : loss:  1.1230674 \t acc:  0.33\n",
      "28 : loss:  1.0804538 \t acc:  0.44\n",
      "30 : loss:  0.9993915 \t acc:  0.51\n",
      "32 : loss:  1.0922354 \t acc:  0.36\n",
      "34 : loss:  1.076751 \t acc:  0.32\n",
      "36 : loss:  1.0810895 \t acc:  0.34\n",
      "38 : loss:  1.0727009 \t acc:  0.36\n",
      "40 : loss:  1.0646306 \t acc:  0.53\n",
      "42 : loss:  1.0473081 \t acc:  0.47\n",
      "44 : loss:  1.064244 \t acc:  0.45\n",
      "46 : loss:  1.0232378 \t acc:  0.51\n",
      "48 : loss:  0.98787236 \t acc:  0.51\n",
      "50 : loss:  0.9200064 \t acc:  0.5\n",
      "52 : loss:  0.9728491 \t acc:  0.58\n",
      "54 : loss:  1.0349118 \t acc:  0.4\n",
      "56 : loss:  1.085752 \t acc:  0.39\n",
      "58 : loss:  0.99687827 \t acc:  0.52\n",
      "60 : loss:  1.0114852 \t acc:  0.46\n",
      "62 : loss:  0.9621299 \t acc:  0.47\n",
      "64 : loss:  1.0449282 \t acc:  0.47\n",
      "66 : loss:  0.9573797 \t acc:  0.52\n",
      "68 : loss:  1.1062593 \t acc:  0.43\n",
      "70 : loss:  1.006865 \t acc:  0.49\n",
      "72 : loss:  0.9027505 \t acc:  0.6\n",
      "74 : loss:  1.0405599 \t acc:  0.49\n",
      "76 : loss:  1.0328281 \t acc:  0.42\n",
      "78 : loss:  1.0494994 \t acc:  0.44\n",
      "80 : loss:  1.0036205 \t acc:  0.52\n",
      "82 : loss:  0.96977526 \t acc:  0.48\n",
      "84 : loss:  1.0287907 \t acc:  0.49\n",
      "86 : loss:  1.0018713 \t acc:  0.51\n",
      "88 : loss:  0.9386842 \t acc:  0.5\n",
      "90 : loss:  1.004577 \t acc:  0.47\n",
      "92 : loss:  0.98246455 \t acc:  0.51\n",
      "94 : loss:  1.0235376 \t acc:  0.45\n",
      "96 : loss:  0.9640152 \t acc:  0.5\n",
      "98 : loss:  0.9982569 \t acc:  0.43\n",
      "100 : loss:  0.9231077 \t acc:  0.61\n",
      "102 : loss:  0.8693618 \t acc:  0.56\n",
      "104 : loss:  0.956591 \t acc:  0.48\n",
      "106 : loss:  1.0736003 \t acc:  0.39\n",
      "108 : loss:  0.95687085 \t acc:  0.46\n",
      "110 : loss:  0.9103617 \t acc:  0.55\n",
      "112 : loss:  0.98247117 \t acc:  0.46\n",
      "114 : loss:  1.0100394 \t acc:  0.41\n",
      "116 : loss:  0.87210715 \t acc:  0.55\n",
      "118 : loss:  0.95122194 \t acc:  0.44\n",
      "120 : loss:  0.9163748 \t acc:  0.58\n",
      "122 : loss:  0.9319651 \t acc:  0.5\n",
      "124 : loss:  0.99832916 \t acc:  0.42\n",
      "126 : loss:  0.85470355 \t acc:  0.59\n",
      "128 : loss:  0.9572253 \t acc:  0.52\n",
      "130 : loss:  0.9945462 \t acc:  0.44\n",
      "132 : loss:  0.9713717 \t acc:  0.48\n",
      "134 : loss:  0.890784 \t acc:  0.55\n",
      "136 : loss:  0.9754477 \t acc:  0.53\n",
      "138 : loss:  0.9732102 \t acc:  0.45\n",
      "140 : loss:  0.9516972 \t acc:  0.47\n",
      "142 : loss:  0.9754866 \t acc:  0.45\n",
      "144 : loss:  0.9614479 \t acc:  0.49\n",
      "146 : loss:  0.93990207 \t acc:  0.48\n",
      "148 : loss:  1.0242591 \t acc:  0.48\n",
      "150 : loss:  1.0416387 \t acc:  0.43\n",
      "152 : loss:  1.0410103 \t acc:  0.42\n",
      "154 : loss:  0.99644834 \t acc:  0.41\n",
      "156 : loss:  0.923236 \t acc:  0.5\n",
      "158 : loss:  0.88213897 \t acc:  0.56\n",
      "160 : loss:  0.92696166 \t acc:  0.53\n",
      "162 : loss:  0.94891006 \t acc:  0.48\n",
      "164 : loss:  1.0000815 \t acc:  0.42\n",
      "166 : loss:  0.8848085 \t acc:  0.58\n",
      "168 : loss:  0.99736106 \t acc:  0.44\n",
      "170 : loss:  1.0075705 \t acc:  0.5\n",
      "172 : loss:  0.9193453 \t acc:  0.48\n",
      "174 : loss:  0.9252426 \t acc:  0.54\n",
      "176 : loss:  0.94436663 \t acc:  0.52\n",
      "178 : loss:  0.90231705 \t acc:  0.51\n",
      "180 : loss:  0.9421475 \t acc:  0.55\n",
      "182 : loss:  1.0386875 \t acc:  0.46\n",
      "184 : loss:  0.9672997 \t acc:  0.44\n",
      "186 : loss:  0.8298582 \t acc:  0.55\n",
      "188 : loss:  0.9106157 \t acc:  0.5\n",
      "190 : loss:  0.92154986 \t acc:  0.57\n",
      "192 : loss:  1.0131774 \t acc:  0.48\n",
      "194 : loss:  0.9688699 \t acc:  0.43\n",
      "196 : loss:  0.86431015 \t acc:  0.56\n",
      "198 : loss:  0.9513975 \t acc:  0.54\n",
      "200 : loss:  0.96928483 \t acc:  0.56\n",
      "202 : loss:  0.91436034 \t acc:  0.46\n",
      "204 : loss:  1.0255275 \t acc:  0.43\n",
      "206 : loss:  0.91561025 \t acc:  0.48\n",
      "208 : loss:  0.9483485 \t acc:  0.55\n",
      "210 : loss:  0.98318243 \t acc:  0.46\n",
      "212 : loss:  0.9792662 \t acc:  0.48\n",
      "214 : loss:  0.90290916 \t acc:  0.54\n",
      "216 : loss:  0.9447679 \t acc:  0.38\n",
      "218 : loss:  1.0094072 \t acc:  0.49\n",
      "220 : loss:  0.955252 \t acc:  0.51\n",
      "222 : loss:  0.9512383 \t acc:  0.52\n",
      "224 : loss:  0.963101 \t acc:  0.5\n",
      "226 : loss:  0.9828726 \t acc:  0.5\n",
      "228 : loss:  0.93922716 \t acc:  0.62\n",
      "230 : loss:  0.9397844 \t acc:  0.5\n",
      "232 : loss:  0.99233335 \t acc:  0.45\n",
      "234 : loss:  0.9446191 \t acc:  0.48\n",
      "236 : loss:  0.97645074 \t acc:  0.52\n",
      "238 : loss:  0.9504222 \t acc:  0.54\n",
      "240 : loss:  0.9737227 \t acc:  0.47\n",
      "242 : loss:  0.86139214 \t acc:  0.55\n",
      "244 : loss:  0.87765574 \t acc:  0.53\n",
      "246 : loss:  0.9504065 \t acc:  0.55\n",
      "248 : loss:  0.993157 \t acc:  0.49\n",
      "250 : loss:  1.0070137 \t acc:  0.56\n",
      "252 : loss:  1.0119597 \t acc:  0.48\n",
      "254 : loss:  0.98374695 \t acc:  0.49\n",
      "256 : loss:  0.8826191 \t acc:  0.57\n",
      "258 : loss:  1.0044855 \t acc:  0.49\n",
      "260 : loss:  0.90625495 \t acc:  0.54\n",
      "262 : loss:  0.9184229 \t acc:  0.48\n",
      "264 : loss:  0.89947677 \t acc:  0.56\n",
      "266 : loss:  0.9176811 \t acc:  0.54\n",
      "268 : loss:  0.865281 \t acc:  0.62\n",
      "270 : loss:  0.91653174 \t acc:  0.45\n",
      "272 : loss:  0.92274594 \t acc:  0.51\n",
      "274 : loss:  1.0236707 \t acc:  0.49\n",
      "276 : loss:  0.9569822 \t acc:  0.51\n",
      "278 : loss:  0.92273253 \t acc:  0.47\n",
      "280 : loss:  0.9463013 \t acc:  0.48\n",
      "282 : loss:  0.9151993 \t acc:  0.52\n",
      "284 : loss:  0.9581173 \t acc:  0.53\n",
      "286 : loss:  1.0083897 \t acc:  0.5\n",
      "288 : loss:  0.9828845 \t acc:  0.51\n",
      "290 : loss:  0.9522046 \t acc:  0.43\n",
      "292 : loss:  0.9490528 \t acc:  0.47\n",
      "294 : loss:  0.9170491 \t acc:  0.57\n",
      "296 : loss:  0.8843434 \t acc:  0.58\n",
      "298 : loss:  0.95622355 \t acc:  0.52\n",
      "300 : loss:  0.9549648 \t acc:  0.53\n",
      "302 : loss:  0.93322533 \t acc:  0.56\n",
      "304 : loss:  0.9055051 \t acc:  0.48\n",
      "306 : loss:  1.0214462 \t acc:  0.5\n",
      "308 : loss:  0.88405013 \t acc:  0.57\n",
      "310 : loss:  0.95902705 \t acc:  0.51\n",
      "312 : loss:  0.91227317 \t acc:  0.53\n",
      "314 : loss:  0.9363575 \t acc:  0.59\n",
      "316 : loss:  0.8742428 \t acc:  0.59\n",
      "318 : loss:  0.8717046 \t acc:  0.59\n",
      "320 : loss:  0.91861016 \t acc:  0.53\n",
      "322 : loss:  0.9058659 \t acc:  0.51\n",
      "324 : loss:  0.92870957 \t acc:  0.5\n",
      "326 : loss:  0.85174996 \t acc:  0.58\n",
      "328 : loss:  0.93652314 \t acc:  0.62\n",
      "330 : loss:  0.9478402 \t acc:  0.58\n",
      "332 : loss:  0.9317125 \t acc:  0.53\n",
      "334 : loss:  0.8731403 \t acc:  0.56\n",
      "336 : loss:  0.95644516 \t acc:  0.45\n",
      "338 : loss:  0.94569516 \t acc:  0.53\n",
      "340 : loss:  0.90780634 \t acc:  0.58\n",
      "342 : loss:  0.99878657 \t acc:  0.51\n",
      "344 : loss:  0.9885234 \t acc:  0.42\n",
      "346 : loss:  0.9738886 \t acc:  0.54\n",
      "348 : loss:  0.8397863 \t acc:  0.6\n",
      "350 : loss:  1.0630263 \t acc:  0.43\n",
      "352 : loss:  0.83797365 \t acc:  0.67\n",
      "354 : loss:  0.9561631 \t acc:  0.59\n",
      "356 : loss:  0.88435364 \t acc:  0.52\n",
      "358 : loss:  0.984795 \t acc:  0.4\n",
      "360 : loss:  0.93964195 \t acc:  0.48\n",
      "362 : loss:  0.8901159 \t acc:  0.58\n",
      "364 : loss:  0.9095269 \t acc:  0.53\n",
      "366 : loss:  0.91965866 \t acc:  0.49\n",
      "368 : loss:  0.97184676 \t acc:  0.45\n",
      "370 : loss:  1.0008881 \t acc:  0.49\n",
      "\n",
      "Saving...\n",
      "saved to models/CNN/pretrained_model.ckpt-372\n",
      "\n",
      "100 \t [63.72436932 50.80681193 33.16962636]\n",
      "0 \tval accuracy:  0.5161 \t f_! score:  [0.63724369 0.50806812 0.33169626]\n",
      "\n",
      "372 : loss:  1.0083745 \t acc:  0.44\n",
      "374 : loss:  0.861385 \t acc:  0.59\n",
      "376 : loss:  0.9111975 \t acc:  0.55\n",
      "378 : loss:  0.9722273 \t acc:  0.57\n",
      "380 : loss:  0.8497618 \t acc:  0.58\n",
      "382 : loss:  0.9458234 \t acc:  0.5\n",
      "384 : loss:  0.99121785 \t acc:  0.46\n",
      "386 : loss:  1.0041368 \t acc:  0.43\n",
      "388 : loss:  0.9450737 \t acc:  0.43\n",
      "390 : loss:  0.96549153 \t acc:  0.48\n",
      "392 : loss:  0.94066864 \t acc:  0.58\n",
      "394 : loss:  1.0571854 \t acc:  0.51\n",
      "396 : loss:  0.9146683 \t acc:  0.62\n",
      "398 : loss:  0.94169116 \t acc:  0.55\n",
      "400 : loss:  0.93243545 \t acc:  0.56\n",
      "402 : loss:  0.9068348 \t acc:  0.58\n",
      "404 : loss:  0.90268403 \t acc:  0.52\n",
      "406 : loss:  0.8957305 \t acc:  0.62\n",
      "408 : loss:  0.9426155 \t acc:  0.55\n",
      "410 : loss:  0.9223789 \t acc:  0.54\n",
      "412 : loss:  0.92799264 \t acc:  0.48\n",
      "414 : loss:  0.96675646 \t acc:  0.49\n",
      "416 : loss:  0.8711983 \t acc:  0.64\n",
      "418 : loss:  0.93187094 \t acc:  0.53\n",
      "420 : loss:  0.9137954 \t acc:  0.58\n",
      "422 : loss:  0.9420171 \t acc:  0.53\n",
      "424 : loss:  0.88039595 \t acc:  0.6\n",
      "426 : loss:  0.91639054 \t acc:  0.56\n",
      "428 : loss:  0.92437816 \t acc:  0.5\n",
      "430 : loss:  0.8380099 \t acc:  0.66\n",
      "432 : loss:  0.96938133 \t acc:  0.49\n",
      "434 : loss:  1.0193851 \t acc:  0.47\n",
      "436 : loss:  0.90806687 \t acc:  0.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "438 : loss:  0.88286173 \t acc:  0.54\n",
      "440 : loss:  0.9505722 \t acc:  0.54\n",
      "442 : loss:  0.95337296 \t acc:  0.45\n",
      "444 : loss:  0.9591419 \t acc:  0.53\n",
      "446 : loss:  0.9560694 \t acc:  0.56\n",
      "448 : loss:  0.9685576 \t acc:  0.53\n",
      "450 : loss:  0.9753407 \t acc:  0.53\n",
      "452 : loss:  0.9101045 \t acc:  0.56\n",
      "454 : loss:  0.9096992 \t acc:  0.54\n",
      "456 : loss:  0.8411113 \t acc:  0.58\n",
      "458 : loss:  0.8468382 \t acc:  0.6\n",
      "460 : loss:  0.87303466 \t acc:  0.56\n",
      "462 : loss:  1.0105354 \t acc:  0.48\n",
      "464 : loss:  0.97310114 \t acc:  0.46\n",
      "466 : loss:  0.8468132 \t acc:  0.54\n",
      "468 : loss:  0.93647265 \t acc:  0.56\n",
      "470 : loss:  0.8750737 \t acc:  0.51\n",
      "472 : loss:  0.84560484 \t acc:  0.61\n",
      "474 : loss:  0.8902275 \t acc:  0.54\n",
      "476 : loss:  0.8914079 \t acc:  0.61\n",
      "478 : loss:  0.89450127 \t acc:  0.61\n",
      "480 : loss:  0.8551758 \t acc:  0.57\n",
      "482 : loss:  0.94152415 \t acc:  0.54\n",
      "484 : loss:  1.0228568 \t acc:  0.48\n",
      "486 : loss:  0.9356558 \t acc:  0.55\n",
      "488 : loss:  0.8326035 \t acc:  0.63\n",
      "490 : loss:  0.8474161 \t acc:  0.57\n",
      "492 : loss:  0.88182145 \t acc:  0.61\n",
      "494 : loss:  0.83042824 \t acc:  0.59\n",
      "496 : loss:  0.8999613 \t acc:  0.54\n",
      "498 : loss:  0.97609055 \t acc:  0.43\n",
      "500 : loss:  0.8645441 \t acc:  0.56\n",
      "502 : loss:  0.86254233 \t acc:  0.66\n",
      "504 : loss:  0.894323 \t acc:  0.57\n",
      "506 : loss:  0.87686867 \t acc:  0.56\n",
      "508 : loss:  0.8822235 \t acc:  0.51\n",
      "510 : loss:  0.98496145 \t acc:  0.5\n",
      "512 : loss:  0.7890882 \t acc:  0.68\n",
      "514 : loss:  0.97091407 \t acc:  0.53\n",
      "516 : loss:  0.814991 \t acc:  0.6333333\n",
      "518 : loss:  1.0098306 \t acc:  0.48\n",
      "520 : loss:  0.8867109 \t acc:  0.56\n",
      "522 : loss:  0.77174234 \t acc:  0.64\n",
      "524 : loss:  0.8002115 \t acc:  0.59\n",
      "526 : loss:  0.8646575 \t acc:  0.66\n",
      "528 : loss:  0.96262825 \t acc:  0.61\n",
      "530 : loss:  0.91032135 \t acc:  0.6\n",
      "532 : loss:  0.92741454 \t acc:  0.58\n",
      "534 : loss:  0.9071036 \t acc:  0.59\n",
      "536 : loss:  0.8879199 \t acc:  0.6\n",
      "538 : loss:  0.89687383 \t acc:  0.57\n",
      "540 : loss:  0.89051706 \t acc:  0.53\n",
      "542 : loss:  0.8602916 \t acc:  0.58\n",
      "544 : loss:  0.8921682 \t acc:  0.53\n",
      "546 : loss:  0.994219 \t acc:  0.61\n",
      "548 : loss:  0.89653087 \t acc:  0.59\n",
      "550 : loss:  0.97008055 \t acc:  0.52\n",
      "552 : loss:  0.8841729 \t acc:  0.56\n",
      "554 : loss:  0.9193688 \t acc:  0.53\n",
      "556 : loss:  0.87516856 \t acc:  0.63\n",
      "558 : loss:  0.9342035 \t acc:  0.55\n",
      "560 : loss:  0.96217036 \t acc:  0.5\n",
      "562 : loss:  0.923793 \t acc:  0.56\n",
      "564 : loss:  0.8785559 \t acc:  0.54\n",
      "566 : loss:  0.9044246 \t acc:  0.54\n",
      "568 : loss:  0.84353584 \t acc:  0.61\n",
      "570 : loss:  0.8145231 \t acc:  0.63\n",
      "572 : loss:  0.77905184 \t acc:  0.64\n",
      "574 : loss:  0.9411344 \t acc:  0.52\n",
      "576 : loss:  0.9110608 \t acc:  0.54\n",
      "578 : loss:  0.90812683 \t acc:  0.58\n",
      "580 : loss:  0.918766 \t acc:  0.54\n",
      "582 : loss:  0.88861084 \t acc:  0.58\n",
      "584 : loss:  0.9074489 \t acc:  0.53\n",
      "586 : loss:  0.88527817 \t acc:  0.58\n",
      "588 : loss:  0.8683515 \t acc:  0.6\n",
      "590 : loss:  0.87625855 \t acc:  0.58\n",
      "592 : loss:  0.96356773 \t acc:  0.5\n",
      "594 : loss:  0.91106504 \t acc:  0.58\n",
      "596 : loss:  0.98943865 \t acc:  0.44\n",
      "598 : loss:  0.8664951 \t acc:  0.6\n",
      "600 : loss:  0.92546266 \t acc:  0.5\n",
      "602 : loss:  0.8620836 \t acc:  0.59\n",
      "604 : loss:  0.86830294 \t acc:  0.54\n",
      "606 : loss:  0.8536524 \t acc:  0.6\n",
      "608 : loss:  0.8264819 \t acc:  0.57\n",
      "610 : loss:  0.7781546 \t acc:  0.66\n",
      "612 : loss:  0.9638501 \t acc:  0.54\n",
      "614 : loss:  0.8862196 \t acc:  0.52\n",
      "616 : loss:  0.849636 \t acc:  0.65\n",
      "618 : loss:  0.86496705 \t acc:  0.57\n",
      "620 : loss:  0.9525783 \t acc:  0.54\n",
      "622 : loss:  0.9167184 \t acc:  0.54\n",
      "624 : loss:  0.84283173 \t acc:  0.64\n",
      "626 : loss:  0.8756494 \t acc:  0.54\n",
      "628 : loss:  0.8673112 \t acc:  0.53\n",
      "630 : loss:  0.88908476 \t acc:  0.52\n",
      "632 : loss:  0.8193758 \t acc:  0.55\n",
      "634 : loss:  0.83084244 \t acc:  0.67\n",
      "636 : loss:  0.92022806 \t acc:  0.49\n",
      "638 : loss:  0.9000106 \t acc:  0.6\n",
      "640 : loss:  0.82423246 \t acc:  0.57\n",
      "642 : loss:  0.8245575 \t acc:  0.63\n",
      "644 : loss:  0.85023665 \t acc:  0.59\n",
      "646 : loss:  0.8593742 \t acc:  0.59\n",
      "648 : loss:  0.8242841 \t acc:  0.61\n",
      "650 : loss:  0.8717956 \t acc:  0.58\n",
      "652 : loss:  0.94479483 \t acc:  0.42\n",
      "654 : loss:  0.7960115 \t acc:  0.6\n",
      "656 : loss:  0.90154403 \t acc:  0.54\n",
      "658 : loss:  0.849869 \t acc:  0.64\n",
      "660 : loss:  0.8907823 \t acc:  0.59\n",
      "662 : loss:  0.7990262 \t acc:  0.67\n",
      "664 : loss:  0.978145 \t acc:  0.53\n",
      "666 : loss:  0.84861046 \t acc:  0.58\n",
      "668 : loss:  0.95030314 \t acc:  0.58\n",
      "670 : loss:  0.80335456 \t acc:  0.66\n",
      "672 : loss:  0.89781904 \t acc:  0.59\n",
      "674 : loss:  0.8259342 \t acc:  0.61\n",
      "676 : loss:  0.8609666 \t acc:  0.61\n",
      "678 : loss:  0.87293816 \t acc:  0.58\n",
      "680 : loss:  0.765098 \t acc:  0.68\n",
      "682 : loss:  0.8609779 \t acc:  0.62\n",
      "684 : loss:  0.9257648 \t acc:  0.53\n",
      "686 : loss:  0.9370039 \t acc:  0.51\n",
      "688 : loss:  0.8230551 \t acc:  0.64\n",
      "690 : loss:  0.7102364 \t acc:  0.73\n",
      "692 : loss:  0.88888764 \t acc:  0.61\n",
      "694 : loss:  0.8288999 \t acc:  0.55\n",
      "696 : loss:  0.7457062 \t acc:  0.61\n",
      "698 : loss:  0.7315925 \t acc:  0.66\n",
      "700 : loss:  0.81096846 \t acc:  0.64\n",
      "702 : loss:  0.8194426 \t acc:  0.59\n",
      "704 : loss:  0.90018237 \t acc:  0.58\n",
      "706 : loss:  0.8217433 \t acc:  0.6\n",
      "708 : loss:  0.81543463 \t acc:  0.63\n",
      "710 : loss:  0.8906296 \t acc:  0.51\n",
      "712 : loss:  0.86926407 \t acc:  0.59\n",
      "714 : loss:  0.88827574 \t acc:  0.55\n",
      "716 : loss:  0.74866945 \t acc:  0.68\n",
      "718 : loss:  0.7960503 \t acc:  0.63\n",
      "720 : loss:  0.8593582 \t acc:  0.57\n",
      "722 : loss:  0.78436697 \t acc:  0.65\n",
      "724 : loss:  0.8332231 \t acc:  0.59\n",
      "726 : loss:  0.78245026 \t acc:  0.63\n",
      "728 : loss:  0.75968325 \t acc:  0.62\n",
      "730 : loss:  0.96531576 \t acc:  0.52\n",
      "732 : loss:  0.83167326 \t acc:  0.59\n",
      "734 : loss:  0.85845655 \t acc:  0.58\n",
      "736 : loss:  0.8182313 \t acc:  0.61\n",
      "738 : loss:  0.7926173 \t acc:  0.62\n",
      "740 : loss:  0.8326113 \t acc:  0.6\n",
      "742 : loss:  0.78470385 \t acc:  0.64\n",
      "\n",
      "Saving...\n",
      "saved to models/CNN/pretrained_model.ckpt-744\n",
      "\n",
      "100 \t [66.37082571 63.15303563 48.6972132 ]\n",
      "1 \tval accuracy:  0.607 \t f_! score:  [0.66370826 0.63153036 0.48697213]\n",
      "\n",
      "744 : loss:  0.8916222 \t acc:  0.55\n",
      "746 : loss:  0.8195887 \t acc:  0.64\n",
      "748 : loss:  0.91369146 \t acc:  0.56\n",
      "750 : loss:  0.8612225 \t acc:  0.61\n",
      "752 : loss:  0.8303835 \t acc:  0.62\n",
      "754 : loss:  0.77635074 \t acc:  0.63\n",
      "756 : loss:  0.84733015 \t acc:  0.63\n",
      "758 : loss:  0.69346404 \t acc:  0.71\n",
      "760 : loss:  0.85508835 \t acc:  0.53\n",
      "762 : loss:  0.8525804 \t acc:  0.59\n",
      "764 : loss:  0.92279816 \t acc:  0.55\n",
      "766 : loss:  0.6979677 \t acc:  0.68\n",
      "768 : loss:  0.8295711 \t acc:  0.61\n",
      "770 : loss:  0.8304528 \t acc:  0.58\n",
      "772 : loss:  0.7804987 \t acc:  0.66\n",
      "774 : loss:  0.82818216 \t acc:  0.59\n",
      "776 : loss:  0.8404711 \t acc:  0.54\n",
      "778 : loss:  0.7213112 \t acc:  0.65\n",
      "780 : loss:  0.73759216 \t acc:  0.7\n",
      "782 : loss:  0.74071455 \t acc:  0.7\n",
      "784 : loss:  0.8618903 \t acc:  0.52\n",
      "786 : loss:  0.85065436 \t acc:  0.55\n",
      "788 : loss:  0.9022144 \t acc:  0.59\n",
      "790 : loss:  0.8102348 \t acc:  0.59\n",
      "792 : loss:  0.9460728 \t acc:  0.5\n",
      "794 : loss:  0.9057858 \t acc:  0.52\n",
      "796 : loss:  0.8453537 \t acc:  0.65\n",
      "798 : loss:  0.75178933 \t acc:  0.64\n",
      "800 : loss:  0.9039084 \t acc:  0.57\n",
      "802 : loss:  0.9110726 \t acc:  0.56\n",
      "804 : loss:  0.8450759 \t acc:  0.6\n",
      "806 : loss:  0.8632967 \t acc:  0.59\n",
      "808 : loss:  0.84239125 \t acc:  0.65\n",
      "810 : loss:  0.81174105 \t acc:  0.54\n",
      "812 : loss:  0.7610378 \t acc:  0.71\n",
      "814 : loss:  0.8351584 \t acc:  0.6\n",
      "816 : loss:  0.8539743 \t acc:  0.56\n",
      "818 : loss:  0.8540495 \t acc:  0.62\n",
      "820 : loss:  0.781523 \t acc:  0.64\n",
      "822 : loss:  1.0316625 \t acc:  0.52\n",
      "824 : loss:  0.8237465 \t acc:  0.59\n",
      "826 : loss:  0.83999914 \t acc:  0.62\n",
      "828 : loss:  0.9060878 \t acc:  0.57\n",
      "830 : loss:  0.8963873 \t acc:  0.58\n",
      "832 : loss:  0.92315245 \t acc:  0.54\n",
      "834 : loss:  0.85599005 \t acc:  0.57\n",
      "836 : loss:  0.7873101 \t acc:  0.67\n",
      "838 : loss:  0.86622846 \t acc:  0.52\n",
      "840 : loss:  0.7570825 \t acc:  0.7\n",
      "842 : loss:  0.85299116 \t acc:  0.62\n",
      "844 : loss:  0.7740811 \t acc:  0.64\n",
      "846 : loss:  0.7762855 \t acc:  0.65\n",
      "848 : loss:  0.8478503 \t acc:  0.62\n",
      "850 : loss:  0.7458088 \t acc:  0.65\n",
      "852 : loss:  0.8565091 \t acc:  0.61\n",
      "854 : loss:  0.8143322 \t acc:  0.57\n",
      "856 : loss:  0.84740746 \t acc:  0.6\n",
      "858 : loss:  0.83794695 \t acc:  0.61\n",
      "860 : loss:  0.8379923 \t acc:  0.6\n",
      "862 : loss:  0.94258845 \t acc:  0.53\n",
      "864 : loss:  0.7427766 \t acc:  0.62\n",
      "866 : loss:  0.9005352 \t acc:  0.5\n",
      "868 : loss:  0.74165636 \t acc:  0.71\n",
      "870 : loss:  0.8064483 \t acc:  0.64\n",
      "872 : loss:  0.9181346 \t acc:  0.62\n",
      "874 : loss:  0.8626383 \t acc:  0.6\n",
      "876 : loss:  0.7573484 \t acc:  0.65\n",
      "878 : loss:  0.85996896 \t acc:  0.59\n",
      "880 : loss:  0.9076383 \t acc:  0.57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "882 : loss:  0.88546973 \t acc:  0.59\n",
      "884 : loss:  0.801887 \t acc:  0.66\n",
      "886 : loss:  0.9400062 \t acc:  0.6\n",
      "888 : loss:  0.8144732 \t acc:  0.61\n",
      "890 : loss:  0.8708709 \t acc:  0.57\n",
      "892 : loss:  0.84233856 \t acc:  0.65\n",
      "894 : loss:  0.897707 \t acc:  0.6\n",
      "896 : loss:  0.8946808 \t acc:  0.55\n",
      "898 : loss:  0.83626074 \t acc:  0.59\n",
      "900 : loss:  0.8796009 \t acc:  0.55\n",
      "902 : loss:  0.82458884 \t acc:  0.59\n",
      "904 : loss:  0.84928954 \t acc:  0.58\n",
      "906 : loss:  0.7349902 \t acc:  0.63\n",
      "908 : loss:  0.83547837 \t acc:  0.63\n",
      "910 : loss:  0.747117 \t acc:  0.69\n",
      "912 : loss:  0.8203536 \t acc:  0.6\n",
      "914 : loss:  0.8770771 \t acc:  0.59\n",
      "916 : loss:  0.95630777 \t acc:  0.5\n",
      "918 : loss:  0.7600842 \t acc:  0.66\n",
      "920 : loss:  0.84990364 \t acc:  0.56\n",
      "922 : loss:  0.84871453 \t acc:  0.65\n",
      "924 : loss:  0.7061712 \t acc:  0.73\n",
      "926 : loss:  0.76609945 \t acc:  0.63\n",
      "928 : loss:  0.8207324 \t acc:  0.58\n",
      "930 : loss:  0.7985154 \t acc:  0.59\n",
      "932 : loss:  0.8303524 \t acc:  0.62\n",
      "934 : loss:  0.8610357 \t acc:  0.64\n",
      "936 : loss:  0.84435767 \t acc:  0.58\n",
      "938 : loss:  0.8429584 \t acc:  0.55\n",
      "940 : loss:  0.80732274 \t acc:  0.62\n",
      "942 : loss:  0.8361682 \t acc:  0.54\n",
      "944 : loss:  0.945274 \t acc:  0.52\n",
      "946 : loss:  0.80632365 \t acc:  0.66\n",
      "948 : loss:  0.92075723 \t acc:  0.52\n",
      "950 : loss:  0.888155 \t acc:  0.61\n",
      "952 : loss:  0.71586096 \t acc:  0.68\n",
      "954 : loss:  0.78142756 \t acc:  0.61\n",
      "956 : loss:  0.73671097 \t acc:  0.62\n",
      "958 : loss:  0.720055 \t acc:  0.68\n",
      "960 : loss:  0.836698 \t acc:  0.64\n",
      "962 : loss:  0.959917 \t acc:  0.5\n",
      "964 : loss:  0.77801293 \t acc:  0.65\n",
      "966 : loss:  0.77181983 \t acc:  0.66\n",
      "968 : loss:  0.9578148 \t acc:  0.59\n",
      "970 : loss:  0.8487047 \t acc:  0.59\n",
      "972 : loss:  0.8670681 \t acc:  0.64\n",
      "974 : loss:  0.8354924 \t acc:  0.63\n",
      "976 : loss:  0.78105134 \t acc:  0.64\n",
      "978 : loss:  0.7108524 \t acc:  0.72\n",
      "980 : loss:  0.7780505 \t acc:  0.66\n",
      "982 : loss:  0.83984447 \t acc:  0.63\n",
      "984 : loss:  0.7972206 \t acc:  0.63\n",
      "986 : loss:  0.8350322 \t acc:  0.61\n",
      "988 : loss:  0.8632583 \t acc:  0.63\n",
      "990 : loss:  0.78868973 \t acc:  0.64\n",
      "992 : loss:  0.83108497 \t acc:  0.63\n",
      "994 : loss:  0.8664722 \t acc:  0.57\n",
      "996 : loss:  0.7554904 \t acc:  0.65\n",
      "998 : loss:  0.70674425 \t acc:  0.67\n",
      "1000 : loss:  0.7071179 \t acc:  0.73\n",
      "1002 : loss:  0.83585143 \t acc:  0.57\n",
      "1004 : loss:  0.8016017 \t acc:  0.59\n",
      "1006 : loss:  0.71220076 \t acc:  0.65\n",
      "1008 : loss:  0.80152243 \t acc:  0.59\n",
      "1010 : loss:  0.89172477 \t acc:  0.54\n",
      "1012 : loss:  0.7457667 \t acc:  0.66\n",
      "1014 : loss:  0.77565026 \t acc:  0.69\n",
      "1016 : loss:  0.72799385 \t acc:  0.69\n",
      "1018 : loss:  0.75899535 \t acc:  0.63\n",
      "1020 : loss:  0.7739591 \t acc:  0.57\n",
      "1022 : loss:  0.8253757 \t acc:  0.62\n",
      "1024 : loss:  0.86414856 \t acc:  0.6\n",
      "1026 : loss:  0.9078816 \t acc:  0.58\n",
      "1028 : loss:  0.7922688 \t acc:  0.63\n",
      "1030 : loss:  0.9400827 \t acc:  0.55\n",
      "1032 : loss:  0.73621386 \t acc:  0.68\n",
      "1034 : loss:  0.77277815 \t acc:  0.63\n",
      "1036 : loss:  0.7722038 \t acc:  0.72\n",
      "1038 : loss:  0.8277414 \t acc:  0.57\n",
      "1040 : loss:  0.75270194 \t acc:  0.66\n",
      "1042 : loss:  0.7094005 \t acc:  0.72\n",
      "1044 : loss:  0.8293627 \t acc:  0.62\n",
      "1046 : loss:  0.75297964 \t acc:  0.66\n",
      "1048 : loss:  0.8398604 \t acc:  0.62\n",
      "1050 : loss:  0.9271798 \t acc:  0.58\n",
      "1052 : loss:  0.80834174 \t acc:  0.63\n",
      "1054 : loss:  0.73382944 \t acc:  0.62\n",
      "1056 : loss:  0.88650537 \t acc:  0.51\n",
      "1058 : loss:  0.81199783 \t acc:  0.57\n",
      "1060 : loss:  0.87247354 \t acc:  0.57\n",
      "1062 : loss:  0.8151253 \t acc:  0.58\n",
      "1064 : loss:  0.91108966 \t acc:  0.59\n",
      "1066 : loss:  0.74763596 \t acc:  0.68\n",
      "1068 : loss:  0.90794754 \t acc:  0.59\n",
      "1070 : loss:  0.8084641 \t acc:  0.54\n",
      "1072 : loss:  0.7730365 \t acc:  0.61\n",
      "1074 : loss:  0.8766214 \t acc:  0.55\n",
      "1076 : loss:  0.8447832 \t acc:  0.61\n",
      "1078 : loss:  0.8840111 \t acc:  0.62\n",
      "1080 : loss:  0.9015633 \t acc:  0.58\n",
      "1082 : loss:  0.88633186 \t acc:  0.6\n",
      "1084 : loss:  0.8270106 \t acc:  0.61\n",
      "1086 : loss:  0.7684557 \t acc:  0.68\n",
      "1088 : loss:  0.78844374 \t acc:  0.61\n",
      "1090 : loss:  0.85440993 \t acc:  0.64\n",
      "1092 : loss:  0.7981867 \t acc:  0.63\n",
      "1094 : loss:  0.82152236 \t acc:  0.58\n",
      "1096 : loss:  0.78099996 \t acc:  0.63\n",
      "1098 : loss:  0.8888931 \t acc:  0.59\n",
      "1100 : loss:  0.8721615 \t acc:  0.59\n",
      "1102 : loss:  0.8639648 \t acc:  0.59\n",
      "1104 : loss:  0.8549853 \t acc:  0.65\n",
      "1106 : loss:  0.80692595 \t acc:  0.6\n",
      "1108 : loss:  0.7877456 \t acc:  0.65\n",
      "1110 : loss:  0.81555104 \t acc:  0.63\n",
      "1112 : loss:  0.6782608 \t acc:  0.7\n",
      "1114 : loss:  0.7354641 \t acc:  0.69\n",
      "\n",
      "Saving...\n",
      "saved to models/CNN/pretrained_model.ckpt-1116\n",
      "\n",
      "100 \t [67.34335454 62.37145905 53.26936586]\n",
      "2 \tval accuracy:  0.61759996 \t f_! score:  [0.67343355 0.62371459 0.53269366]\n",
      "\n",
      "1116 : loss:  0.84148526 \t acc:  0.6\n",
      "1118 : loss:  0.7281052 \t acc:  0.67\n",
      "1120 : loss:  0.9316014 \t acc:  0.62\n",
      "1122 : loss:  0.72913814 \t acc:  0.66\n",
      "1124 : loss:  0.7584175 \t acc:  0.67\n",
      "1126 : loss:  0.8348904 \t acc:  0.59\n",
      "1128 : loss:  0.79198927 \t acc:  0.66\n",
      "1130 : loss:  0.83452094 \t acc:  0.68\n",
      "1132 : loss:  0.8885101 \t acc:  0.55\n",
      "1134 : loss:  0.76804566 \t acc:  0.62\n",
      "1136 : loss:  0.78632563 \t acc:  0.6\n",
      "1138 : loss:  0.7340153 \t acc:  0.69\n",
      "1140 : loss:  0.7080774 \t acc:  0.67\n",
      "1142 : loss:  0.70947385 \t acc:  0.65\n",
      "1144 : loss:  0.8506314 \t acc:  0.58\n",
      "1146 : loss:  0.84736407 \t acc:  0.59\n",
      "1148 : loss:  0.67898417 \t acc:  0.72\n",
      "1150 : loss:  0.85805404 \t acc:  0.61\n",
      "1152 : loss:  0.8402788 \t acc:  0.6\n",
      "1154 : loss:  0.7486344 \t acc:  0.66\n",
      "1156 : loss:  0.7133026 \t acc:  0.7\n",
      "1158 : loss:  0.9461304 \t acc:  0.56\n",
      "1160 : loss:  0.7664515 \t acc:  0.63\n",
      "1162 : loss:  0.7384494 \t acc:  0.6\n",
      "1164 : loss:  0.68385106 \t acc:  0.68\n",
      "1166 : loss:  0.67860925 \t acc:  0.64\n",
      "1168 : loss:  0.6904978 \t acc:  0.71\n",
      "1170 : loss:  0.7866709 \t acc:  0.63\n",
      "1172 : loss:  0.7884729 \t acc:  0.62\n",
      "1174 : loss:  0.77995247 \t acc:  0.61\n",
      "1176 : loss:  0.8575268 \t acc:  0.63\n",
      "1178 : loss:  0.7884733 \t acc:  0.62\n",
      "1180 : loss:  0.75295526 \t acc:  0.66\n",
      "1182 : loss:  0.72518903 \t acc:  0.73\n",
      "1184 : loss:  0.8404691 \t acc:  0.6\n",
      "1186 : loss:  0.8485636 \t acc:  0.61\n",
      "1188 : loss:  0.80152166 \t acc:  0.6\n",
      "1190 : loss:  0.7674593 \t acc:  0.7\n",
      "1192 : loss:  0.71259016 \t acc:  0.73\n",
      "1194 : loss:  0.9011357 \t acc:  0.57\n",
      "1196 : loss:  0.81472105 \t acc:  0.64\n",
      "1198 : loss:  0.703473 \t acc:  0.65\n",
      "1200 : loss:  0.77897173 \t acc:  0.66\n",
      "1202 : loss:  0.699443 \t acc:  0.66\n",
      "1204 : loss:  0.85960466 \t acc:  0.62\n",
      "1206 : loss:  0.81787276 \t acc:  0.63\n",
      "1208 : loss:  0.7870363 \t acc:  0.69\n",
      "1210 : loss:  0.7892827 \t acc:  0.6\n",
      "1212 : loss:  0.73343015 \t acc:  0.66\n",
      "1214 : loss:  0.8298655 \t acc:  0.61\n",
      "1216 : loss:  0.76089305 \t acc:  0.61\n",
      "1218 : loss:  0.7513901 \t acc:  0.67\n",
      "1220 : loss:  0.8113027 \t acc:  0.6\n",
      "1222 : loss:  0.76912653 \t acc:  0.67\n",
      "1224 : loss:  0.84873396 \t acc:  0.61\n",
      "1226 : loss:  0.76419497 \t acc:  0.66\n",
      "1228 : loss:  0.81486464 \t acc:  0.56\n",
      "1230 : loss:  0.751605 \t acc:  0.69\n",
      "1232 : loss:  0.8235146 \t acc:  0.64\n",
      "1234 : loss:  0.87346566 \t acc:  0.57\n",
      "1236 : loss:  0.73786473 \t acc:  0.68\n",
      "1238 : loss:  0.7842693 \t acc:  0.65\n",
      "1240 : loss:  0.8229325 \t acc:  0.6\n",
      "1242 : loss:  0.86377853 \t acc:  0.6\n",
      "1244 : loss:  0.7652694 \t acc:  0.57\n",
      "1246 : loss:  0.8261659 \t acc:  0.51\n",
      "1248 : loss:  0.7823438 \t acc:  0.64\n",
      "1250 : loss:  0.811567 \t acc:  0.61\n",
      "1252 : loss:  0.7367559 \t acc:  0.71\n",
      "1254 : loss:  0.7532212 \t acc:  0.62\n",
      "1256 : loss:  0.7630834 \t acc:  0.66\n",
      "1258 : loss:  0.8206556 \t acc:  0.55\n",
      "1260 : loss:  0.82504344 \t acc:  0.63\n",
      "1262 : loss:  0.7361458 \t acc:  0.63\n",
      "1264 : loss:  0.784405 \t acc:  0.66\n",
      "1266 : loss:  0.8866542 \t acc:  0.54\n",
      "1268 : loss:  0.9605088 \t acc:  0.58\n",
      "1270 : loss:  0.7700037 \t acc:  0.68\n",
      "1272 : loss:  0.8383214 \t acc:  0.62\n",
      "1274 : loss:  0.7112278 \t acc:  0.69\n",
      "1276 : loss:  0.81146055 \t acc:  0.6\n",
      "1278 : loss:  0.69386125 \t acc:  0.72\n",
      "1280 : loss:  0.7151168 \t acc:  0.65\n",
      "1282 : loss:  0.7332622 \t acc:  0.66\n",
      "1284 : loss:  0.7089501 \t acc:  0.7\n",
      "1286 : loss:  0.82272696 \t acc:  0.6\n",
      "1288 : loss:  0.7457256 \t acc:  0.68\n",
      "1290 : loss:  0.85415816 \t acc:  0.66\n",
      "1292 : loss:  0.8402198 \t acc:  0.57\n",
      "1294 : loss:  0.6891692 \t acc:  0.7\n",
      "1296 : loss:  0.91428137 \t acc:  0.54\n",
      "1298 : loss:  0.8240637 \t acc:  0.6\n",
      "1300 : loss:  0.8072271 \t acc:  0.62\n",
      "1302 : loss:  0.82188916 \t acc:  0.54\n",
      "1304 : loss:  0.9154279 \t acc:  0.57\n",
      "1306 : loss:  0.83342636 \t acc:  0.58\n",
      "1308 : loss:  0.7567386 \t acc:  0.64\n",
      "1310 : loss:  0.7573484 \t acc:  0.65\n",
      "1312 : loss:  0.7934864 \t acc:  0.62\n",
      "1314 : loss:  0.77620286 \t acc:  0.67\n",
      "1316 : loss:  0.8019788 \t acc:  0.64\n",
      "1318 : loss:  0.8654357 \t acc:  0.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1320 : loss:  0.7846176 \t acc:  0.59\n",
      "1322 : loss:  0.7243121 \t acc:  0.69\n",
      "1324 : loss:  0.752239 \t acc:  0.69\n",
      "1326 : loss:  0.8090014 \t acc:  0.65\n",
      "1328 : loss:  0.7786683 \t acc:  0.65\n",
      "1330 : loss:  0.8212597 \t acc:  0.57\n",
      "1332 : loss:  0.8105448 \t acc:  0.61\n",
      "1334 : loss:  0.9463553 \t acc:  0.51\n",
      "1336 : loss:  0.7364812 \t acc:  0.58\n",
      "1338 : loss:  0.73971057 \t acc:  0.66\n",
      "1340 : loss:  0.91768736 \t acc:  0.56\n",
      "1342 : loss:  0.76301146 \t acc:  0.64\n",
      "1344 : loss:  0.8063117 \t acc:  0.69\n",
      "1346 : loss:  0.88393694 \t acc:  0.56\n",
      "1348 : loss:  0.7459226 \t acc:  0.65\n",
      "1350 : loss:  0.836582 \t acc:  0.65\n",
      "1352 : loss:  0.85140973 \t acc:  0.64\n",
      "1354 : loss:  0.79581064 \t acc:  0.63\n",
      "1356 : loss:  0.74262047 \t acc:  0.66\n",
      "1358 : loss:  0.79936653 \t acc:  0.61\n",
      "1360 : loss:  0.9414326 \t acc:  0.55\n",
      "1362 : loss:  0.7562604 \t acc:  0.6\n",
      "1364 : loss:  0.81596565 \t acc:  0.61\n",
      "1366 : loss:  0.8587252 \t acc:  0.57\n",
      "1368 : loss:  0.8179461 \t acc:  0.68\n",
      "1370 : loss:  0.90487236 \t acc:  0.55\n",
      "1372 : loss:  0.6892228 \t acc:  0.66\n",
      "1374 : loss:  0.664531 \t acc:  0.71\n",
      "1376 : loss:  0.72124755 \t acc:  0.69\n",
      "1378 : loss:  0.9945417 \t acc:  0.59\n",
      "1380 : loss:  0.93879133 \t acc:  0.6\n",
      "1382 : loss:  0.80144954 \t acc:  0.63\n",
      "1384 : loss:  0.7792969 \t acc:  0.66\n",
      "1386 : loss:  0.75133413 \t acc:  0.7\n",
      "1388 : loss:  0.9144214 \t acc:  0.55\n",
      "1390 : loss:  0.94053113 \t acc:  0.51\n",
      "1392 : loss:  0.8314607 \t acc:  0.61\n",
      "1394 : loss:  0.9543554 \t acc:  0.49\n",
      "1396 : loss:  0.7297335 \t acc:  0.7\n",
      "1398 : loss:  0.817543 \t acc:  0.65\n",
      "1400 : loss:  0.78528124 \t acc:  0.63\n",
      "1402 : loss:  0.8604219 \t acc:  0.55\n",
      "1404 : loss:  0.76103294 \t acc:  0.63\n",
      "1406 : loss:  0.91758955 \t acc:  0.55\n",
      "1408 : loss:  0.8257266 \t acc:  0.59\n",
      "1410 : loss:  0.7483641 \t acc:  0.68\n",
      "1412 : loss:  0.7445553 \t acc:  0.64\n",
      "1414 : loss:  0.73716086 \t acc:  0.64\n",
      "1416 : loss:  0.9005188 \t acc:  0.58\n",
      "1418 : loss:  0.7180062 \t acc:  0.72\n",
      "1420 : loss:  0.8370296 \t acc:  0.56\n",
      "1422 : loss:  0.8342524 \t acc:  0.54\n",
      "1424 : loss:  0.7882369 \t acc:  0.58\n",
      "1426 : loss:  0.8093954 \t acc:  0.67\n",
      "1428 : loss:  0.8340342 \t acc:  0.62\n",
      "1430 : loss:  0.72947085 \t acc:  0.7\n",
      "1432 : loss:  0.8158467 \t acc:  0.61\n",
      "1434 : loss:  0.81230116 \t acc:  0.67\n",
      "1436 : loss:  0.78343236 \t acc:  0.66\n",
      "1438 : loss:  0.7789577 \t acc:  0.67\n",
      "1440 : loss:  0.684739 \t acc:  0.66\n",
      "1442 : loss:  0.7780739 \t acc:  0.64\n",
      "1444 : loss:  0.7835836 \t acc:  0.64\n",
      "1446 : loss:  0.7430928 \t acc:  0.69\n",
      "1448 : loss:  0.801457 \t acc:  0.63\n",
      "1450 : loss:  0.7868695 \t acc:  0.66\n",
      "1452 : loss:  0.73405015 \t acc:  0.63\n",
      "1454 : loss:  0.7734096 \t acc:  0.65\n",
      "1456 : loss:  0.7643877 \t acc:  0.64\n",
      "1458 : loss:  0.6550095 \t acc:  0.7\n",
      "1460 : loss:  0.80869967 \t acc:  0.6\n",
      "1462 : loss:  0.9020013 \t acc:  0.51\n",
      "1464 : loss:  0.833607 \t acc:  0.61\n",
      "1466 : loss:  0.76012385 \t acc:  0.72\n",
      "1468 : loss:  0.71767163 \t acc:  0.68\n",
      "1470 : loss:  0.7812571 \t acc:  0.6\n",
      "1472 : loss:  0.8329227 \t acc:  0.61\n",
      "1474 : loss:  0.924611 \t acc:  0.58\n",
      "1476 : loss:  0.8528025 \t acc:  0.59\n",
      "1478 : loss:  0.68748474 \t acc:  0.69\n",
      "1480 : loss:  0.7469923 \t acc:  0.65\n",
      "1482 : loss:  0.7120862 \t acc:  0.7\n",
      "1484 : loss:  0.7585935 \t acc:  0.65\n",
      "1486 : loss:  0.8359087 \t acc:  0.63\n",
      "\n",
      "Saving...\n",
      "saved to models/CNN/pretrained_model.ckpt-1488\n",
      "\n",
      "100 \t [68.31163679 64.15288478 52.14716094]\n",
      "3 \tval accuracy:  0.62560004 \t f_! score:  [0.68311637 0.64152885 0.52147161]\n",
      "\n",
      "1488 : loss:  0.76484406 \t acc:  0.61\n",
      "1490 : loss:  0.68756026 \t acc:  0.72\n",
      "1492 : loss:  0.8332097 \t acc:  0.66\n",
      "1494 : loss:  0.7025328 \t acc:  0.71\n",
      "1496 : loss:  0.76336265 \t acc:  0.67\n",
      "1498 : loss:  0.84053725 \t acc:  0.61\n",
      "1500 : loss:  0.7177328 \t acc:  0.73\n",
      "1502 : loss:  0.71220034 \t acc:  0.68\n",
      "1504 : loss:  0.5960382 \t acc:  0.78\n",
      "1506 : loss:  0.7258849 \t acc:  0.72\n",
      "1508 : loss:  0.7224002 \t acc:  0.7\n",
      "1510 : loss:  0.6931417 \t acc:  0.69\n",
      "1512 : loss:  0.74567705 \t acc:  0.7\n",
      "1514 : loss:  0.78221023 \t acc:  0.67\n",
      "1516 : loss:  0.79856247 \t acc:  0.65\n",
      "1518 : loss:  0.7294752 \t acc:  0.68\n",
      "1520 : loss:  0.78280824 \t acc:  0.62\n",
      "1522 : loss:  0.8047265 \t acc:  0.64\n",
      "1524 : loss:  0.6957646 \t acc:  0.69\n",
      "1526 : loss:  0.879543 \t acc:  0.54\n",
      "1528 : loss:  0.86029917 \t acc:  0.61\n",
      "1530 : loss:  0.73321617 \t acc:  0.65\n",
      "1532 : loss:  0.61838436 \t acc:  0.77\n",
      "1534 : loss:  0.7503134 \t acc:  0.62\n",
      "1536 : loss:  0.6738933 \t acc:  0.78\n",
      "1538 : loss:  0.85073805 \t acc:  0.66\n",
      "1540 : loss:  0.85823935 \t acc:  0.6\n",
      "1542 : loss:  0.8372762 \t acc:  0.56\n",
      "1544 : loss:  0.6751286 \t acc:  0.72\n",
      "1546 : loss:  0.86697495 \t acc:  0.57\n",
      "1548 : loss:  0.7597507 \t acc:  0.64\n",
      "1550 : loss:  0.76707536 \t acc:  0.63\n",
      "1552 : loss:  0.83066905 \t acc:  0.61\n",
      "1554 : loss:  0.7328566 \t acc:  0.69\n",
      "1556 : loss:  0.73870605 \t acc:  0.7\n",
      "1558 : loss:  0.7519515 \t acc:  0.65\n",
      "1560 : loss:  0.90553457 \t acc:  0.59\n",
      "1562 : loss:  0.7059084 \t acc:  0.68\n",
      "1564 : loss:  0.79268265 \t acc:  0.66\n",
      "1566 : loss:  0.78674567 \t acc:  0.59\n",
      "1568 : loss:  0.77725077 \t acc:  0.67\n",
      "1570 : loss:  0.8117494 \t acc:  0.62\n",
      "1572 : loss:  0.65267026 \t acc:  0.73\n",
      "1574 : loss:  0.7368978 \t acc:  0.68\n",
      "1576 : loss:  0.70083994 \t acc:  0.68\n",
      "1578 : loss:  0.7838113 \t acc:  0.64\n",
      "1580 : loss:  0.7596988 \t acc:  0.62\n",
      "1582 : loss:  0.8495097 \t acc:  0.61\n",
      "1584 : loss:  0.78659457 \t acc:  0.68\n",
      "1586 : loss:  0.764588 \t acc:  0.64\n",
      "1588 : loss:  0.74936265 \t acc:  0.63\n",
      "1590 : loss:  0.77200204 \t acc:  0.61\n",
      "1592 : loss:  0.6968612 \t acc:  0.67\n",
      "1594 : loss:  0.6801406 \t acc:  0.65\n",
      "1596 : loss:  0.84607756 \t acc:  0.6\n",
      "1598 : loss:  0.7908489 \t acc:  0.65\n",
      "1600 : loss:  0.74500656 \t acc:  0.67\n",
      "1602 : loss:  0.77789235 \t acc:  0.62\n",
      "1604 : loss:  0.8715449 \t acc:  0.62\n",
      "1606 : loss:  0.83868986 \t acc:  0.63\n",
      "1608 : loss:  0.67864203 \t acc:  0.7\n",
      "1610 : loss:  0.89416826 \t acc:  0.59\n",
      "1612 : loss:  0.96046114 \t acc:  0.5\n",
      "1614 : loss:  0.8828163 \t acc:  0.58\n",
      "1616 : loss:  0.76524734 \t acc:  0.63\n",
      "1618 : loss:  0.7257033 \t acc:  0.65\n",
      "1620 : loss:  0.7374519 \t acc:  0.62\n",
      "1622 : loss:  0.74764764 \t acc:  0.63\n",
      "1624 : loss:  0.7426099 \t acc:  0.67\n",
      "1626 : loss:  0.9821778 \t acc:  0.58\n",
      "1628 : loss:  0.8452192 \t acc:  0.62\n",
      "1630 : loss:  0.91991997 \t acc:  0.56\n",
      "1632 : loss:  0.89758 \t acc:  0.64\n",
      "1634 : loss:  0.75449556 \t acc:  0.62\n",
      "1636 : loss:  0.8065019 \t acc:  0.56\n",
      "1638 : loss:  0.827682 \t acc:  0.59\n",
      "1640 : loss:  0.88592017 \t acc:  0.6\n",
      "1642 : loss:  0.7656489 \t acc:  0.68\n",
      "1644 : loss:  0.8837698 \t acc:  0.54\n",
      "1646 : loss:  0.9780965 \t acc:  0.52\n",
      "1648 : loss:  0.8728847 \t acc:  0.61\n",
      "1650 : loss:  0.7732959 \t acc:  0.63\n",
      "1652 : loss:  0.833734 \t acc:  0.6\n",
      "1654 : loss:  0.8616219 \t acc:  0.63\n",
      "1656 : loss:  0.785124 \t acc:  0.63\n",
      "1658 : loss:  0.76803946 \t acc:  0.73\n",
      "1660 : loss:  0.7078325 \t acc:  0.69\n",
      "1662 : loss:  0.7763672 \t acc:  0.65\n",
      "1664 : loss:  0.6825378 \t acc:  0.73\n",
      "1666 : loss:  0.781823 \t acc:  0.66\n",
      "1668 : loss:  0.6949934 \t acc:  0.7\n",
      "1670 : loss:  0.60781115 \t acc:  0.76\n",
      "1672 : loss:  0.64955634 \t acc:  0.67\n",
      "1674 : loss:  0.7698332 \t acc:  0.63\n",
      "1676 : loss:  0.91733813 \t acc:  0.58\n",
      "1678 : loss:  0.8038091 \t acc:  0.64\n",
      "1680 : loss:  0.783684 \t acc:  0.64\n",
      "1682 : loss:  0.7369759 \t acc:  0.72\n",
      "1684 : loss:  0.8070731 \t acc:  0.64\n",
      "1686 : loss:  0.81739885 \t acc:  0.61\n",
      "1688 : loss:  0.80733687 \t acc:  0.59\n",
      "1690 : loss:  0.804843 \t acc:  0.68\n",
      "1692 : loss:  0.631593 \t acc:  0.74\n",
      "1694 : loss:  0.9263659 \t acc:  0.58\n",
      "1696 : loss:  0.99951935 \t acc:  0.54\n",
      "1698 : loss:  0.80317223 \t acc:  0.6\n",
      "1700 : loss:  0.7957748 \t acc:  0.66\n",
      "1702 : loss:  0.7958329 \t acc:  0.68\n",
      "1704 : loss:  0.6577851 \t acc:  0.77\n",
      "1706 : loss:  0.954726 \t acc:  0.55\n",
      "1708 : loss:  0.6809301 \t acc:  0.66\n",
      "1710 : loss:  0.8861217 \t acc:  0.64\n",
      "1712 : loss:  0.73399997 \t acc:  0.67\n",
      "1714 : loss:  0.72521186 \t acc:  0.69\n",
      "1716 : loss:  0.88550705 \t acc:  0.6\n",
      "1718 : loss:  0.7679816 \t acc:  0.67\n",
      "1720 : loss:  0.8225147 \t acc:  0.62\n",
      "1722 : loss:  0.74931383 \t acc:  0.67\n",
      "1724 : loss:  0.7691451 \t acc:  0.65\n",
      "1726 : loss:  0.7205254 \t acc:  0.71\n",
      "1728 : loss:  0.77265096 \t acc:  0.63\n",
      "1730 : loss:  0.7304346 \t acc:  0.67\n",
      "1732 : loss:  0.6471306 \t acc:  0.72\n",
      "1734 : loss:  0.84843737 \t acc:  0.59\n",
      "1736 : loss:  0.89510846 \t acc:  0.62\n",
      "1738 : loss:  0.8150449 \t acc:  0.64\n",
      "1740 : loss:  0.77277535 \t acc:  0.61\n",
      "1742 : loss:  0.764201 \t acc:  0.7\n",
      "1744 : loss:  0.78535944 \t acc:  0.61\n",
      "1746 : loss:  0.8284703 \t acc:  0.62\n",
      "1748 : loss:  0.7971243 \t acc:  0.62\n",
      "1750 : loss:  0.8718922 \t acc:  0.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1752 : loss:  0.79964066 \t acc:  0.61\n",
      "1754 : loss:  0.82613546 \t acc:  0.6\n",
      "1756 : loss:  0.7649016 \t acc:  0.64\n",
      "1758 : loss:  0.6872853 \t acc:  0.68\n",
      "1760 : loss:  0.8186683 \t acc:  0.64\n",
      "1762 : loss:  0.82510394 \t acc:  0.63\n",
      "1764 : loss:  0.87233347 \t acc:  0.58\n",
      "1766 : loss:  0.7389818 \t acc:  0.69\n",
      "1768 : loss:  0.76347506 \t acc:  0.61\n",
      "1770 : loss:  0.77865267 \t acc:  0.65\n",
      "1772 : loss:  0.86842227 \t acc:  0.59\n",
      "1774 : loss:  0.71391565 \t acc:  0.63\n",
      "1776 : loss:  0.6471219 \t acc:  0.68\n",
      "1778 : loss:  0.880515 \t acc:  0.62\n",
      "1780 : loss:  0.7002704 \t acc:  0.74\n",
      "1782 : loss:  0.78552717 \t acc:  0.66\n",
      "1784 : loss:  0.7122881 \t acc:  0.69\n",
      "1786 : loss:  0.7136198 \t acc:  0.71\n",
      "1788 : loss:  0.78790635 \t acc:  0.69\n",
      "1790 : loss:  0.6095589 \t acc:  0.77\n",
      "1792 : loss:  0.8015262 \t acc:  0.57\n",
      "1794 : loss:  0.66412073 \t acc:  0.68\n",
      "1796 : loss:  0.6773423 \t acc:  0.7\n",
      "1798 : loss:  0.80534315 \t acc:  0.6\n",
      "1800 : loss:  0.7532923 \t acc:  0.69\n",
      "1802 : loss:  0.8225033 \t acc:  0.67\n",
      "1804 : loss:  0.7313613 \t acc:  0.69\n",
      "1806 : loss:  0.75556105 \t acc:  0.6\n",
      "1808 : loss:  0.693219 \t acc:  0.74\n",
      "1810 : loss:  0.657202 \t acc:  0.74\n",
      "1812 : loss:  0.71287596 \t acc:  0.65\n",
      "1814 : loss:  0.8559594 \t acc:  0.54\n",
      "1816 : loss:  0.7617977 \t acc:  0.6\n",
      "1818 : loss:  0.7478738 \t acc:  0.66\n",
      "1820 : loss:  0.74511117 \t acc:  0.72\n",
      "1822 : loss:  0.6851449 \t acc:  0.67\n",
      "1824 : loss:  0.72075534 \t acc:  0.67\n",
      "1826 : loss:  0.71687734 \t acc:  0.67\n",
      "1828 : loss:  0.6581659 \t acc:  0.72\n",
      "1830 : loss:  0.72365075 \t acc:  0.66\n",
      "1832 : loss:  0.72606 \t acc:  0.7\n",
      "1834 : loss:  0.7160933 \t acc:  0.68\n",
      "1836 : loss:  0.7366146 \t acc:  0.71\n",
      "1838 : loss:  0.796819 \t acc:  0.6\n",
      "1840 : loss:  0.71601576 \t acc:  0.66\n",
      "1842 : loss:  0.7267099 \t acc:  0.73\n",
      "1844 : loss:  0.8679232 \t acc:  0.62\n",
      "1846 : loss:  0.7485401 \t acc:  0.64\n",
      "1848 : loss:  0.7418889 \t acc:  0.69\n",
      "1850 : loss:  0.7145912 \t acc:  0.73\n",
      "1852 : loss:  0.6821617 \t acc:  0.73\n",
      "1854 : loss:  0.7932889 \t acc:  0.65\n",
      "1856 : loss:  0.78831655 \t acc:  0.6\n",
      "1858 : loss:  0.8004011 \t acc:  0.66\n",
      "\n",
      "Saving...\n",
      "saved to models/CNN/pretrained_model.ckpt-1860\n",
      "\n",
      "100 \t [68.99381775 62.64569002 48.91191859]\n",
      "4 \tval accuracy:  0.61980003 \t f_! score:  [0.68993818 0.6264569  0.48911919]\n",
      "\n",
      "1860 : loss:  0.8105896 \t acc:  0.65\n",
      "1862 : loss:  0.81383115 \t acc:  0.68\n",
      "1864 : loss:  0.6382751 \t acc:  0.7\n",
      "1866 : loss:  0.65081507 \t acc:  0.71\n",
      "1868 : loss:  0.70955276 \t acc:  0.67\n",
      "1870 : loss:  0.6339397 \t acc:  0.72\n",
      "1872 : loss:  0.7169645 \t acc:  0.66\n",
      "1874 : loss:  0.681 \t acc:  0.71\n",
      "1876 : loss:  0.7068662 \t acc:  0.77\n",
      "1878 : loss:  0.6446188 \t acc:  0.7\n",
      "1880 : loss:  0.90688425 \t acc:  0.58\n",
      "1882 : loss:  0.7205849 \t acc:  0.63\n",
      "1884 : loss:  0.5780039 \t acc:  0.73333335\n",
      "1886 : loss:  0.713853 \t acc:  0.68\n",
      "1888 : loss:  0.7561747 \t acc:  0.67\n",
      "1890 : loss:  0.67132944 \t acc:  0.73\n",
      "1892 : loss:  0.735641 \t acc:  0.64\n",
      "1894 : loss:  0.78097045 \t acc:  0.59\n",
      "1896 : loss:  0.73280317 \t acc:  0.65\n",
      "1898 : loss:  0.72867554 \t acc:  0.69\n",
      "1900 : loss:  0.726178 \t acc:  0.68\n",
      "1902 : loss:  0.7752111 \t acc:  0.67\n",
      "1904 : loss:  0.90779567 \t acc:  0.57\n",
      "1906 : loss:  0.8484015 \t acc:  0.62\n",
      "1908 : loss:  0.71957415 \t acc:  0.7\n",
      "1910 : loss:  0.66998804 \t acc:  0.68\n",
      "1912 : loss:  0.75657743 \t acc:  0.63\n",
      "1914 : loss:  0.80281454 \t acc:  0.64\n",
      "1916 : loss:  0.77286047 \t acc:  0.66\n",
      "1918 : loss:  0.6367719 \t acc:  0.73\n",
      "1920 : loss:  0.7812133 \t acc:  0.69\n",
      "1922 : loss:  0.8319402 \t acc:  0.58\n",
      "1924 : loss:  0.71223295 \t acc:  0.66\n",
      "1926 : loss:  0.7647441 \t acc:  0.62\n",
      "1928 : loss:  0.7029016 \t acc:  0.72\n",
      "1930 : loss:  0.7963112 \t acc:  0.62\n",
      "1932 : loss:  0.6791941 \t acc:  0.72\n",
      "1934 : loss:  0.7691495 \t acc:  0.66\n",
      "1936 : loss:  0.5817156 \t acc:  0.74\n",
      "1938 : loss:  0.6560574 \t acc:  0.73\n",
      "1940 : loss:  0.7142323 \t acc:  0.63\n",
      "1942 : loss:  0.6956796 \t acc:  0.64\n",
      "1944 : loss:  0.6693358 \t acc:  0.65\n",
      "1946 : loss:  0.7788844 \t acc:  0.6\n",
      "1948 : loss:  0.76630956 \t acc:  0.65\n",
      "1950 : loss:  0.7376273 \t acc:  0.66\n",
      "1952 : loss:  0.8115167 \t acc:  0.59\n",
      "1954 : loss:  0.7204958 \t acc:  0.65\n",
      "1956 : loss:  0.64556956 \t acc:  0.74\n",
      "1958 : loss:  0.70678484 \t acc:  0.69\n",
      "1960 : loss:  0.67065656 \t acc:  0.69\n",
      "1962 : loss:  0.75658643 \t acc:  0.61\n",
      "1964 : loss:  0.7095487 \t acc:  0.68\n",
      "1966 : loss:  0.7751671 \t acc:  0.63\n",
      "1968 : loss:  0.84993535 \t acc:  0.6\n",
      "1970 : loss:  0.6995334 \t acc:  0.67\n",
      "1972 : loss:  0.71187866 \t acc:  0.66\n",
      "1974 : loss:  0.6691156 \t acc:  0.71\n",
      "1976 : loss:  0.76756394 \t acc:  0.72\n",
      "1978 : loss:  0.8165071 \t acc:  0.6\n",
      "1980 : loss:  0.7247437 \t acc:  0.67\n",
      "1982 : loss:  0.68034226 \t acc:  0.68\n",
      "1984 : loss:  0.80361784 \t acc:  0.6\n",
      "1986 : loss:  0.69893616 \t acc:  0.64\n",
      "1988 : loss:  0.6901453 \t acc:  0.63\n",
      "1990 : loss:  0.8208535 \t acc:  0.56\n",
      "1992 : loss:  0.69104844 \t acc:  0.64\n",
      "1994 : loss:  0.7004719 \t acc:  0.69\n",
      "1996 : loss:  0.7358205 \t acc:  0.67\n",
      "1998 : loss:  0.69582266 \t acc:  0.7\n",
      "2000 : loss:  0.6653931 \t acc:  0.69\n",
      "2002 : loss:  0.62018853 \t acc:  0.69\n",
      "2004 : loss:  0.9249025 \t acc:  0.58\n",
      "2006 : loss:  0.70622987 \t acc:  0.69\n",
      "2008 : loss:  0.8530405 \t acc:  0.6\n",
      "2010 : loss:  0.7949548 \t acc:  0.65\n",
      "2012 : loss:  0.72947717 \t acc:  0.64\n",
      "2014 : loss:  0.72368515 \t acc:  0.65\n",
      "2016 : loss:  0.75575805 \t acc:  0.65\n",
      "2018 : loss:  0.8271083 \t acc:  0.62\n",
      "2020 : loss:  0.7319173 \t acc:  0.63\n",
      "2022 : loss:  0.80934614 \t acc:  0.61\n",
      "2024 : loss:  0.732048 \t acc:  0.67\n",
      "2026 : loss:  0.791018 \t acc:  0.62\n",
      "2028 : loss:  0.7351067 \t acc:  0.59\n",
      "2030 : loss:  0.70944947 \t acc:  0.67\n",
      "2032 : loss:  0.73287755 \t acc:  0.68\n",
      "2034 : loss:  0.7702792 \t acc:  0.66\n",
      "2036 : loss:  0.7889619 \t acc:  0.64\n",
      "2038 : loss:  0.72695 \t acc:  0.71\n",
      "2040 : loss:  0.839501 \t acc:  0.54\n",
      "2042 : loss:  0.648222 \t acc:  0.7\n",
      "2044 : loss:  0.69216317 \t acc:  0.71\n",
      "2046 : loss:  0.67879796 \t acc:  0.69\n",
      "2048 : loss:  0.77121943 \t acc:  0.68\n",
      "2050 : loss:  0.68092847 \t acc:  0.71\n",
      "2052 : loss:  0.6870608 \t acc:  0.69\n",
      "2054 : loss:  0.74559 \t acc:  0.59\n",
      "2056 : loss:  0.68319875 \t acc:  0.72\n",
      "2058 : loss:  0.73086005 \t acc:  0.71\n",
      "2060 : loss:  0.7276488 \t acc:  0.69\n",
      "2062 : loss:  0.583656 \t acc:  0.77\n",
      "2064 : loss:  0.81863266 \t acc:  0.67\n",
      "2066 : loss:  0.691734 \t acc:  0.65\n",
      "2068 : loss:  0.69175047 \t acc:  0.72\n",
      "2070 : loss:  0.7432733 \t acc:  0.66\n",
      "2072 : loss:  0.95340556 \t acc:  0.54\n",
      "2074 : loss:  0.81653666 \t acc:  0.64\n",
      "2076 : loss:  0.69446975 \t acc:  0.67\n",
      "2078 : loss:  0.6278468 \t acc:  0.71\n",
      "2080 : loss:  0.728862 \t acc:  0.66\n",
      "2082 : loss:  0.68394923 \t acc:  0.73\n",
      "2084 : loss:  0.83190036 \t acc:  0.59\n",
      "2086 : loss:  0.6722624 \t acc:  0.7\n",
      "2088 : loss:  0.7994592 \t acc:  0.63\n",
      "2090 : loss:  0.75360847 \t acc:  0.67\n",
      "2092 : loss:  0.76787895 \t acc:  0.65\n",
      "2094 : loss:  0.8041058 \t acc:  0.67\n",
      "2096 : loss:  0.63616055 \t acc:  0.74\n",
      "2098 : loss:  0.67968225 \t acc:  0.72\n",
      "2100 : loss:  0.74324805 \t acc:  0.64\n",
      "2102 : loss:  0.8040155 \t acc:  0.62\n",
      "2104 : loss:  0.71327376 \t acc:  0.65\n",
      "2106 : loss:  0.6909678 \t acc:  0.66\n",
      "2108 : loss:  0.68407583 \t acc:  0.67\n",
      "2110 : loss:  0.8082098 \t acc:  0.64\n",
      "2112 : loss:  0.8128379 \t acc:  0.61\n",
      "2114 : loss:  0.8465548 \t acc:  0.52\n",
      "2116 : loss:  0.79928374 \t acc:  0.6\n",
      "2118 : loss:  0.7786694 \t acc:  0.71\n",
      "2120 : loss:  0.7099114 \t acc:  0.75\n",
      "2122 : loss:  0.7958952 \t acc:  0.62\n",
      "2124 : loss:  0.70316213 \t acc:  0.67\n",
      "2126 : loss:  0.8256797 \t acc:  0.65\n",
      "2128 : loss:  0.7664765 \t acc:  0.69\n",
      "2130 : loss:  0.68674624 \t acc:  0.75\n",
      "2132 : loss:  0.9127598 \t acc:  0.54\n",
      "2134 : loss:  0.69355273 \t acc:  0.68\n",
      "2136 : loss:  0.83392423 \t acc:  0.65\n",
      "2138 : loss:  0.6703526 \t acc:  0.7\n",
      "2140 : loss:  0.79529357 \t acc:  0.7\n",
      "2142 : loss:  0.95644003 \t acc:  0.56\n",
      "2144 : loss:  0.78241974 \t acc:  0.65\n",
      "2146 : loss:  0.77412856 \t acc:  0.65\n",
      "2148 : loss:  0.6779237 \t acc:  0.76\n",
      "2150 : loss:  0.79640245 \t acc:  0.64\n",
      "2152 : loss:  0.7639096 \t acc:  0.7\n",
      "2154 : loss:  0.7187453 \t acc:  0.7\n",
      "2156 : loss:  0.6317576 \t acc:  0.69\n",
      "2158 : loss:  0.6829352 \t acc:  0.74\n",
      "2160 : loss:  0.7791765 \t acc:  0.68\n",
      "2162 : loss:  0.89220667 \t acc:  0.59\n",
      "2164 : loss:  0.76348585 \t acc:  0.69\n",
      "2166 : loss:  0.88002867 \t acc:  0.58\n",
      "2168 : loss:  0.78507483 \t acc:  0.61\n",
      "2170 : loss:  0.74480516 \t acc:  0.65\n",
      "2172 : loss:  0.74801725 \t acc:  0.66\n",
      "2174 : loss:  0.60248125 \t acc:  0.77\n",
      "2176 : loss:  0.7306859 \t acc:  0.66\n",
      "2178 : loss:  0.71547645 \t acc:  0.66\n",
      "2180 : loss:  0.74979705 \t acc:  0.62\n",
      "2182 : loss:  0.709859 \t acc:  0.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2184 : loss:  0.7370945 \t acc:  0.66\n",
      "2186 : loss:  0.7547663 \t acc:  0.63\n",
      "2188 : loss:  0.6966341 \t acc:  0.66\n",
      "2190 : loss:  0.72898424 \t acc:  0.73\n",
      "2192 : loss:  0.7864224 \t acc:  0.66\n",
      "2194 : loss:  0.60982317 \t acc:  0.75\n",
      "2196 : loss:  0.6543782 \t acc:  0.71\n",
      "2198 : loss:  0.7386315 \t acc:  0.67\n",
      "2200 : loss:  0.74702317 \t acc:  0.66\n",
      "2202 : loss:  0.742188 \t acc:  0.66\n",
      "2204 : loss:  0.69931823 \t acc:  0.65\n",
      "2206 : loss:  0.75949264 \t acc:  0.64\n",
      "2208 : loss:  0.7898488 \t acc:  0.62\n",
      "2210 : loss:  0.73352784 \t acc:  0.69\n",
      "2212 : loss:  0.83205825 \t acc:  0.61\n",
      "2214 : loss:  0.7905211 \t acc:  0.72\n",
      "2216 : loss:  0.932858 \t acc:  0.55\n",
      "2218 : loss:  0.8333247 \t acc:  0.62\n",
      "2220 : loss:  0.70071185 \t acc:  0.68\n",
      "2222 : loss:  0.85215485 \t acc:  0.54\n",
      "2224 : loss:  0.791717 \t acc:  0.6\n",
      "2226 : loss:  0.6907898 \t acc:  0.71\n",
      "2228 : loss:  0.68393755 \t acc:  0.7\n",
      "2230 : loss:  0.89882874 \t acc:  0.53\n",
      "\n",
      "Saving...\n",
      "saved to models/CNN/pretrained_model.ckpt-2232\n",
      "\n",
      "100 \t [67.32942506 64.81789344 53.62746825]\n",
      "5 \tval accuracy:  0.6269 \t f_! score:  [0.67329425 0.64817893 0.53627468]\n",
      "\n",
      "2232 : loss:  0.7281855 \t acc:  0.66\n",
      "2234 : loss:  0.6739232 \t acc:  0.75\n",
      "2236 : loss:  0.8685684 \t acc:  0.66\n",
      "2238 : loss:  0.6958297 \t acc:  0.68\n",
      "2240 : loss:  0.72950155 \t acc:  0.65\n",
      "2242 : loss:  0.7167475 \t acc:  0.64\n",
      "2244 : loss:  0.84982026 \t acc:  0.63\n",
      "2246 : loss:  0.71322703 \t acc:  0.68\n",
      "2248 : loss:  0.61584085 \t acc:  0.76\n",
      "2250 : loss:  0.610872 \t acc:  0.72\n",
      "2252 : loss:  0.6511849 \t acc:  0.73\n",
      "2254 : loss:  0.74653405 \t acc:  0.68\n",
      "2256 : loss:  0.68336034 \t acc:  0.75\n",
      "2258 : loss:  0.7280419 \t acc:  0.72\n",
      "2260 : loss:  0.7462862 \t acc:  0.67\n",
      "2262 : loss:  0.68674546 \t acc:  0.75\n",
      "2264 : loss:  0.7689199 \t acc:  0.65\n",
      "2266 : loss:  0.57610077 \t acc:  0.76\n",
      "2268 : loss:  0.89542174 \t acc:  0.55\n",
      "2270 : loss:  0.5849644 \t acc:  0.74\n",
      "2272 : loss:  0.8098668 \t acc:  0.6\n",
      "2274 : loss:  0.68329585 \t acc:  0.73\n",
      "2276 : loss:  0.8992054 \t acc:  0.62\n",
      "2278 : loss:  0.7148787 \t acc:  0.65\n",
      "2280 : loss:  0.7046242 \t acc:  0.7\n",
      "2282 : loss:  0.62023854 \t acc:  0.73\n",
      "2284 : loss:  0.6700505 \t acc:  0.73\n",
      "2286 : loss:  0.63516444 \t acc:  0.78\n",
      "2288 : loss:  0.73193985 \t acc:  0.62\n",
      "2290 : loss:  0.6175703 \t acc:  0.72\n",
      "2292 : loss:  0.6317551 \t acc:  0.71\n",
      "2294 : loss:  0.8403521 \t acc:  0.62\n",
      "2296 : loss:  0.7042187 \t acc:  0.75\n",
      "2298 : loss:  0.6439388 \t acc:  0.7\n",
      "2300 : loss:  0.6551705 \t acc:  0.72\n",
      "2302 : loss:  0.62063694 \t acc:  0.7\n",
      "2304 : loss:  0.6865363 \t acc:  0.66\n",
      "2306 : loss:  0.73209894 \t acc:  0.65\n",
      "2308 : loss:  0.69764596 \t acc:  0.67\n",
      "2310 : loss:  0.7586382 \t acc:  0.63\n",
      "2312 : loss:  0.82263213 \t acc:  0.6\n",
      "2314 : loss:  0.6327386 \t acc:  0.72\n",
      "2316 : loss:  0.64544034 \t acc:  0.72\n",
      "2318 : loss:  0.7022535 \t acc:  0.71\n",
      "2320 : loss:  0.706475 \t acc:  0.66\n",
      "2322 : loss:  0.6093158 \t acc:  0.76\n",
      "2324 : loss:  0.8151591 \t acc:  0.62\n",
      "2326 : loss:  0.72312844 \t acc:  0.65\n",
      "2328 : loss:  0.7553087 \t acc:  0.62\n",
      "2330 : loss:  0.65184516 \t acc:  0.72\n",
      "2332 : loss:  0.7708427 \t acc:  0.62\n",
      "2334 : loss:  0.98745555 \t acc:  0.54\n",
      "2336 : loss:  0.80549103 \t acc:  0.67\n",
      "2338 : loss:  0.6065679 \t acc:  0.78\n",
      "2340 : loss:  0.62643105 \t acc:  0.73\n",
      "2342 : loss:  0.6743188 \t acc:  0.67\n",
      "2344 : loss:  0.69345045 \t acc:  0.69\n",
      "2346 : loss:  0.80822486 \t acc:  0.65\n",
      "2348 : loss:  0.65685785 \t acc:  0.69\n",
      "2350 : loss:  0.6755211 \t acc:  0.72\n",
      "2352 : loss:  0.78343177 \t acc:  0.71\n",
      "2354 : loss:  0.8454068 \t acc:  0.59\n",
      "2356 : loss:  0.7899279 \t acc:  0.6\n",
      "2358 : loss:  0.8210333 \t acc:  0.6\n",
      "2360 : loss:  0.8175492 \t acc:  0.69\n",
      "2362 : loss:  1.0128856 \t acc:  0.53\n",
      "2364 : loss:  0.7646972 \t acc:  0.71\n",
      "2366 : loss:  0.68181646 \t acc:  0.71\n",
      "2368 : loss:  0.82494414 \t acc:  0.62\n",
      "2370 : loss:  0.7737585 \t acc:  0.62\n",
      "2372 : loss:  0.6978142 \t acc:  0.66\n",
      "2374 : loss:  0.73329383 \t acc:  0.57\n",
      "2376 : loss:  0.6991128 \t acc:  0.66\n",
      "2378 : loss:  0.76201636 \t acc:  0.67\n",
      "2380 : loss:  0.77189827 \t acc:  0.61\n",
      "2382 : loss:  0.70845443 \t acc:  0.66\n",
      "2384 : loss:  0.67622995 \t acc:  0.71\n",
      "2386 : loss:  0.5990792 \t acc:  0.78\n",
      "2388 : loss:  0.69448936 \t acc:  0.68\n",
      "2390 : loss:  0.8921517 \t acc:  0.51\n",
      "2392 : loss:  0.68086135 \t acc:  0.72\n",
      "2394 : loss:  0.7474496 \t acc:  0.65\n",
      "2396 : loss:  0.56435096 \t acc:  0.84\n",
      "2398 : loss:  0.6843155 \t acc:  0.72\n",
      "2400 : loss:  0.72538567 \t acc:  0.66\n",
      "2402 : loss:  0.92566675 \t acc:  0.55\n",
      "2404 : loss:  0.71416396 \t acc:  0.67\n",
      "2406 : loss:  0.65279037 \t acc:  0.75\n",
      "2408 : loss:  0.7412956 \t acc:  0.65\n",
      "2410 : loss:  0.774355 \t acc:  0.67\n",
      "2412 : loss:  0.6616415 \t acc:  0.71\n",
      "2414 : loss:  0.5638674 \t acc:  0.75\n",
      "2416 : loss:  0.69541585 \t acc:  0.66\n",
      "2418 : loss:  0.7650606 \t acc:  0.67\n",
      "2420 : loss:  0.6523058 \t acc:  0.74\n",
      "2422 : loss:  0.7469646 \t acc:  0.63\n",
      "2424 : loss:  0.749591 \t acc:  0.62\n",
      "2426 : loss:  0.8101358 \t acc:  0.65\n",
      "2428 : loss:  0.78291893 \t acc:  0.6\n",
      "2430 : loss:  0.67994416 \t acc:  0.68\n",
      "2432 : loss:  0.60795164 \t acc:  0.74\n",
      "2434 : loss:  0.7239579 \t acc:  0.64\n",
      "2436 : loss:  0.69059265 \t acc:  0.69\n",
      "2438 : loss:  0.67191064 \t acc:  0.73\n",
      "2440 : loss:  0.7621245 \t acc:  0.7\n",
      "2442 : loss:  0.68377036 \t acc:  0.71\n",
      "2444 : loss:  0.76068556 \t acc:  0.67\n",
      "2446 : loss:  0.76697004 \t acc:  0.73\n",
      "2448 : loss:  0.6993569 \t acc:  0.69\n",
      "2450 : loss:  0.6302986 \t acc:  0.7\n",
      "2452 : loss:  0.79166085 \t acc:  0.65\n",
      "2454 : loss:  0.8798922 \t acc:  0.61\n",
      "2456 : loss:  0.7485476 \t acc:  0.6\n",
      "2458 : loss:  0.7385666 \t acc:  0.66\n",
      "2460 : loss:  0.6421003 \t acc:  0.75\n",
      "2462 : loss:  0.75990766 \t acc:  0.59\n",
      "2464 : loss:  0.71824396 \t acc:  0.69\n",
      "2466 : loss:  0.8200489 \t acc:  0.6\n",
      "2468 : loss:  0.75975084 \t acc:  0.68\n",
      "2470 : loss:  0.70912766 \t acc:  0.65\n",
      "2472 : loss:  0.7126667 \t acc:  0.64\n",
      "2474 : loss:  0.6996856 \t acc:  0.72\n",
      "2476 : loss:  0.74723214 \t acc:  0.68\n",
      "2478 : loss:  0.69673955 \t acc:  0.72\n",
      "2480 : loss:  0.76942986 \t acc:  0.6\n",
      "2482 : loss:  0.6690516 \t acc:  0.67\n",
      "2484 : loss:  0.8171747 \t acc:  0.7\n",
      "2486 : loss:  0.61361 \t acc:  0.72\n",
      "2488 : loss:  0.72468674 \t acc:  0.71\n",
      "2490 : loss:  0.77297753 \t acc:  0.62\n",
      "2492 : loss:  0.57035196 \t acc:  0.77\n",
      "2494 : loss:  0.76150846 \t acc:  0.61\n",
      "2496 : loss:  0.71936166 \t acc:  0.69\n",
      "2498 : loss:  0.7026284 \t acc:  0.62\n",
      "2500 : loss:  0.62194103 \t acc:  0.75\n",
      "2502 : loss:  0.5841076 \t acc:  0.81\n",
      "2504 : loss:  0.699615 \t acc:  0.68\n",
      "2506 : loss:  0.7584405 \t acc:  0.67\n",
      "2508 : loss:  0.75434786 \t acc:  0.66\n",
      "2510 : loss:  0.71183497 \t acc:  0.69\n",
      "2512 : loss:  0.5920543 \t acc:  0.82\n",
      "2514 : loss:  0.6680213 \t acc:  0.69\n",
      "2516 : loss:  0.72113425 \t acc:  0.67\n",
      "2518 : loss:  0.76147765 \t acc:  0.68\n",
      "2520 : loss:  0.6806467 \t acc:  0.67\n",
      "2522 : loss:  0.6480478 \t acc:  0.7\n",
      "2524 : loss:  0.83039176 \t acc:  0.55\n",
      "2526 : loss:  0.73728216 \t acc:  0.66\n",
      "2528 : loss:  0.6626355 \t acc:  0.7\n",
      "2530 : loss:  0.6705336 \t acc:  0.7\n",
      "2532 : loss:  0.7765393 \t acc:  0.68\n",
      "2534 : loss:  0.7145367 \t acc:  0.75\n",
      "2536 : loss:  0.7367497 \t acc:  0.65\n",
      "2538 : loss:  0.64521414 \t acc:  0.72\n",
      "2540 : loss:  0.6961997 \t acc:  0.66\n",
      "2542 : loss:  0.6090645 \t acc:  0.77\n",
      "2544 : loss:  0.6919995 \t acc:  0.65\n",
      "2546 : loss:  0.58396775 \t acc:  0.77\n",
      "2548 : loss:  0.70925796 \t acc:  0.65\n",
      "2550 : loss:  0.7087208 \t acc:  0.71\n",
      "2552 : loss:  0.7069264 \t acc:  0.66\n",
      "2554 : loss:  0.7566768 \t acc:  0.67\n",
      "2556 : loss:  0.7655548 \t acc:  0.68\n",
      "2558 : loss:  0.7013029 \t acc:  0.65\n",
      "2560 : loss:  0.8111485 \t acc:  0.54\n",
      "2562 : loss:  0.77486306 \t acc:  0.71\n",
      "2564 : loss:  0.8367794 \t acc:  0.62\n",
      "2566 : loss:  0.7084659 \t acc:  0.72\n",
      "2568 : loss:  0.71764475 \t acc:  0.69\n",
      "2570 : loss:  0.80344343 \t acc:  0.63\n",
      "2572 : loss:  0.65221316 \t acc:  0.75\n",
      "2574 : loss:  0.6304282 \t acc:  0.71\n",
      "2576 : loss:  0.69716364 \t acc:  0.68\n",
      "2578 : loss:  0.7981142 \t acc:  0.6\n",
      "2580 : loss:  0.6974995 \t acc:  0.68\n",
      "2582 : loss:  0.7144743 \t acc:  0.66\n",
      "2584 : loss:  0.7076426 \t acc:  0.7\n",
      "2586 : loss:  0.7009749 \t acc:  0.66\n",
      "2588 : loss:  0.68703353 \t acc:  0.69\n",
      "2590 : loss:  0.68708766 \t acc:  0.67\n",
      "2592 : loss:  0.8301181 \t acc:  0.57\n",
      "2594 : loss:  0.7128599 \t acc:  0.64\n",
      "2596 : loss:  0.7419708 \t acc:  0.64\n",
      "2598 : loss:  0.7732644 \t acc:  0.66\n",
      "2600 : loss:  0.7652245 \t acc:  0.67\n",
      "2602 : loss:  0.66412616 \t acc:  0.69\n",
      "\n",
      "Saving...\n",
      "saved to models/CNN/pretrained_model.ckpt-2604\n",
      "\n",
      "100 \t [69.29576609 59.76477343 59.40858893]\n",
      "6 \tval accuracy:  0.6319 \t f_! score:  [0.69295766 0.59764773 0.59408589]\n",
      "\n",
      "100 \t [69.29576609 59.76477343 59.40858893]\n",
      "100 \t [69.2914519  62.15796491 58.11519717]\n",
      "100 \t [69.90524955 58.08147786 61.41556187]\n",
      "100 \t [3313. 3340. 3347.]\n",
      "--- Test   Twitter ---\n",
      "0.6319\n",
      "f1:  [0.69295766 0.59764773 0.59408589]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yannik/ba/TestHelper.py:120: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  input_data = self.decide_which_input(input_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509411\n",
      "Vorsicht! es muss vorher ein  CNN  Netz abgespeichert worden sein\n",
      "\n",
      "Restoring...\n",
      "models/CNN/pretrained_model.ckpt-2604\n",
      "INFO:tensorflow:Restoring parameters from models/CNN/pretrained_model.ckpt-2604\n",
      "acc:  0.22\n",
      "acc:  0.28\n",
      "acc:  0.32\n",
      "acc:  0.22\n",
      "acc:  0.31\n",
      "acc:  0.31\n",
      "acc:  0.19\n",
      "acc:  0.27\n",
      "acc:  0.35\n",
      "acc:  0.22\n",
      "acc:  0.25\n",
      "acc:  0.26\n",
      "acc:  0.25\n",
      "acc:  0.23\n",
      "acc:  0.24\n",
      "acc:  0.23\n",
      "acc:  0.31\n",
      "acc:  0.27\n",
      "acc:  0.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yannik/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.22\n",
      "acc:  0.31\n",
      "acc:  0.25\n",
      "acc:  0.25\n",
      "acc:  0.26\n",
      "acc:  0.29\n",
      "acc:  0.27\n",
      "acc:  0.3\n",
      "acc:  0.28\n",
      "acc:  0.3\n",
      "acc:  0.19\n",
      "acc:  0.3\n",
      "acc:  0.29\n",
      "acc:  0.26\n",
      "acc:  0.34\n",
      "acc:  0.24\n",
      "acc:  0.37\n",
      "acc:  0.26\n",
      "acc:  0.21\n",
      "acc:  0.27\n",
      "acc:  0.33\n",
      "acc:  0.23\n",
      "acc:  0.21\n",
      "acc:  0.32\n",
      "acc:  0.23\n",
      "acc:  0.26\n",
      "acc:  0.29\n",
      "acc:  0.27\n",
      "acc:  0.22\n",
      "acc:  0.31\n",
      "acc:  0.28\n",
      "acc:  0.21\n",
      "acc:  0.34\n",
      "acc:  0.36\n",
      "acc:  0.29\n",
      "acc:  0.28\n",
      "acc:  0.37\n",
      "acc:  0.38\n",
      "acc:  0.31\n",
      "acc:  0.31\n",
      "acc:  0.25\n",
      "acc:  0.25\n",
      "acc:  0.23\n",
      "acc:  0.26\n",
      "acc:  0.28\n",
      "acc:  0.35\n",
      "acc:  0.2\n",
      "acc:  0.19\n",
      "acc:  0.26\n",
      "acc:  0.26\n",
      "acc:  0.28\n",
      "acc:  0.35\n",
      "acc:  0.3\n",
      "acc:  0.26\n",
      "acc:  0.26\n",
      "acc:  0.28\n",
      "acc:  0.22\n",
      "acc:  0.24\n",
      "acc:  0.29\n",
      "acc:  0.36\n",
      "acc:  0.18\n",
      "acc:  0.32\n",
      "acc:  0.31\n",
      "acc:  0.32\n",
      "acc:  0.22\n",
      "acc:  0.27\n",
      "acc:  0.26\n",
      "acc:  0.21\n",
      "acc:  0.36\n",
      "acc:  0.31\n",
      "acc:  0.27\n",
      "acc:  0.36\n",
      "acc:  0.33\n",
      "acc:  0.29\n",
      "acc:  0.28\n",
      "acc:  0.27\n",
      "acc:  0.29\n",
      "acc:  0.24\n",
      "acc:  0.28\n",
      "acc:  0.31\n",
      "acc:  0.34\n",
      "acc:  0.19\n",
      "acc:  0.29\n",
      "acc:  0.3\n",
      "acc:  0.32\n",
      "acc:  0.35\n",
      "acc:  0.19\n",
      "acc:  0.33\n",
      "acc:  0.28\n",
      "acc:  0.36\n",
      "acc:  0.26\n",
      "acc:  0.19\n",
      "acc:  0.25\n",
      "acc:  0.26\n",
      "acc:  0.3\n",
      "acc:  0.28\n",
      "acc:  0.29\n",
      "acc:  0.34\n",
      "acc:  0.27\n",
      "acc:  0.25\n",
      "acc:  0.34\n",
      "acc:  0.25\n",
      "acc:  0.23\n",
      "acc:  0.27\n",
      "acc:  0.29\n",
      "acc:  0.27\n",
      "acc:  0.32\n",
      "acc:  0.22\n",
      "acc:  0.27\n",
      "acc:  0.26\n",
      "acc:  0.2\n",
      "acc:  0.27\n",
      "acc:  0.27\n",
      "acc:  0.29\n",
      "acc:  0.39\n",
      "acc:  0.27\n",
      "acc:  0.26\n",
      "acc:  0.28\n",
      "acc:  0.27\n",
      "acc:  0.38\n",
      "acc:  0.27\n",
      "acc:  0.29\n",
      "acc:  0.3\n",
      "acc:  0.27\n",
      "acc:  0.32\n",
      "acc:  0.23\n",
      "acc:  0.25\n",
      "acc:  0.22\n",
      "acc:  0.25\n",
      "acc:  0.24\n",
      "acc:  0.26\n",
      "acc:  0.29\n",
      "acc:  0.22\n",
      "acc:  0.22\n",
      "acc:  0.25\n",
      "acc:  0.27\n",
      "acc:  0.27\n",
      "acc:  0.32\n",
      "acc:  0.3\n",
      "acc:  0.3\n",
      "acc:  0.26\n",
      "acc:  0.34\n",
      "acc:  0.3\n",
      "acc:  0.31\n",
      "acc:  0.26\n",
      "acc:  0.32\n",
      "acc:  0.25\n",
      "acc:  0.32\n",
      "acc:  0.21\n",
      "acc:  0.23\n",
      "acc:  0.25\n",
      "acc:  0.3\n",
      "acc:  0.24\n",
      "acc:  0.21\n",
      "acc:  0.26\n",
      "acc:  0.29\n",
      "acc:  0.26\n",
      "acc:  0.28\n",
      "acc:  0.21\n",
      "acc:  0.25\n",
      "acc:  0.25\n",
      "acc:  0.31\n",
      "acc:  0.26\n",
      "acc:  0.2\n",
      "acc:  0.23\n",
      "acc:  0.28\n",
      "acc:  0.31\n",
      "acc:  0.36\n",
      "acc:  0.28\n",
      "acc:  0.29\n",
      "acc:  0.24\n",
      "acc:  0.2\n",
      "acc:  0.24\n",
      "acc:  0.26\n",
      "acc:  0.29\n",
      "acc:  0.32\n",
      "acc:  0.26\n",
      "acc:  0.31\n",
      "acc:  0.34\n",
      "acc:  0.29\n",
      "acc:  0.24\n",
      "acc:  0.3\n",
      "acc:  0.25\n",
      "acc:  0.25\n",
      "acc:  0.27\n",
      "acc:  0.22\n",
      "acc:  0.25\n",
      "acc:  0.27\n",
      "acc:  0.24\n",
      "acc:  0.23\n",
      "acc:  0.29\n",
      "acc:  0.27\n",
      "acc:  0.35\n",
      "acc:  0.28\n",
      "acc:  0.2\n",
      "acc:  0.26\n",
      "acc:  0.25\n",
      "acc:  0.29\n",
      "acc:  0.3\n",
      "acc:  0.32\n",
      "acc:  0.39\n",
      "acc:  0.34\n",
      "acc:  0.27\n",
      "acc:  0.22\n",
      "acc:  0.33\n",
      "acc:  0.26\n",
      "acc:  0.32\n",
      "acc:  0.31\n",
      "acc:  0.28\n",
      "acc:  0.28\n",
      "acc:  0.24\n",
      "acc:  0.35\n",
      "acc:  0.32\n",
      "acc:  0.3\n",
      "acc:  0.24\n",
      "acc:  0.27\n",
      "acc:  0.19\n",
      "acc:  0.27\n",
      "acc:  0.35\n",
      "acc:  0.25\n",
      "acc:  0.31\n",
      "acc:  0.29\n",
      "acc:  0.24\n",
      "acc:  0.31\n",
      "acc:  0.37\n",
      "acc:  0.23\n",
      "acc:  0.25\n",
      "acc:  0.3\n",
      "acc:  0.26\n",
      "acc:  0.25\n",
      "acc:  0.31\n",
      "acc:  0.24\n",
      "acc:  0.23\n",
      "acc:  0.32\n",
      "acc:  0.31\n",
      "acc:  0.22\n",
      "acc:  0.32\n",
      "acc:  0.29\n",
      "acc:  0.31\n",
      "acc:  0.27\n",
      "acc:  0.3\n",
      "acc:  0.21\n",
      "acc:  0.19\n",
      "acc:  0.32\n",
      "acc:  0.33\n",
      "acc:  0.22\n",
      "acc:  0.37\n",
      "acc:  0.24\n",
      "acc:  0.35\n",
      "acc:  0.3\n",
      "acc:  0.26\n",
      "acc:  0.25\n",
      "acc:  0.27\n",
      "acc:  0.26\n",
      "acc:  0.27\n",
      "acc:  0.3\n",
      "acc:  0.35\n",
      "acc:  0.34\n",
      "acc:  0.24\n",
      "acc:  0.24\n",
      "acc:  0.23\n",
      "acc:  0.22\n",
      "acc:  0.34\n",
      "acc:  0.2\n",
      "acc:  0.26\n",
      "acc:  0.26\n",
      "acc:  0.37\n",
      "acc:  0.28\n",
      "acc:  0.34\n",
      "acc:  0.18\n",
      "acc:  0.26\n",
      "acc:  0.37\n",
      "acc:  0.23\n",
      "acc:  0.29\n",
      "acc:  0.24\n",
      "acc:  0.4\n",
      "acc:  0.27\n",
      "acc:  0.29\n",
      "acc:  0.24\n",
      "acc:  0.27\n",
      "acc:  0.33\n",
      "acc:  0.38\n",
      "acc:  0.27\n",
      "acc:  0.31\n",
      "acc:  0.27\n",
      "acc:  0.32\n",
      "acc:  0.22\n",
      "acc:  0.27\n",
      "acc:  0.27\n",
      "acc:  0.23\n",
      "acc:  0.26\n",
      "acc:  0.28\n",
      "acc:  0.16\n",
      "acc:  0.25\n",
      "acc:  0.26\n",
      "acc:  0.28\n",
      "acc:  0.25\n",
      "acc:  0.24\n",
      "acc:  0.32\n",
      "acc:  0.3\n",
      "acc:  0.26\n",
      "acc:  0.19\n",
      "acc:  0.24\n",
      "acc:  0.23\n",
      "acc:  0.3\n",
      "acc:  0.28\n",
      "acc:  0.24\n",
      "acc:  0.3\n",
      "acc:  0.26\n",
      "acc:  0.25\n",
      "acc:  0.3\n",
      "acc:  0.27\n",
      "acc:  0.31\n",
      "acc:  0.3\n",
      "acc:  0.23\n",
      "acc:  0.27\n",
      "acc:  0.26\n",
      "acc:  0.36\n",
      "acc:  0.31\n",
      "acc:  0.21\n",
      "acc:  0.29\n",
      "acc:  0.3\n",
      "acc:  0.27\n",
      "acc:  0.28\n",
      "acc:  0.32\n",
      "acc:  0.29\n",
      "acc:  0.33\n",
      "acc:  0.29\n",
      "acc:  0.31\n",
      "acc:  0.29\n",
      "acc:  0.27\n",
      "acc:  0.27\n",
      "acc:  0.24\n",
      "acc:  0.25\n",
      "acc:  0.28\n",
      "acc:  0.28\n",
      "acc:  0.24\n",
      "acc:  0.33\n",
      "acc:  0.32\n",
      "acc:  0.34\n",
      "acc:  0.29\n",
      "acc:  0.27\n",
      "acc:  0.31\n",
      "acc:  0.36\n",
      "acc:  0.28\n",
      "acc:  0.25\n",
      "acc:  0.27\n",
      "acc:  0.21\n",
      "acc:  0.28\n",
      "acc:  0.23\n",
      "acc:  0.28\n",
      "acc:  0.25\n",
      "acc:  0.27\n",
      "acc:  0.24\n",
      "acc:  0.29\n",
      "acc:  0.28\n",
      "acc:  0.33\n",
      "acc:  0.23\n",
      "acc:  0.26\n",
      "acc:  0.29\n",
      "acc:  0.26\n",
      "acc:  0.3\n",
      "acc:  0.27\n",
      "acc:  0.28\n",
      "acc:  0.27\n",
      "acc:  0.28\n",
      "acc:  0.33\n",
      "acc:  0.27\n",
      "acc:  0.29\n",
      "acc:  0.25\n",
      "acc:  0.26\n",
      "acc:  0.36\n",
      "acc:  0.27\n",
      "acc:  0.32\n",
      "acc:  0.36\n",
      "acc:  0.35\n",
      "acc:  0.27\n",
      "acc:  0.27\n",
      "acc:  0.27\n",
      "acc:  0.34\n",
      "acc:  0.33\n",
      "acc:  0.2\n",
      "acc:  0.26\n",
      "acc:  0.27\n",
      "acc:  0.25\n",
      "acc:  0.31\n",
      "acc:  0.29\n",
      "acc:  0.28\n",
      "acc:  0.31\n",
      "acc:  0.29\n",
      "acc:  0.32\n",
      "acc:  0.18\n",
      "acc:  0.36\n",
      "acc:  0.32\n",
      "acc:  0.27\n",
      "acc:  0.29\n",
      "acc:  0.27\n",
      "acc:  0.27\n",
      "acc:  0.24\n",
      "acc:  0.25\n",
      "acc:  0.35\n",
      "acc:  0.29\n",
      "acc:  0.18\n",
      "acc:  0.24\n",
      "acc:  0.3\n",
      "acc:  0.31\n",
      "acc:  0.17\n",
      "acc:  0.3\n",
      "acc:  0.31\n",
      "acc:  0.23\n",
      "acc:  0.29\n",
      "acc:  0.29\n",
      "acc:  0.24\n",
      "acc:  0.22\n",
      "acc:  0.29\n",
      "acc:  0.39\n",
      "acc:  0.31\n",
      "acc:  0.26\n",
      "acc:  0.28\n",
      "acc:  0.22\n",
      "acc:  0.19\n",
      "acc:  0.27\n",
      "acc:  0.31\n",
      "acc:  0.27\n",
      "acc:  0.21\n",
      "acc:  0.23\n",
      "acc:  0.21\n",
      "acc:  0.25\n",
      "acc:  0.32\n",
      "acc:  0.33\n",
      "acc:  0.24\n",
      "acc:  0.26\n",
      "acc:  0.33\n",
      "acc:  0.25\n",
      "acc:  0.26\n",
      "acc:  0.3\n",
      "acc:  0.32\n",
      "acc:  0.28\n",
      "acc:  0.32\n",
      "acc:  0.24\n",
      "acc:  0.27\n",
      "acc:  0.26\n",
      "acc:  0.31\n",
      "acc:  0.28\n",
      "acc:  0.29\n",
      "acc:  0.28\n",
      "acc:  0.32\n",
      "acc:  0.31\n",
      "acc:  0.29\n",
      "acc:  0.28\n",
      "acc:  0.25\n",
      "acc:  0.28\n",
      "acc:  0.28\n",
      "acc:  0.32\n",
      "acc:  0.24\n",
      "acc:  0.37\n",
      "acc:  0.29\n",
      "acc:  0.22\n",
      "acc:  0.29\n",
      "acc:  0.27\n",
      "acc:  0.3\n",
      "acc:  0.28\n",
      "acc:  0.23\n",
      "acc:  0.2\n",
      "acc:  0.28\n",
      "acc:  0.27\n",
      "acc:  0.21\n",
      "acc:  0.27\n",
      "acc:  0.33\n",
      "acc:  0.35\n",
      "acc:  0.18\n",
      "acc:  0.25\n",
      "acc:  0.29\n",
      "acc:  0.27\n",
      "acc:  0.26\n",
      "acc:  0.24\n",
      "acc:  0.26\n",
      "acc:  0.26\n",
      "acc:  0.23\n",
      "acc:  0.27\n",
      "acc:  0.3\n",
      "\n",
      "500 \t [  0.         215.67320262   0.        ]\n",
      "val accuracy:  0.27646 \t f_! score:  [0.         0.43134641 0.        ]\n",
      "\n",
      "500 \t [  0.         215.67320262   0.        ]\n",
      "500 \t [  0.   138.23   0.  ]\n",
      "500 \t [  0. 500.   0.]\n",
      "500 \t [18794. 13823. 17383.]\n",
      "---just Test  Review ---\n",
      "0.27646\n",
      "f1:  [0.         0.43134641 0.        ]\n",
      "(7733, 3)\n",
      "7733\n",
      "Vorsicht! es muss vorher ein  CNN  Netz abgespeichert worden sein\n",
      "\n",
      "Restoring...\n",
      "models/CNN/pretrained_model.ckpt-2604\n",
      "INFO:tensorflow:Restoring parameters from models/CNN/pretrained_model.ckpt-2604\n",
      "acc:  0.04\n",
      "acc:  0.14\n",
      "acc:  0.15\n",
      "acc:  0.04\n",
      "acc:  0.1\n",
      "acc:  0.08\n",
      "acc:  0.06\n",
      "acc:  0.1\n",
      "acc:  0.04\n",
      "acc:  0.1\n",
      "acc:  0.09\n",
      "acc:  0.03\n",
      "acc:  0.11\n",
      "acc:  0.13\n",
      "\n",
      "14 \t [1.13661236 2.2094485  0.        ]\n",
      "val accuracy:  0.086428575 \t f_! score:  [0.0811866  0.15781775 0.        ]\n",
      "\n",
      "14 \t [1.13661236 2.2094485  0.        ]\n",
      "14 \t [1.07601467 1.22285668 0.        ]\n",
      "14 \t [ 1.68644689 13.26818182  0.        ]\n",
      "14 \t [ 105.  116. 1179.]\n",
      "---just Test  Medical ---\n",
      "0.086428575\n",
      "f1:  [0.0811866  0.15781775 0.        ]\n",
      "(37126,)\n",
      "(37126,)\n",
      "37126\n",
      "Vorsicht! es muss vorher ein  CNN  Netz abgespeichert worden sein\n",
      "\n",
      "Restoring...\n",
      "models/CNN/pretrained_model.ckpt-2604\n",
      "INFO:tensorflow:Restoring parameters from models/CNN/pretrained_model.ckpt-2604\n",
      "acc:  0.08\n",
      "acc:  0.02\n",
      "acc:  0.1\n",
      "acc:  0.09\n",
      "acc:  0.13\n",
      "acc:  0.08\n",
      "acc:  0.0\n",
      "acc:  0.14\n",
      "acc:  0.03\n",
      "acc:  0.03\n",
      "acc:  0.17\n",
      "acc:  0.08\n",
      "acc:  0.06\n",
      "acc:  0.03\n",
      "acc:  0.01\n",
      "acc:  0.07\n",
      "acc:  0.15\n",
      "acc:  0.08\n",
      "acc:  0.05\n",
      "acc:  0.09\n",
      "acc:  0.22\n",
      "acc:  0.1\n",
      "acc:  0.08\n",
      "acc:  0.05\n",
      "acc:  0.06\n",
      "acc:  0.15\n",
      "acc:  0.06\n",
      "acc:  0.02\n",
      "acc:  0.06\n",
      "acc:  0.07\n",
      "acc:  0.12\n",
      "acc:  0.06\n",
      "acc:  0.04\n",
      "acc:  0.02\n",
      "acc:  0.02\n",
      "acc:  0.01\n",
      "acc:  0.06\n",
      "acc:  0.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yannik/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.09\n",
      "acc:  0.06\n",
      "acc:  0.05\n",
      "acc:  0.03\n",
      "acc:  0.04\n",
      "acc:  0.05\n",
      "acc:  0.09\n",
      "acc:  0.05\n",
      "acc:  0.05\n",
      "acc:  0.1\n",
      "acc:  0.03\n",
      "acc:  0.1\n",
      "acc:  0.12\n",
      "acc:  0.0\n",
      "acc:  0.08\n",
      "acc:  0.09\n",
      "acc:  0.1\n",
      "acc:  0.09\n",
      "acc:  0.11\n",
      "acc:  0.08\n",
      "acc:  0.1\n",
      "acc:  0.06\n",
      "acc:  0.09\n",
      "acc:  0.05\n",
      "acc:  0.1\n",
      "acc:  0.13\n",
      "acc:  0.07\n",
      "acc:  0.04\n",
      "acc:  0.11\n",
      "acc:  0.01\n",
      "acc:  0.02\n",
      "acc:  0.18\n",
      "\n",
      "70 \t [8.6577448  8.40545293 0.06884431]\n",
      "val accuracy:  0.07428571 \t f_! score:  [0.12368207 0.1200779  0.00098349]\n",
      "\n",
      "70 \t [8.6577448  8.40545293 0.06884431]\n",
      "70 \t [5.09993247 5.414872   3.        ]\n",
      "70 \t [3.89524846e+01 2.50490724e+01 3.48225199e-02]\n",
      "70 \t [ 501.  664. 5835.]\n",
      "---just Test  Prime ---\n",
      "0.07428571\n",
      "f1:  [0.12368207 0.1200779  0.00098349]\n"
     ]
    }
   ],
   "source": [
    "tw_loss, tw_acc = testhelper.train_input(\"Twitter\", net_name)\n",
    "\n",
    "testhelper.just_test(\"Review\", net_name)\n",
    "testhelper.just_test(\"Medical\", net_name)\n",
    "testhelper.just_test(\"Prime\", net_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  Medical\n",
      "delete old models\n",
      "(7733, 3)\n",
      "7733\n",
      "Tensor(\"ConvNet/dense/BiasAdd:0\", shape=(?, 3), dtype=float32)\n",
      "Tensor(\"ConvNet_1/dense/BiasAdd:0\", shape=(?, 3), dtype=float32)\n",
      "---init ready   CNN ---\n",
      "\n",
      "Saving...\n",
      "saved to models/CNN/pretrained_model.ckpt-0\n",
      "0 : loss:  1.5707161 \t acc:  0.79\n",
      "2 : loss:  1.0684996 \t acc:  0.39\n",
      "4 : loss:  1.0041944 \t acc:  0.62\n",
      "6 : loss:  1.2366259 \t acc:  0.44\n",
      "8 : loss:  0.7501376 \t acc:  0.76\n",
      "10 : loss:  0.7884842 \t acc:  0.71\n",
      "12 : loss:  1.1635433 \t acc:  0.38\n",
      "14 : loss:  0.9397733 \t acc:  0.6\n",
      "16 : loss:  0.7315557 \t acc:  0.76\n",
      "18 : loss:  0.5689894 \t acc:  0.88\n",
      "20 : loss:  0.8658363 \t acc:  0.7\n",
      "22 : loss:  0.5941899 \t acc:  0.83\n",
      "24 : loss:  0.7741238 \t acc:  0.74\n",
      "26 : loss:  0.8339624 \t acc:  0.72\n",
      "28 : loss:  1.0090278 \t acc:  0.51\n",
      "30 : loss:  0.9898223 \t acc:  0.61\n",
      "32 : loss:  0.8953754 \t acc:  0.7\n",
      "34 : loss:  0.7769976 \t acc:  0.81\n",
      "36 : loss:  0.95916456 \t acc:  0.6\n",
      "38 : loss:  0.65417206 \t acc:  0.79\n",
      "40 : loss:  1.1224198 \t acc:  0.56\n",
      "\n",
      "Saving...\n",
      "saved to models/CNN/pretrained_model.ckpt-42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yannik/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14 \t [ 0.          0.         12.77481666]\n",
      "0 \tval accuracy:  0.84214294 \t f_! score:  [0.        0.        0.9124869]\n",
      "\n",
      "42 : loss:  0.7013182 \t acc:  0.79\n",
      "44 : loss:  0.71115446 \t acc:  0.92\n",
      "46 : loss:  0.9110034 \t acc:  0.68\n",
      "48 : loss:  0.86700356 \t acc:  0.79\n",
      "50 : loss:  0.92018396 \t acc:  0.69\n",
      "52 : loss:  0.844819 \t acc:  0.79\n",
      "54 : loss:  0.7920409 \t acc:  0.83\n",
      "56 : loss:  1.1784182 \t acc:  0.37\n",
      "58 : loss:  0.99636614 \t acc:  0.6\n",
      "60 : loss:  0.56200254 \t acc:  0.88\n",
      "62 : loss:  0.96407354 \t acc:  0.56\n",
      "64 : loss:  0.88683426 \t acc:  0.67\n",
      "66 : loss:  0.8346969 \t acc:  0.7\n",
      "68 : loss:  1.1186141 \t acc:  0.44\n",
      "70 : loss:  0.79361564 \t acc:  0.74\n",
      "72 : loss:  0.938145 \t acc:  0.62\n",
      "74 : loss:  1.3861475 \t acc:  0.38\n",
      "76 : loss:  0.65544134 \t acc:  0.82\n",
      "78 : loss:  1.0198073 \t acc:  0.61\n",
      "80 : loss:  1.0015378 \t acc:  0.65\n",
      "82 : loss:  0.8498003 \t acc:  0.7\n",
      "\n",
      "Saving...\n",
      "saved to models/CNN/pretrained_model.ckpt-84\n",
      "\n",
      "14 \t [ 0.          0.         12.77481666]\n",
      "1 \tval accuracy:  0.84214294 \t f_! score:  [0.        0.        0.9124869]\n",
      "\n",
      "84 : loss:  0.76817805 \t acc:  0.77\n",
      "86 : loss:  0.8150694 \t acc:  0.79\n",
      "88 : loss:  0.9404536 \t acc:  0.64\n",
      "90 : loss:  0.87641275 \t acc:  0.72\n",
      "92 : loss:  0.7921856 \t acc:  0.79\n",
      "94 : loss:  0.82846457 \t acc:  0.7\n",
      "96 : loss:  1.1892601 \t acc:  0.4\n",
      "98 : loss:  0.8201254 \t acc:  0.71\n",
      "100 : loss:  0.52641946 \t acc:  0.85\n",
      "102 : loss:  0.94690675 \t acc:  0.61\n",
      "104 : loss:  0.89465964 \t acc:  0.64\n",
      "106 : loss:  0.92657954 \t acc:  0.62\n",
      "108 : loss:  0.7291303 \t acc:  0.88\n",
      "110 : loss:  0.7559398 \t acc:  0.81\n",
      "112 : loss:  0.73947984 \t acc:  0.79\n",
      "114 : loss:  1.0264688 \t acc:  0.52\n",
      "116 : loss:  0.83673114 \t acc:  0.7\n",
      "118 : loss:  1.0160794 \t acc:  0.56\n",
      "120 : loss:  0.8133769 \t acc:  0.7\n",
      "122 : loss:  0.78731567 \t acc:  0.71\n",
      "124 : loss:  0.8605317 \t acc:  0.67\n",
      "\n",
      "Saving...\n",
      "saved to models/CNN/pretrained_model.ckpt-126\n",
      "\n",
      "14 \t [ 0.          0.         12.77481666]\n",
      "2 \tval accuracy:  0.84214294 \t f_! score:  [0.        0.        0.9124869]\n",
      "\n",
      "126 : loss:  0.81909233 \t acc:  0.7\n",
      "128 : loss:  1.1515114 \t acc:  0.4\n",
      "130 : loss:  0.7865802 \t acc:  0.74\n",
      "132 : loss:  0.7788682 \t acc:  0.76\n",
      "134 : loss:  0.8395662 \t acc:  0.69\n",
      "136 : loss:  0.86956614 \t acc:  0.66\n",
      "138 : loss:  0.8125908 \t acc:  0.7\n",
      "140 : loss:  0.8311341 \t acc:  0.72\n",
      "142 : loss:  0.5376378 \t acc:  0.86\n",
      "144 : loss:  1.056034 \t acc:  0.58\n",
      "146 : loss:  1.1469244 \t acc:  0.52\n",
      "148 : loss:  0.86802053 \t acc:  0.67\n",
      "150 : loss:  1.1677611 \t acc:  0.44\n",
      "152 : loss:  1.037831 \t acc:  0.5\n",
      "154 : loss:  1.1832187 \t acc:  0.37\n",
      "156 : loss:  0.9596914 \t acc:  0.6\n",
      "158 : loss:  0.9245456 \t acc:  0.61\n",
      "160 : loss:  0.5743501 \t acc:  0.92\n",
      "162 : loss:  0.6778622 \t acc:  0.81\n",
      "164 : loss:  1.0846788 \t acc:  0.48\n",
      "166 : loss:  0.73756397 \t acc:  0.75757575\n",
      "\n",
      "Saving...\n",
      "saved to models/CNN/pretrained_model.ckpt-168\n",
      "\n",
      "14 \t [ 0.          0.         12.77481666]\n",
      "3 \tval accuracy:  0.84214294 \t f_! score:  [0.        0.        0.9124869]\n",
      "\n",
      "168 : loss:  0.67080194 \t acc:  0.8\n",
      "170 : loss:  0.5579126 \t acc:  0.86\n",
      "172 : loss:  0.8659592 \t acc:  0.65\n",
      "174 : loss:  0.8030893 \t acc:  0.71\n",
      "176 : loss:  0.8895094 \t acc:  0.64\n",
      "178 : loss:  1.0618552 \t acc:  0.48\n",
      "180 : loss:  0.8691795 \t acc:  0.67\n",
      "182 : loss:  0.80769175 \t acc:  0.7\n",
      "184 : loss:  0.6052271 \t acc:  0.83\n",
      "186 : loss:  1.3176816 \t acc:  0.37\n",
      "188 : loss:  0.68650246 \t acc:  0.82\n",
      "190 : loss:  0.7580942 \t acc:  0.76\n",
      "192 : loss:  0.7226516 \t acc:  0.79\n",
      "194 : loss:  0.8661565 \t acc:  0.7\n",
      "196 : loss:  0.79543626 \t acc:  0.74\n",
      "198 : loss:  0.778999 \t acc:  0.75757575\n",
      "200 : loss:  0.96051985 \t acc:  0.57\n",
      "202 : loss:  1.0597149 \t acc:  0.52\n",
      "204 : loss:  0.95947695 \t acc:  0.61\n",
      "206 : loss:  1.281362 \t acc:  0.4\n",
      "208 : loss:  0.5382998 \t acc:  0.85\n",
      "\n",
      "Saving...\n",
      "saved to models/CNN/pretrained_model.ckpt-210\n",
      "\n",
      "14 \t [ 0.          0.         12.77481666]\n",
      "4 \tval accuracy:  0.84214294 \t f_! score:  [0.        0.        0.9124869]\n",
      "\n",
      "210 : loss:  0.7704334 \t acc:  0.71\n",
      "212 : loss:  0.73258704 \t acc:  0.76\n",
      "214 : loss:  0.67363966 \t acc:  0.79\n",
      "216 : loss:  0.5877626 \t acc:  0.86\n",
      "218 : loss:  0.83486503 \t acc:  0.69\n",
      "220 : loss:  0.97159004 \t acc:  0.57\n",
      "222 : loss:  0.6048503 \t acc:  0.88\n",
      "224 : loss:  0.59122545 \t acc:  0.85\n",
      "226 : loss:  0.86553663 \t acc:  0.67\n",
      "228 : loss:  0.79452443 \t acc:  0.7\n",
      "230 : loss:  1.1555874 \t acc:  0.44\n",
      "232 : loss:  0.67801017 \t acc:  0.79\n",
      "234 : loss:  0.9888513 \t acc:  0.58\n",
      "236 : loss:  0.727039 \t acc:  0.82\n",
      "238 : loss:  0.90687835 \t acc:  0.59\n",
      "240 : loss:  1.0552291 \t acc:  0.52\n",
      "242 : loss:  0.7895148 \t acc:  0.72\n",
      "244 : loss:  0.7007515 \t acc:  0.79\n",
      "246 : loss:  0.888005 \t acc:  0.65\n",
      "248 : loss:  0.5679345 \t acc:  0.83\n",
      "250 : loss:  0.98567384 \t acc:  0.61\n",
      "\n",
      "Saving...\n",
      "saved to models/CNN/pretrained_model.ckpt-252\n",
      "\n",
      "14 \t [ 0.          0.         12.77481666]\n",
      "5 \tval accuracy:  0.84214294 \t f_! score:  [0.        0.        0.9124869]\n",
      "\n",
      "252 : loss:  0.761232 \t acc:  0.76\n",
      "254 : loss:  0.79984534 \t acc:  0.74\n",
      "256 : loss:  0.7997954 \t acc:  0.71\n",
      "258 : loss:  0.67443204 \t acc:  0.83\n",
      "260 : loss:  0.873905 \t acc:  0.65\n",
      "262 : loss:  0.75805587 \t acc:  0.74\n",
      "264 : loss:  0.7051781 \t acc:  0.77\n",
      "266 : loss:  1.2872702 \t acc:  0.37\n",
      "268 : loss:  0.98226464 \t acc:  0.6\n",
      "270 : loss:  1.1203089 \t acc:  0.46\n",
      "272 : loss:  0.85464823 \t acc:  0.7\n",
      "274 : loss:  0.9517755 \t acc:  0.61\n",
      "276 : loss:  0.80542123 \t acc:  0.71\n",
      "278 : loss:  0.70718735 \t acc:  0.79\n",
      "280 : loss:  0.76557744 \t acc:  0.75757575\n",
      "282 : loss:  0.5591845 \t acc:  0.85\n",
      "284 : loss:  0.76185095 \t acc:  0.74\n",
      "286 : loss:  0.5984 \t acc:  0.83\n",
      "288 : loss:  0.98340535 \t acc:  0.55\n",
      "290 : loss:  0.8770442 \t acc:  0.67\n",
      "292 : loss:  0.8479321 \t acc:  0.66\n",
      "\n",
      "Saving...\n",
      "saved to models/CNN/pretrained_model.ckpt-294\n",
      "\n",
      "14 \t [ 0.          0.         12.77481666]\n",
      "6 \tval accuracy:  0.84214294 \t f_! score:  [0.        0.        0.9124869]\n",
      "\n",
      "14 \t [ 0.          0.         12.77481666]\n",
      "14 \t [ 0.    0.   11.79]\n",
      "14 \t [ 0.  0. 14.]\n",
      "14 \t [ 105.  116. 1179.]\n",
      "--- Test   Medical ---\n",
      "0.84214294\n",
      "f1:  [0.        0.        0.9124869]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yannik/ba/TestHelper.py:120: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  input_data = self.decide_which_input(input_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509411\n",
      "Vorsicht! es muss vorher ein  CNN  Netz abgespeichert worden sein\n",
      "\n",
      "Restoring...\n",
      "models/CNN/pretrained_model.ckpt-294\n",
      "INFO:tensorflow:Restoring parameters from models/CNN/pretrained_model.ckpt-294\n",
      "acc:  0.37\n",
      "acc:  0.33\n",
      "acc:  0.27\n",
      "acc:  0.3\n",
      "acc:  0.34\n",
      "acc:  0.37\n",
      "acc:  0.39\n",
      "acc:  0.3\n",
      "acc:  0.33\n",
      "acc:  0.25\n",
      "acc:  0.36\n",
      "acc:  0.36\n",
      "acc:  0.31\n",
      "acc:  0.4\n",
      "acc:  0.32\n",
      "acc:  0.32\n",
      "acc:  0.28\n",
      "acc:  0.4\n",
      "acc:  0.4\n",
      "acc:  0.27\n",
      "acc:  0.31\n",
      "acc:  0.3\n",
      "acc:  0.41\n",
      "acc:  0.3\n",
      "acc:  0.33\n",
      "acc:  0.44\n",
      "acc:  0.3\n",
      "acc:  0.36\n",
      "acc:  0.32\n",
      "acc:  0.31\n",
      "acc:  0.33\n",
      "acc:  0.34\n",
      "acc:  0.35\n",
      "acc:  0.37\n",
      "acc:  0.38\n",
      "acc:  0.35\n",
      "acc:  0.33\n",
      "acc:  0.43\n",
      "acc:  0.4\n",
      "acc:  0.3\n",
      "acc:  0.31\n",
      "acc:  0.39\n",
      "acc:  0.38\n",
      "acc:  0.28\n",
      "acc:  0.38\n",
      "acc:  0.37\n",
      "acc:  0.37\n",
      "acc:  0.32\n",
      "acc:  0.27\n",
      "acc:  0.31\n",
      "acc:  0.39\n",
      "acc:  0.3\n",
      "acc:  0.35\n",
      "acc:  0.34\n",
      "acc:  0.28\n",
      "acc:  0.34\n",
      "acc:  0.31\n",
      "acc:  0.26\n",
      "acc:  0.42\n",
      "acc:  0.35\n",
      "acc:  0.38\n",
      "acc:  0.3\n",
      "acc:  0.36\n",
      "acc:  0.35\n",
      "acc:  0.33\n",
      "acc:  0.37\n",
      "acc:  0.3\n",
      "acc:  0.34\n",
      "acc:  0.32\n",
      "acc:  0.29\n",
      "acc:  0.3\n",
      "acc:  0.3\n",
      "acc:  0.39\n",
      "acc:  0.37\n",
      "acc:  0.34\n",
      "acc:  0.39\n",
      "acc:  0.28\n",
      "acc:  0.31\n",
      "acc:  0.34\n",
      "acc:  0.38\n",
      "acc:  0.35\n",
      "acc:  0.29\n",
      "acc:  0.3\n",
      "acc:  0.3\n",
      "acc:  0.36\n",
      "acc:  0.31\n",
      "acc:  0.34\n",
      "acc:  0.34\n",
      "acc:  0.29\n",
      "acc:  0.42\n",
      "acc:  0.33\n",
      "acc:  0.33\n",
      "acc:  0.3\n",
      "acc:  0.32\n",
      "acc:  0.3\n",
      "acc:  0.34\n",
      "acc:  0.35\n",
      "acc:  0.27\n",
      "acc:  0.42\n",
      "acc:  0.37\n",
      "acc:  0.35\n",
      "acc:  0.34\n",
      "acc:  0.33\n",
      "acc:  0.34\n",
      "acc:  0.29\n",
      "acc:  0.32\n",
      "acc:  0.29\n",
      "acc:  0.26\n",
      "acc:  0.38\n",
      "acc:  0.29\n",
      "acc:  0.3\n",
      "acc:  0.42\n",
      "acc:  0.33\n",
      "acc:  0.38\n",
      "acc:  0.27\n",
      "acc:  0.28\n",
      "acc:  0.32\n",
      "acc:  0.39\n",
      "acc:  0.37\n",
      "acc:  0.36\n",
      "acc:  0.41\n",
      "acc:  0.34\n",
      "acc:  0.38\n",
      "acc:  0.36\n",
      "acc:  0.37\n",
      "acc:  0.34\n",
      "acc:  0.34\n",
      "acc:  0.33\n",
      "acc:  0.35\n",
      "acc:  0.29\n",
      "acc:  0.38\n",
      "acc:  0.31\n",
      "acc:  0.37\n",
      "acc:  0.29\n",
      "acc:  0.34\n",
      "acc:  0.28\n",
      "acc:  0.26\n",
      "acc:  0.35\n",
      "acc:  0.25\n",
      "acc:  0.35\n",
      "acc:  0.26\n",
      "acc:  0.38\n",
      "acc:  0.28\n",
      "acc:  0.39\n",
      "acc:  0.33\n",
      "acc:  0.28\n",
      "acc:  0.33\n",
      "acc:  0.31\n",
      "acc:  0.31\n",
      "acc:  0.42\n",
      "acc:  0.35\n",
      "acc:  0.29\n",
      "acc:  0.34\n",
      "acc:  0.27\n",
      "acc:  0.34\n",
      "acc:  0.34\n",
      "acc:  0.37\n",
      "acc:  0.41\n",
      "acc:  0.26\n",
      "acc:  0.24\n",
      "acc:  0.32\n",
      "acc:  0.48\n",
      "acc:  0.3\n",
      "acc:  0.31\n",
      "acc:  0.4\n",
      "acc:  0.38\n",
      "acc:  0.37\n",
      "acc:  0.3\n",
      "acc:  0.42\n",
      "acc:  0.34\n",
      "acc:  0.29\n",
      "acc:  0.34\n",
      "acc:  0.32\n",
      "acc:  0.31\n",
      "acc:  0.33\n",
      "acc:  0.36\n",
      "acc:  0.3\n",
      "acc:  0.36\n",
      "acc:  0.35\n",
      "acc:  0.46\n",
      "acc:  0.33\n",
      "acc:  0.28\n",
      "acc:  0.32\n",
      "acc:  0.32\n",
      "acc:  0.26\n",
      "acc:  0.26\n",
      "acc:  0.26\n",
      "acc:  0.35\n",
      "acc:  0.4\n",
      "acc:  0.36\n",
      "acc:  0.31\n",
      "acc:  0.33\n",
      "acc:  0.34\n",
      "acc:  0.35\n",
      "acc:  0.4\n",
      "acc:  0.28\n",
      "acc:  0.37\n",
      "acc:  0.36\n",
      "acc:  0.39\n",
      "acc:  0.37\n",
      "acc:  0.36\n",
      "acc:  0.36\n",
      "acc:  0.32\n",
      "acc:  0.32\n",
      "acc:  0.25\n",
      "acc:  0.34\n",
      "acc:  0.34\n",
      "acc:  0.35\n",
      "acc:  0.27\n",
      "acc:  0.34\n",
      "acc:  0.38\n",
      "acc:  0.24\n",
      "acc:  0.36\n",
      "acc:  0.32\n",
      "acc:  0.27\n",
      "acc:  0.35\n",
      "acc:  0.34\n",
      "acc:  0.35\n",
      "acc:  0.31\n",
      "acc:  0.33\n",
      "acc:  0.31\n",
      "acc:  0.32\n",
      "acc:  0.35\n",
      "acc:  0.34\n",
      "acc:  0.32\n",
      "acc:  0.28\n",
      "acc:  0.47\n",
      "acc:  0.4\n",
      "acc:  0.27\n",
      "acc:  0.26\n",
      "acc:  0.27\n",
      "acc:  0.35\n",
      "acc:  0.28\n",
      "acc:  0.31\n",
      "acc:  0.31\n",
      "acc:  0.29\n",
      "acc:  0.27\n",
      "acc:  0.32\n",
      "acc:  0.4\n",
      "acc:  0.36\n",
      "acc:  0.3\n",
      "acc:  0.36\n",
      "acc:  0.32\n",
      "acc:  0.39\n",
      "acc:  0.29\n",
      "acc:  0.33\n",
      "acc:  0.4\n",
      "acc:  0.38\n",
      "acc:  0.34\n",
      "acc:  0.42\n",
      "acc:  0.31\n",
      "acc:  0.27\n",
      "acc:  0.26\n",
      "acc:  0.28\n",
      "acc:  0.28\n",
      "acc:  0.36\n",
      "acc:  0.31\n",
      "acc:  0.47\n",
      "acc:  0.35\n",
      "acc:  0.31\n",
      "acc:  0.35\n",
      "acc:  0.34\n",
      "acc:  0.31\n",
      "acc:  0.39\n",
      "acc:  0.3\n",
      "acc:  0.35\n",
      "acc:  0.35\n",
      "acc:  0.28\n",
      "acc:  0.29\n",
      "acc:  0.4\n",
      "acc:  0.37\n",
      "acc:  0.41\n",
      "acc:  0.37\n",
      "acc:  0.28\n",
      "acc:  0.37\n",
      "acc:  0.32\n",
      "acc:  0.31\n",
      "acc:  0.31\n",
      "acc:  0.29\n",
      "acc:  0.3\n",
      "acc:  0.31\n",
      "acc:  0.39\n",
      "acc:  0.29\n",
      "acc:  0.4\n",
      "acc:  0.32\n",
      "acc:  0.32\n",
      "acc:  0.31\n",
      "acc:  0.36\n",
      "acc:  0.32\n",
      "acc:  0.35\n",
      "acc:  0.35\n",
      "acc:  0.33\n",
      "acc:  0.4\n",
      "acc:  0.26\n",
      "acc:  0.26\n",
      "acc:  0.35\n",
      "acc:  0.21\n",
      "acc:  0.34\n",
      "acc:  0.47\n",
      "acc:  0.42\n",
      "acc:  0.23\n",
      "acc:  0.27\n",
      "acc:  0.39\n",
      "acc:  0.39\n",
      "acc:  0.33\n",
      "acc:  0.35\n",
      "acc:  0.32\n",
      "acc:  0.35\n",
      "acc:  0.33\n",
      "acc:  0.4\n",
      "acc:  0.37\n",
      "acc:  0.3\n",
      "acc:  0.28\n",
      "acc:  0.29\n",
      "acc:  0.29\n",
      "acc:  0.26\n",
      "acc:  0.36\n",
      "acc:  0.26\n",
      "acc:  0.3\n",
      "acc:  0.3\n",
      "acc:  0.3\n",
      "acc:  0.36\n",
      "acc:  0.41\n",
      "acc:  0.39\n",
      "acc:  0.37\n",
      "acc:  0.31\n",
      "acc:  0.31\n",
      "acc:  0.41\n",
      "acc:  0.37\n",
      "acc:  0.31\n",
      "acc:  0.29\n",
      "acc:  0.39\n",
      "acc:  0.31\n",
      "acc:  0.29\n",
      "acc:  0.36\n",
      "acc:  0.35\n",
      "acc:  0.27\n",
      "acc:  0.33\n",
      "acc:  0.26\n",
      "acc:  0.32\n",
      "acc:  0.44\n",
      "acc:  0.33\n",
      "acc:  0.43\n",
      "acc:  0.3\n",
      "acc:  0.31\n",
      "acc:  0.4\n",
      "acc:  0.27\n",
      "acc:  0.33\n",
      "acc:  0.33\n",
      "acc:  0.27\n",
      "acc:  0.26\n",
      "acc:  0.4\n",
      "acc:  0.42\n",
      "acc:  0.38\n",
      "acc:  0.29\n",
      "acc:  0.23\n",
      "acc:  0.31\n",
      "acc:  0.38\n",
      "acc:  0.38\n",
      "acc:  0.39\n",
      "acc:  0.3\n",
      "acc:  0.37\n",
      "acc:  0.32\n",
      "acc:  0.29\n",
      "acc:  0.38\n",
      "acc:  0.33\n",
      "acc:  0.41\n",
      "acc:  0.33\n",
      "acc:  0.3\n",
      "acc:  0.32\n",
      "acc:  0.37\n",
      "acc:  0.3\n",
      "acc:  0.28\n",
      "acc:  0.34\n",
      "acc:  0.33\n",
      "acc:  0.37\n",
      "acc:  0.37\n",
      "acc:  0.35\n",
      "acc:  0.34\n",
      "acc:  0.27\n",
      "acc:  0.39\n",
      "acc:  0.31\n",
      "acc:  0.33\n",
      "acc:  0.34\n",
      "acc:  0.31\n",
      "acc:  0.36\n",
      "acc:  0.34\n",
      "acc:  0.32\n",
      "acc:  0.36\n",
      "acc:  0.27\n",
      "acc:  0.31\n",
      "acc:  0.35\n",
      "acc:  0.38\n",
      "acc:  0.35\n",
      "acc:  0.36\n",
      "acc:  0.27\n",
      "acc:  0.41\n",
      "acc:  0.38\n",
      "acc:  0.28\n",
      "acc:  0.36\n",
      "acc:  0.33\n",
      "acc:  0.39\n",
      "acc:  0.37\n",
      "acc:  0.36\n",
      "acc:  0.25\n",
      "acc:  0.33\n",
      "acc:  0.28\n",
      "acc:  0.22\n",
      "acc:  0.27\n",
      "acc:  0.49\n",
      "acc:  0.32\n",
      "acc:  0.37\n",
      "acc:  0.32\n",
      "acc:  0.32\n",
      "acc:  0.3\n",
      "acc:  0.35\n",
      "acc:  0.38\n",
      "acc:  0.3\n",
      "acc:  0.31\n",
      "acc:  0.33\n",
      "acc:  0.35\n",
      "acc:  0.35\n",
      "acc:  0.39\n",
      "acc:  0.27\n",
      "acc:  0.32\n",
      "acc:  0.34\n",
      "acc:  0.34\n",
      "acc:  0.41\n",
      "acc:  0.35\n",
      "acc:  0.24\n",
      "acc:  0.35\n",
      "acc:  0.3\n",
      "acc:  0.31\n",
      "acc:  0.37\n",
      "acc:  0.39\n",
      "acc:  0.32\n",
      "acc:  0.31\n",
      "acc:  0.3\n",
      "acc:  0.37\n",
      "acc:  0.38\n",
      "acc:  0.27\n",
      "acc:  0.35\n",
      "acc:  0.32\n",
      "acc:  0.3\n",
      "acc:  0.43\n",
      "acc:  0.28\n",
      "acc:  0.34\n",
      "acc:  0.35\n",
      "acc:  0.34\n",
      "acc:  0.3\n",
      "acc:  0.3\n",
      "acc:  0.34\n",
      "acc:  0.32\n",
      "acc:  0.26\n",
      "acc:  0.39\n",
      "acc:  0.37\n",
      "acc:  0.34\n",
      "acc:  0.33\n",
      "acc:  0.24\n",
      "acc:  0.36\n",
      "acc:  0.45\n",
      "acc:  0.32\n",
      "acc:  0.37\n",
      "acc:  0.31\n",
      "acc:  0.36\n",
      "acc:  0.43\n",
      "acc:  0.44\n",
      "acc:  0.3\n",
      "acc:  0.38\n",
      "acc:  0.39\n",
      "acc:  0.37\n",
      "acc:  0.3\n",
      "acc:  0.32\n",
      "acc:  0.25\n",
      "acc:  0.31\n",
      "acc:  0.36\n",
      "acc:  0.39\n",
      "acc:  0.29\n",
      "acc:  0.36\n",
      "acc:  0.35\n",
      "acc:  0.32\n",
      "acc:  0.27\n",
      "acc:  0.31\n",
      "acc:  0.37\n",
      "acc:  0.37\n",
      "acc:  0.3\n",
      "acc:  0.28\n",
      "acc:  0.34\n",
      "acc:  0.29\n",
      "acc:  0.28\n",
      "acc:  0.3\n",
      "acc:  0.41\n",
      "acc:  0.31\n",
      "acc:  0.37\n",
      "acc:  0.36\n",
      "acc:  0.26\n",
      "acc:  0.37\n",
      "acc:  0.43\n",
      "acc:  0.31\n",
      "acc:  0.37\n",
      "\n",
      "500 \t [127.00231395 152.91189822 202.48275183]\n",
      "val accuracy:  0.33457997 \t f_! score:  [0.25400463 0.3058238  0.4049655 ]\n",
      "\n",
      "500 \t [127.00231395 152.91189822 202.48275183]\n",
      "500 \t [167.69658292 155.01371626 174.11758505]\n",
      "500 \t [104.13023893 154.63941186 245.50391845]\n",
      "500 \t [18794. 13823. 17383.]\n",
      "---just Test  Review ---\n",
      "0.33457997\n",
      "f1:  [0.25400463 0.3058238  0.4049655 ]\n",
      "65730\n",
      "Vorsicht! es muss vorher ein  CNN  Netz abgespeichert worden sein\n",
      "\n",
      "Restoring...\n",
      "models/CNN/pretrained_model.ckpt-294\n",
      "INFO:tensorflow:Restoring parameters from models/CNN/pretrained_model.ckpt-294\n",
      "acc:  0.34\n",
      "acc:  0.4\n",
      "acc:  0.34\n",
      "acc:  0.32\n",
      "acc:  0.33\n",
      "acc:  0.29\n",
      "acc:  0.38\n",
      "acc:  0.32\n",
      "acc:  0.33\n",
      "acc:  0.36\n",
      "acc:  0.28\n",
      "acc:  0.35\n",
      "acc:  0.36\n",
      "acc:  0.36\n",
      "acc:  0.33\n",
      "acc:  0.32\n",
      "acc:  0.3\n",
      "acc:  0.32\n",
      "acc:  0.35\n",
      "acc:  0.3\n",
      "acc:  0.31\n",
      "acc:  0.3\n",
      "acc:  0.29\n",
      "acc:  0.29\n",
      "acc:  0.34\n",
      "acc:  0.29\n",
      "acc:  0.31\n",
      "acc:  0.3\n",
      "acc:  0.36\n",
      "acc:  0.31\n",
      "acc:  0.33\n",
      "acc:  0.25\n",
      "acc:  0.3\n",
      "acc:  0.29\n",
      "acc:  0.3\n",
      "acc:  0.42\n",
      "acc:  0.34\n",
      "acc:  0.33\n",
      "acc:  0.36\n",
      "acc:  0.39\n",
      "acc:  0.34\n",
      "acc:  0.38\n",
      "acc:  0.34\n",
      "acc:  0.36\n",
      "acc:  0.34\n",
      "acc:  0.36\n",
      "acc:  0.31\n",
      "acc:  0.32\n",
      "acc:  0.31\n",
      "acc:  0.25\n",
      "acc:  0.31\n",
      "acc:  0.43\n",
      "acc:  0.34\n",
      "acc:  0.38\n",
      "acc:  0.41\n",
      "acc:  0.37\n",
      "acc:  0.35\n",
      "acc:  0.36\n",
      "acc:  0.32\n",
      "acc:  0.42\n",
      "acc:  0.29\n",
      "acc:  0.38\n",
      "acc:  0.34\n",
      "acc:  0.23\n",
      "acc:  0.33\n",
      "acc:  0.39\n",
      "acc:  0.24\n",
      "acc:  0.29\n",
      "acc:  0.28\n",
      "acc:  0.3\n",
      "acc:  0.36\n",
      "acc:  0.34\n",
      "acc:  0.39\n",
      "acc:  0.34\n",
      "acc:  0.31\n",
      "acc:  0.32\n",
      "acc:  0.38\n",
      "acc:  0.34\n",
      "acc:  0.3\n",
      "acc:  0.36\n",
      "acc:  0.29\n",
      "acc:  0.32\n",
      "acc:  0.32\n",
      "acc:  0.38\n",
      "acc:  0.35\n",
      "acc:  0.4\n",
      "acc:  0.26\n",
      "acc:  0.38\n",
      "acc:  0.32\n",
      "acc:  0.39\n",
      "acc:  0.33\n",
      "acc:  0.35\n",
      "acc:  0.39\n",
      "acc:  0.25\n",
      "acc:  0.32\n",
      "acc:  0.33\n",
      "acc:  0.28\n",
      "acc:  0.33\n",
      "acc:  0.32\n",
      "acc:  0.34\n",
      "\n",
      "100 \t [ 0.         0.        49.7664238]\n",
      "val accuracy:  0.3325 \t f_! score:  [0.         0.         0.49766424]\n",
      "\n",
      "100 \t [ 0.         0.        49.7664238]\n",
      "100 \t [ 0.    0.   33.25]\n",
      "100 \t [  0.   0. 100.]\n",
      "100 \t [3323. 3352. 3325.]\n",
      "---just Test  Twitter ---\n",
      "0.3325\n",
      "f1:  [0.         0.         0.49766424]\n",
      "(37126,)\n",
      "(37126,)\n",
      "37126\n",
      "Vorsicht! es muss vorher ein  CNN  Netz abgespeichert worden sein\n",
      "\n",
      "Restoring...\n",
      "models/CNN/pretrained_model.ckpt-294\n",
      "INFO:tensorflow:Restoring parameters from models/CNN/pretrained_model.ckpt-294\n",
      "acc:  0.0\n",
      "acc:  0.04\n",
      "acc:  0.02\n",
      "acc:  0.15\n",
      "acc:  0.03\n",
      "acc:  0.05\n",
      "acc:  0.03\n",
      "acc:  0.03\n",
      "acc:  0.06\n",
      "acc:  0.12\n",
      "acc:  0.17\n",
      "acc:  0.04\n",
      "acc:  0.02\n",
      "acc:  0.1\n",
      "acc:  0.11\n",
      "acc:  0.16\n",
      "acc:  0.09\n",
      "acc:  0.09\n",
      "acc:  0.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yannik/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.07\n",
      "acc:  0.05\n",
      "acc:  0.1\n",
      "acc:  0.05\n",
      "acc:  0.06\n",
      "acc:  0.07\n",
      "acc:  0.01\n",
      "acc:  0.17\n",
      "acc:  0.08\n",
      "acc:  0.11\n",
      "acc:  0.1\n",
      "acc:  0.06\n",
      "acc:  0.04\n",
      "acc:  0.03\n",
      "acc:  0.11\n",
      "acc:  0.14\n",
      "acc:  0.04\n",
      "acc:  0.04\n",
      "acc:  0.04\n",
      "acc:  0.18\n",
      "acc:  0.14\n",
      "acc:  0.14\n",
      "acc:  0.06\n",
      "acc:  0.09\n",
      "acc:  0.08\n",
      "acc:  0.08\n",
      "acc:  0.11\n",
      "acc:  0.12\n",
      "acc:  0.04\n",
      "acc:  0.17\n",
      "acc:  0.14\n",
      "acc:  0.01\n",
      "acc:  0.1\n",
      "acc:  0.22\n",
      "acc:  0.21\n",
      "acc:  0.06\n",
      "acc:  0.04\n",
      "acc:  0.1\n",
      "acc:  0.03\n",
      "acc:  0.15\n",
      "acc:  0.11\n",
      "acc:  0.06\n",
      "acc:  0.28\n",
      "acc:  0.06\n",
      "acc:  0.11\n",
      "acc:  0.11\n",
      "acc:  0.04\n",
      "acc:  0.15\n",
      "acc:  0.09\n",
      "acc:  0.09\n",
      "acc:  0.05\n",
      "\n",
      "70 \t [ 7.99030406 12.1047356   0.        ]\n",
      "val accuracy:  0.08842858 \t f_! score:  [0.1141472  0.17292479 0.        ]\n",
      "\n",
      "70 \t [ 7.99030406 12.1047356   0.        ]\n",
      "70 \t [4.83041076 7.48890931 0.        ]\n",
      "70 \t [31.77592204 37.63307816  0.        ]\n",
      "70 \t [ 501.  664. 5835.]\n",
      "---just Test  Prime ---\n",
      "0.08842858\n",
      "f1:  [0.1141472  0.17292479 0.        ]\n"
     ]
    }
   ],
   "source": [
    "med_loss, med_acc = testhelper.train_input(\"Medical\", net_name)\n",
    "\n",
    "testhelper.just_test(\"Review\", net_name)\n",
    "testhelper.just_test(\"Twitter\", net_name)\n",
    "testhelper.just_test(\"Prime\", net_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  Prime\n",
      "delete old models\n",
      "(37126,)\n",
      "(37126,)\n",
      "37126\n",
      "Tensor(\"ConvNet/dense/BiasAdd:0\", shape=(?, 3), dtype=float32)\n",
      "Tensor(\"ConvNet_1/dense/BiasAdd:0\", shape=(?, 3), dtype=float32)\n",
      "---init ready   CNN ---\n",
      "\n",
      "Saving...\n",
      "saved to models/CNN/pretrained_model.ckpt-0\n",
      "0 : loss:  2.8770044 \t acc:  0.73\n",
      "2 : loss:  0.98231196 \t acc:  0.69\n",
      "4 : loss:  0.92788804 \t acc:  0.72\n",
      "6 : loss:  0.57732767 \t acc:  0.84\n",
      "8 : loss:  0.6055896 \t acc:  0.84\n",
      "10 : loss:  0.5482247 \t acc:  0.85\n",
      "12 : loss:  0.5923347 \t acc:  0.83\n",
      "14 : loss:  0.84221095 \t acc:  0.75\n",
      "16 : loss:  0.802317 \t acc:  0.85\n",
      "18 : loss:  1.012411 \t acc:  0.56\n",
      "20 : loss:  0.6102184 \t acc:  0.84\n",
      "22 : loss:  0.7211288 \t acc:  0.76\n",
      "24 : loss:  0.34848073 \t acc:  0.91\n",
      "26 : loss:  0.44605488 \t acc:  0.88\n",
      "28 : loss:  0.63242143 \t acc:  0.79\n",
      "30 : loss:  0.80028033 \t acc:  0.72\n",
      "32 : loss:  1.237333 \t acc:  0.49\n",
      "34 : loss:  0.8379158 \t acc:  0.71\n",
      "36 : loss:  0.6554705 \t acc:  0.85\n",
      "38 : loss:  0.74974054 \t acc:  0.78\n",
      "40 : loss:  0.7043683 \t acc:  0.78\n",
      "42 : loss:  0.5719647 \t acc:  0.82\n",
      "44 : loss:  0.7001341 \t acc:  0.78\n",
      "46 : loss:  0.5812208 \t acc:  0.82\n",
      "48 : loss:  0.4604958 \t acc:  0.89\n",
      "50 : loss:  0.86108595 \t acc:  0.7\n",
      "52 : loss:  0.8905672 \t acc:  0.67\n",
      "54 : loss:  0.6544487 \t acc:  0.89\n",
      "56 : loss:  0.7613757 \t acc:  0.75\n",
      "58 : loss:  0.398646 \t acc:  0.95\n",
      "60 : loss:  0.8666113 \t acc:  0.73\n",
      "62 : loss:  0.43900874 \t acc:  0.91\n",
      "64 : loss:  0.53689474 \t acc:  0.86\n",
      "66 : loss:  0.38478875 \t acc:  0.89\n",
      "68 : loss:  0.7830389 \t acc:  0.73\n",
      "70 : loss:  0.7663881 \t acc:  0.72\n",
      "72 : loss:  0.71859986 \t acc:  0.78\n",
      "74 : loss:  0.611841 \t acc:  0.85\n",
      "76 : loss:  0.54806274 \t acc:  0.84\n",
      "78 : loss:  0.6225221 \t acc:  0.8\n",
      "80 : loss:  0.49033624 \t acc:  0.87\n",
      "82 : loss:  0.7563627 \t acc:  0.74\n",
      "84 : loss:  0.4111739 \t acc:  0.89\n",
      "86 : loss:  0.72989684 \t acc:  0.76\n",
      "88 : loss:  0.556962 \t acc:  0.84\n",
      "90 : loss:  0.94061625 \t acc:  0.66\n",
      "92 : loss:  0.56563795 \t acc:  0.83\n",
      "94 : loss:  0.85589784 \t acc:  0.68\n",
      "96 : loss:  0.8639767 \t acc:  0.64\n",
      "98 : loss:  0.53077966 \t acc:  0.86\n",
      "100 : loss:  0.75525117 \t acc:  0.72\n",
      "102 : loss:  0.57055765 \t acc:  0.83\n",
      "104 : loss:  0.8306081 \t acc:  0.66\n",
      "106 : loss:  0.61065626 \t acc:  0.82\n",
      "108 : loss:  0.5358942 \t acc:  0.85\n",
      "110 : loss:  0.789092 \t acc:  0.69\n",
      "112 : loss:  0.6347272 \t acc:  0.81\n",
      "114 : loss:  0.5560565 \t acc:  0.82\n",
      "116 : loss:  0.4304604 \t acc:  0.88\n",
      "118 : loss:  0.5234795 \t acc:  0.84\n",
      "120 : loss:  0.89297485 \t acc:  0.65\n",
      "122 : loss:  0.5820454 \t acc:  0.84\n",
      "124 : loss:  0.6206923 \t acc:  0.87\n",
      "126 : loss:  0.6260725 \t acc:  0.86\n",
      "128 : loss:  0.92762774 \t acc:  0.61\n",
      "130 : loss:  0.40124598 \t acc:  0.9\n",
      "132 : loss:  0.3855192 \t acc:  0.89\n",
      "134 : loss:  1.1825656 \t acc:  0.61\n",
      "136 : loss:  0.73173887 \t acc:  0.78\n",
      "138 : loss:  0.61612624 \t acc:  0.8\n",
      "140 : loss:  0.7223072 \t acc:  0.75\n",
      "142 : loss:  0.8675929 \t acc:  0.67\n",
      "144 : loss:  0.79511094 \t acc:  0.71\n",
      "146 : loss:  0.79616237 \t acc:  0.68\n",
      "148 : loss:  0.6432869 \t acc:  0.79\n",
      "150 : loss:  0.6466297 \t acc:  0.81\n",
      "152 : loss:  1.029286 \t acc:  0.61\n",
      "154 : loss:  0.78793246 \t acc:  0.74\n",
      "156 : loss:  0.26865476 \t acc:  0.95\n",
      "158 : loss:  0.41371387 \t acc:  0.9\n",
      "160 : loss:  0.34446838 \t acc:  0.97\n",
      "162 : loss:  0.7087994 \t acc:  0.76\n",
      "164 : loss:  0.84253645 \t acc:  0.67\n",
      "166 : loss:  0.67944616 \t acc:  0.75\n",
      "168 : loss:  0.5543971 \t acc:  0.83\n",
      "170 : loss:  0.5424498 \t acc:  0.83\n",
      "172 : loss:  0.74081403 \t acc:  0.74\n",
      "174 : loss:  0.57726794 \t acc:  0.8\n",
      "176 : loss:  0.5315303 \t acc:  0.85\n",
      "178 : loss:  0.5518152 \t acc:  0.83\n",
      "180 : loss:  0.35878864 \t acc:  0.92\n",
      "182 : loss:  0.7393156 \t acc:  0.76\n",
      "184 : loss:  0.47245166 \t acc:  0.84\n",
      "186 : loss:  0.6420548 \t acc:  0.81\n",
      "188 : loss:  0.37935758 \t acc:  0.9\n",
      "190 : loss:  0.6855237 \t acc:  0.75\n",
      "192 : loss:  0.69400895 \t acc:  0.78\n",
      "194 : loss:  0.65728325 \t acc:  0.86\n",
      "196 : loss:  0.7025021 \t acc:  0.8\n",
      "198 : loss:  0.80894035 \t acc:  0.7\n",
      "200 : loss:  0.6779708 \t acc:  0.79\n",
      "\n",
      "Saving...\n",
      "saved to models/CNN/pretrained_model.ckpt-201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yannik/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(2,)\n",
      "\n",
      "68 \t [ 0.          0.         61.49417912]\n",
      "0 \tval accuracy:  0.8316667 \t f_! score:  [0.         0.         0.90432616]\n",
      "\n",
      "202 : loss:  0.7269965 \t acc:  0.76\n",
      "204 : loss:  0.5977089 \t acc:  0.81\n",
      "206 : loss:  0.87215316 \t acc:  0.72\n",
      "208 : loss:  1.5634277 \t acc:  0.49\n",
      "210 : loss:  0.37047505 \t acc:  0.89\n",
      "212 : loss:  0.70509076 \t acc:  0.76\n",
      "214 : loss:  0.42908564 \t acc:  0.92\n",
      "216 : loss:  0.6513096 \t acc:  0.79\n",
      "218 : loss:  0.63874745 \t acc:  0.8\n",
      "220 : loss:  1.1044782 \t acc:  0.48\n",
      "222 : loss:  0.7623877 \t acc:  0.71\n",
      "224 : loss:  1.0354095 \t acc:  0.6\n",
      "226 : loss:  0.8865043 \t acc:  0.66\n",
      "228 : loss:  0.8076674 \t acc:  0.7\n",
      "230 : loss:  0.52406067 \t acc:  0.86\n",
      "232 : loss:  0.78389364 \t acc:  0.66\n",
      "234 : loss:  0.6917163 \t acc:  0.75\n",
      "236 : loss:  1.1462197 \t acc:  0.61\n",
      "238 : loss:  0.5380965 \t acc:  0.86\n",
      "240 : loss:  1.1528807 \t acc:  0.52\n",
      "242 : loss:  0.7066264 \t acc:  0.78\n",
      "244 : loss:  1.0216118 \t acc:  0.48\n",
      "246 : loss:  0.873616 \t acc:  0.62\n",
      "248 : loss:  0.3608494 \t acc:  0.97\n",
      "250 : loss:  0.27708423 \t acc:  0.96\n",
      "252 : loss:  0.35101876 \t acc:  0.9\n",
      "254 : loss:  0.33933488 \t acc:  0.91\n",
      "256 : loss:  0.7169889 \t acc:  0.73\n",
      "258 : loss:  0.6722641 \t acc:  0.75\n",
      "260 : loss:  0.20926441 \t acc:  0.96\n",
      "262 : loss:  0.46850547 \t acc:  0.84\n",
      "264 : loss:  0.3176361 \t acc:  0.92\n",
      "266 : loss:  0.32657352 \t acc:  0.96\n",
      "268 : loss:  0.5799932 \t acc:  0.84\n",
      "270 : loss:  0.5419409 \t acc:  0.84\n",
      "272 : loss:  0.77653944 \t acc:  0.68\n",
      "274 : loss:  0.48453823 \t acc:  0.85\n",
      "276 : loss:  0.7144339 \t acc:  0.71\n",
      "278 : loss:  0.72822106 \t acc:  0.71\n",
      "280 : loss:  0.4844306 \t acc:  0.9\n",
      "282 : loss:  0.5639495 \t acc:  0.84\n",
      "284 : loss:  1.0422388 \t acc:  0.49\n",
      "286 : loss:  0.33792755 \t acc:  0.93\n",
      "288 : loss:  1.0131437 \t acc:  0.61\n",
      "290 : loss:  0.35252082 \t acc:  0.92\n",
      "292 : loss:  0.78638476 \t acc:  0.69\n",
      "294 : loss:  0.63444847 \t acc:  0.79\n",
      "296 : loss:  0.822112 \t acc:  0.64\n",
      "298 : loss:  0.4928626 \t acc:  0.84\n",
      "300 : loss:  0.49641258 \t acc:  0.83\n",
      "302 : loss:  0.20987236 \t acc:  0.96\n",
      "304 : loss:  0.9385701 \t acc:  0.68\n",
      "306 : loss:  0.58542114 \t acc:  0.8\n",
      "308 : loss:  0.6886133 \t acc:  0.73\n",
      "310 : loss:  0.74500257 \t acc:  0.74\n",
      "312 : loss:  0.47216183 \t acc:  0.83\n",
      "314 : loss:  0.56764245 \t acc:  0.84\n",
      "316 : loss:  1.1510053 \t acc:  0.53\n",
      "318 : loss:  0.8504227 \t acc:  0.63\n",
      "320 : loss:  0.43244228 \t acc:  0.88\n",
      "322 : loss:  0.82284164 \t acc:  0.61\n",
      "324 : loss:  0.58979356 \t acc:  0.8\n",
      "326 : loss:  0.6061022 \t acc:  0.75\n",
      "328 : loss:  0.33261955 \t acc:  0.9\n",
      "330 : loss:  0.9565236 \t acc:  0.66\n",
      "332 : loss:  0.5741678 \t acc:  0.76\n",
      "334 : loss:  0.59932494 \t acc:  0.77\n",
      "336 : loss:  0.5806638 \t acc:  0.84\n",
      "338 : loss:  0.60869145 \t acc:  0.82\n",
      "340 : loss:  0.50055957 \t acc:  0.85\n",
      "342 : loss:  0.3120307 \t acc:  0.9\n",
      "344 : loss:  0.3799599 \t acc:  0.89\n",
      "346 : loss:  0.6123208 \t acc:  0.75\n",
      "348 : loss:  0.40108532 \t acc:  0.87\n",
      "350 : loss:  0.77777094 \t acc:  0.67\n",
      "352 : loss:  0.4959794 \t acc:  0.86\n",
      "354 : loss:  0.41653883 \t acc:  0.92\n",
      "356 : loss:  0.48747388 \t acc:  0.83\n",
      "358 : loss:  0.8888371 \t acc:  0.57\n",
      "360 : loss:  0.6016863 \t acc:  0.79\n",
      "362 : loss:  0.2527099 \t acc:  0.92\n",
      "364 : loss:  0.66865504 \t acc:  0.73\n",
      "366 : loss:  0.32327676 \t acc:  0.95\n",
      "368 : loss:  0.5408772 \t acc:  0.87\n",
      "370 : loss:  0.65468615 \t acc:  0.78\n",
      "372 : loss:  0.23862553 \t acc:  0.95\n",
      "374 : loss:  0.77903175 \t acc:  0.72\n",
      "376 : loss:  0.77021736 \t acc:  0.67\n",
      "378 : loss:  0.68159425 \t acc:  0.7\n",
      "380 : loss:  0.8099578 \t acc:  0.66\n",
      "382 : loss:  0.8102289 \t acc:  0.61\n",
      "384 : loss:  0.67740667 \t acc:  0.68\n",
      "386 : loss:  0.3450838 \t acc:  0.97\n",
      "388 : loss:  0.46685946 \t acc:  0.85\n",
      "390 : loss:  0.40630832 \t acc:  0.9\n",
      "392 : loss:  0.40731516 \t acc:  0.88\n",
      "394 : loss:  0.94230086 \t acc:  0.59\n",
      "396 : loss:  0.66375357 \t acc:  0.75\n",
      "398 : loss:  0.5966158 \t acc:  0.76\n",
      "400 : loss:  0.7572203 \t acc:  0.72\n",
      "\n",
      "Saving...\n",
      "saved to models/CNN/pretrained_model.ckpt-402\n",
      "(2,)\n",
      "(2,)\n",
      "\n",
      "68 \t [ 0.          0.         61.49417912]\n",
      "1 \tval accuracy:  0.83166665 \t f_! score:  [0.         0.         0.90432616]\n",
      "\n",
      "402 : loss:  0.539338 \t acc:  0.78\n",
      "404 : loss:  0.4602101 \t acc:  0.84\n",
      "406 : loss:  0.31537706 \t acc:  0.92\n",
      "408 : loss:  0.48273018 \t acc:  0.85\n",
      "410 : loss:  0.7452712 \t acc:  0.67\n",
      "412 : loss:  0.7294121 \t acc:  0.67\n",
      "414 : loss:  0.4995091 \t acc:  0.83\n",
      "416 : loss:  0.89852023 \t acc:  0.61\n",
      "418 : loss:  0.43394566 \t acc:  0.9\n",
      "420 : loss:  0.4976313 \t acc:  0.88\n",
      "422 : loss:  0.49623013 \t acc:  0.79\n",
      "424 : loss:  0.5533365 \t acc:  0.77\n",
      "426 : loss:  0.8515741 \t acc:  0.66\n",
      "428 : loss:  0.47335312 \t acc:  0.84\n",
      "430 : loss:  0.6652541 \t acc:  0.74\n",
      "432 : loss:  0.41410926 \t acc:  0.92\n",
      "434 : loss:  0.4769025 \t acc:  0.81\n",
      "436 : loss:  0.57405126 \t acc:  0.79\n",
      "438 : loss:  0.614454 \t acc:  0.73\n",
      "440 : loss:  0.22890672 \t acc:  0.95\n",
      "442 : loss:  0.6437527 \t acc:  0.72\n",
      "444 : loss:  0.17845367 \t acc:  0.97\n",
      "446 : loss:  0.43864894 \t acc:  0.82\n",
      "448 : loss:  0.44308168 \t acc:  0.85\n",
      "450 : loss:  0.56515276 \t acc:  0.77\n",
      "452 : loss:  0.48376778 \t acc:  0.8\n",
      "454 : loss:  0.35460702 \t acc:  0.91\n",
      "456 : loss:  0.61914015 \t acc:  0.71\n",
      "458 : loss:  0.37401065 \t acc:  0.9\n",
      "460 : loss:  0.27586162 \t acc:  0.91\n",
      "462 : loss:  0.8813022 \t acc:  0.58\n",
      "464 : loss:  0.8521673 \t acc:  0.58\n",
      "466 : loss:  0.57951194 \t acc:  0.82\n",
      "468 : loss:  0.83977526 \t acc:  0.64\n",
      "470 : loss:  0.36218426 \t acc:  0.91\n",
      "472 : loss:  0.430915 \t acc:  0.85\n",
      "474 : loss:  0.54985726 \t acc:  0.87\n",
      "476 : loss:  0.67427516 \t acc:  0.75\n",
      "478 : loss:  0.452032 \t acc:  0.87\n",
      "480 : loss:  0.38085473 \t acc:  0.84\n",
      "482 : loss:  0.7161917 \t acc:  0.69\n",
      "484 : loss:  0.6928076 \t acc:  0.72\n",
      "486 : loss:  0.7894699 \t acc:  0.68\n",
      "488 : loss:  0.6812564 \t acc:  0.74\n",
      "490 : loss:  0.6625944 \t acc:  0.75\n",
      "492 : loss:  0.45617044 \t acc:  0.86\n",
      "494 : loss:  0.5334146 \t acc:  0.78\n",
      "496 : loss:  0.60434324 \t acc:  0.78\n",
      "498 : loss:  0.3750797 \t acc:  0.88\n",
      "500 : loss:  0.96971774 \t acc:  0.68\n",
      "502 : loss:  0.25038233 \t acc:  0.93\n",
      "504 : loss:  0.83890253 \t acc:  0.58\n",
      "506 : loss:  0.44824037 \t acc:  0.87\n",
      "508 : loss:  0.43030113 \t acc:  0.85\n",
      "510 : loss:  0.5375395 \t acc:  0.76\n",
      "512 : loss:  0.5957149 \t acc:  0.76\n",
      "514 : loss:  0.5880737 \t acc:  0.76\n",
      "516 : loss:  0.7905347 \t acc:  0.74\n",
      "518 : loss:  0.5858992 \t acc:  0.73\n",
      "520 : loss:  0.9154835 \t acc:  0.52\n",
      "522 : loss:  0.61181176 \t acc:  0.75\n",
      "524 : loss:  0.40556905 \t acc:  0.87\n",
      "526 : loss:  0.3600871 \t acc:  0.91\n",
      "528 : loss:  0.55141985 \t acc:  0.84\n",
      "530 : loss:  0.8876395 \t acc:  0.61\n",
      "532 : loss:  0.49927408 \t acc:  0.84\n",
      "534 : loss:  0.4599134 \t acc:  0.8\n",
      "536 : loss:  0.42220166 \t acc:  0.86\n",
      "538 : loss:  0.6981195 \t acc:  0.72\n",
      "540 : loss:  0.38187444 \t acc:  0.85\n",
      "542 : loss:  0.38275415 \t acc:  0.88\n",
      "544 : loss:  0.4672741 \t acc:  0.81\n",
      "546 : loss:  0.7968335 \t acc:  0.63\n",
      "548 : loss:  0.521962 \t acc:  0.84\n",
      "550 : loss:  0.9495228 \t acc:  0.52\n",
      "552 : loss:  0.52391535 \t acc:  0.84\n",
      "554 : loss:  0.20558563 \t acc:  0.96\n",
      "556 : loss:  0.8828847 \t acc:  0.59\n",
      "558 : loss:  0.5695273 \t acc:  0.8\n",
      "560 : loss:  0.2777395 \t acc:  0.93\n",
      "562 : loss:  0.6200955 \t acc:  0.71\n",
      "564 : loss:  0.5667242 \t acc:  0.8\n",
      "566 : loss:  0.37260708 \t acc:  0.89\n",
      "568 : loss:  0.40879744 \t acc:  0.86\n",
      "570 : loss:  0.5340443 \t acc:  0.79\n",
      "572 : loss:  0.50972754 \t acc:  0.85\n",
      "574 : loss:  1.3310028 \t acc:  0.35\n",
      "576 : loss:  0.6129192 \t acc:  0.75\n",
      "578 : loss:  0.6156017 \t acc:  0.83\n",
      "580 : loss:  0.35171714 \t acc:  0.96\n",
      "582 : loss:  0.5849843 \t acc:  0.81\n",
      "584 : loss:  1.0466223 \t acc:  0.43\n",
      "586 : loss:  0.970732 \t acc:  0.48\n",
      "588 : loss:  0.5434313 \t acc:  0.79\n",
      "590 : loss:  0.4288368 \t acc:  0.86\n",
      "592 : loss:  0.4830942 \t acc:  0.84\n",
      "594 : loss:  0.7624739 \t acc:  0.66\n",
      "596 : loss:  0.73890185 \t acc:  0.65\n",
      "598 : loss:  0.66306704 \t acc:  0.71\n",
      "600 : loss:  0.28648332 \t acc:  0.94\n",
      "602 : loss:  0.5974757 \t acc:  0.74\n",
      "\n",
      "Saving...\n",
      "saved to models/CNN/pretrained_model.ckpt-603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yannik/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "70 \t [18.33559969  0.         63.68375197]\n",
      "2 \tval accuracy:  0.8342857 \t f_! score:  [0.26193714 0.         0.90976789]\n",
      "\n",
      "604 : loss:  0.21388265 \t acc:  0.96\n",
      "606 : loss:  0.39446527 \t acc:  0.86\n",
      "608 : loss:  0.7003877 \t acc:  0.72\n",
      "610 : loss:  0.28601286 \t acc:  0.94\n",
      "612 : loss:  0.38977355 \t acc:  0.86\n",
      "614 : loss:  0.59652376 \t acc:  0.74\n",
      "616 : loss:  0.26306075 \t acc:  0.92\n",
      "618 : loss:  0.60353893 \t acc:  0.72\n",
      "620 : loss:  0.5383528 \t acc:  0.8\n",
      "622 : loss:  0.44762766 \t acc:  0.82\n",
      "624 : loss:  0.48340797 \t acc:  0.83\n",
      "626 : loss:  0.55998224 \t acc:  0.82\n",
      "628 : loss:  0.6244943 \t acc:  0.72\n",
      "630 : loss:  0.49227935 \t acc:  0.79\n",
      "632 : loss:  0.560303 \t acc:  0.75\n",
      "634 : loss:  0.50811905 \t acc:  0.78\n",
      "636 : loss:  0.7348862 \t acc:  0.65\n",
      "638 : loss:  0.6362212 \t acc:  0.75\n",
      "640 : loss:  0.85941076 \t acc:  0.61\n",
      "642 : loss:  0.35569993 \t acc:  0.88\n",
      "644 : loss:  0.3364436 \t acc:  0.9\n",
      "646 : loss:  0.8307686 \t acc:  0.61\n",
      "648 : loss:  0.49169528 \t acc:  0.83\n",
      "650 : loss:  0.43478474 \t acc:  0.86\n",
      "652 : loss:  0.7681459 \t acc:  0.63\n",
      "654 : loss:  0.5928355 \t acc:  0.78\n",
      "656 : loss:  0.34577832 \t acc:  0.89\n",
      "658 : loss:  0.45489562 \t acc:  0.79\n",
      "660 : loss:  0.39709008 \t acc:  0.87\n",
      "662 : loss:  0.50053906 \t acc:  0.83\n",
      "664 : loss:  0.5739798 \t acc:  0.73\n",
      "666 : loss:  0.8957703 \t acc:  0.61\n",
      "668 : loss:  0.31129867 \t acc:  0.9\n",
      "670 : loss:  0.3587059 \t acc:  0.9\n",
      "672 : loss:  0.5059179 \t acc:  0.79\n",
      "674 : loss:  0.3063084 \t acc:  0.9\n",
      "676 : loss:  0.52443415 \t acc:  0.79\n",
      "678 : loss:  0.7170837 \t acc:  0.7\n",
      "680 : loss:  0.4954064 \t acc:  0.77\n",
      "682 : loss:  0.7467854 \t acc:  0.63\n",
      "684 : loss:  0.40123093 \t acc:  0.89\n",
      "686 : loss:  0.35064587 \t acc:  0.89\n",
      "688 : loss:  0.31446466 \t acc:  0.91\n",
      "690 : loss:  0.4648142 \t acc:  0.8\n",
      "692 : loss:  0.7065449 \t acc:  0.65384614\n",
      "694 : loss:  0.44357288 \t acc:  0.86\n",
      "696 : loss:  0.49557063 \t acc:  0.8\n",
      "698 : loss:  0.6352498 \t acc:  0.78\n",
      "700 : loss:  0.33314762 \t acc:  0.91\n",
      "702 : loss:  0.5668261 \t acc:  0.76\n",
      "704 : loss:  0.44321412 \t acc:  0.77\n",
      "706 : loss:  0.49929908 \t acc:  0.84\n",
      "708 : loss:  0.9224145 \t acc:  0.53\n",
      "710 : loss:  0.28100467 \t acc:  0.97\n",
      "712 : loss:  0.5672432 \t acc:  0.77\n",
      "714 : loss:  0.5606922 \t acc:  0.81\n",
      "716 : loss:  0.26933366 \t acc:  0.93\n",
      "718 : loss:  0.6987901 \t acc:  0.67\n",
      "720 : loss:  0.37654096 \t acc:  0.9\n",
      "722 : loss:  0.7516069 \t acc:  0.68\n",
      "724 : loss:  0.4676907 \t acc:  0.89\n",
      "726 : loss:  0.39785343 \t acc:  0.9\n",
      "728 : loss:  0.67903674 \t acc:  0.68\n",
      "730 : loss:  1.0135938 \t acc:  0.59\n",
      "732 : loss:  0.7843292 \t acc:  0.63\n",
      "734 : loss:  0.32408157 \t acc:  0.89\n",
      "736 : loss:  0.6345709 \t acc:  0.71\n",
      "738 : loss:  0.34694797 \t acc:  0.92\n",
      "740 : loss:  0.3639716 \t acc:  0.9\n",
      "742 : loss:  0.42642936 \t acc:  0.83\n",
      "744 : loss:  0.57514024 \t acc:  0.75\n",
      "746 : loss:  0.5853384 \t acc:  0.8\n",
      "748 : loss:  0.57744634 \t acc:  0.76\n",
      "750 : loss:  0.6105207 \t acc:  0.73\n",
      "752 : loss:  0.40947434 \t acc:  0.87\n",
      "754 : loss:  0.4601801 \t acc:  0.84\n",
      "756 : loss:  1.1839445 \t acc:  0.49\n",
      "758 : loss:  0.65033233 \t acc:  0.77\n",
      "760 : loss:  0.42668584 \t acc:  0.88\n",
      "762 : loss:  0.5898333 \t acc:  0.75\n",
      "764 : loss:  0.5904501 \t acc:  0.74\n",
      "766 : loss:  0.6741357 \t acc:  0.69\n",
      "768 : loss:  0.5310359 \t acc:  0.78\n",
      "770 : loss:  0.5562499 \t acc:  0.77\n",
      "772 : loss:  0.71405625 \t acc:  0.67\n",
      "774 : loss:  0.51780856 \t acc:  0.82\n",
      "776 : loss:  0.6470291 \t acc:  0.76\n",
      "778 : loss:  0.47933483 \t acc:  0.78\n",
      "780 : loss:  0.2873961 \t acc:  0.93\n",
      "782 : loss:  0.609345 \t acc:  0.75\n",
      "784 : loss:  0.33866778 \t acc:  0.88\n",
      "786 : loss:  0.54708326 \t acc:  0.78\n",
      "788 : loss:  0.46400085 \t acc:  0.82\n",
      "790 : loss:  0.463298 \t acc:  0.85\n",
      "792 : loss:  0.51242363 \t acc:  0.81\n",
      "794 : loss:  0.41654012 \t acc:  0.86\n",
      "796 : loss:  0.3878302 \t acc:  0.87\n",
      "798 : loss:  0.23389988 \t acc:  0.93\n",
      "800 : loss:  0.5048669 \t acc:  0.86\n",
      "802 : loss:  0.37452495 \t acc:  0.88\n",
      "\n",
      "Saving...\n",
      "saved to models/CNN/pretrained_model.ckpt-804\n",
      "\n",
      "70 \t [ 8.80217838  0.         63.4931619 ]\n",
      "3 \tval accuracy:  0.83085716 \t f_! score:  [0.12574541 0.         0.90704517]\n",
      "\n",
      "804 : loss:  0.31977895 \t acc:  0.97\n",
      "806 : loss:  0.41029102 \t acc:  0.89\n",
      "808 : loss:  0.3251046 \t acc:  0.89\n",
      "810 : loss:  0.4168356 \t acc:  0.82\n",
      "812 : loss:  0.43887985 \t acc:  0.84\n",
      "814 : loss:  0.3635198 \t acc:  0.86\n",
      "816 : loss:  0.659234 \t acc:  0.7\n",
      "818 : loss:  0.6164664 \t acc:  0.77\n",
      "820 : loss:  0.6791251 \t acc:  0.7\n",
      "822 : loss:  0.5997454 \t acc:  0.73\n",
      "824 : loss:  0.31672275 \t acc:  0.89\n",
      "826 : loss:  0.17212014 \t acc:  0.96\n",
      "828 : loss:  0.7079872 \t acc:  0.64\n",
      "830 : loss:  0.47603816 \t acc:  0.83\n",
      "832 : loss:  0.7192094 \t acc:  0.67\n",
      "834 : loss:  0.73295957 \t acc:  0.66\n",
      "836 : loss:  0.24151057 \t acc:  0.93\n",
      "838 : loss:  0.19255742 \t acc:  0.95\n",
      "840 : loss:  0.9844839 \t acc:  0.58\n",
      "842 : loss:  0.6061973 \t acc:  0.75\n",
      "844 : loss:  0.4637033 \t acc:  0.82\n",
      "846 : loss:  0.59885645 \t acc:  0.75\n",
      "848 : loss:  0.505627 \t acc:  0.84\n",
      "850 : loss:  0.60576826 \t acc:  0.8\n",
      "852 : loss:  0.5144089 \t acc:  0.79\n",
      "854 : loss:  0.8338094 \t acc:  0.63\n",
      "856 : loss:  0.29494205 \t acc:  0.94\n",
      "858 : loss:  0.50966907 \t acc:  0.78\n",
      "860 : loss:  0.39402112 \t acc:  0.87\n",
      "862 : loss:  0.55760884 \t acc:  0.76\n",
      "864 : loss:  0.46849963 \t acc:  0.83\n",
      "866 : loss:  0.6043213 \t acc:  0.75\n",
      "868 : loss:  0.429675 \t acc:  0.84\n",
      "870 : loss:  0.3295119 \t acc:  0.86\n",
      "872 : loss:  0.56475735 \t acc:  0.77\n",
      "874 : loss:  0.41874444 \t acc:  0.84\n",
      "876 : loss:  0.46036565 \t acc:  0.81\n",
      "878 : loss:  0.54243255 \t acc:  0.76\n",
      "880 : loss:  0.61334443 \t acc:  0.71\n",
      "882 : loss:  0.5384501 \t acc:  0.79\n",
      "884 : loss:  0.53055394 \t acc:  0.79\n",
      "886 : loss:  0.5165578 \t acc:  0.82\n",
      "888 : loss:  0.5434608 \t acc:  0.78\n",
      "890 : loss:  0.53005576 \t acc:  0.84\n",
      "892 : loss:  0.9096062 \t acc:  0.62\n",
      "894 : loss:  0.6344293 \t acc:  0.75\n",
      "896 : loss:  0.6122793 \t acc:  0.74\n",
      "898 : loss:  0.7465084 \t acc:  0.7\n",
      "900 : loss:  0.3731255 \t acc:  0.83\n",
      "902 : loss:  0.3643119 \t acc:  0.9\n",
      "904 : loss:  0.5679016 \t acc:  0.79\n",
      "906 : loss:  0.26547644 \t acc:  0.95\n",
      "908 : loss:  0.25451452 \t acc:  0.97\n",
      "910 : loss:  0.7875798 \t acc:  0.6\n",
      "912 : loss:  0.5374896 \t acc:  0.8\n",
      "914 : loss:  0.61716753 \t acc:  0.73\n",
      "916 : loss:  0.41199222 \t acc:  0.82\n",
      "918 : loss:  0.70681137 \t acc:  0.75\n",
      "920 : loss:  0.63844186 \t acc:  0.76\n",
      "922 : loss:  0.8762182 \t acc:  0.61\n",
      "924 : loss:  0.4303175 \t acc:  0.86\n",
      "926 : loss:  0.5229011 \t acc:  0.8\n",
      "928 : loss:  0.14777833 \t acc:  0.95\n",
      "930 : loss:  1.0076113 \t acc:  0.58\n",
      "932 : loss:  0.33683544 \t acc:  0.88\n",
      "934 : loss:  0.4333117 \t acc:  0.84\n",
      "936 : loss:  0.35331458 \t acc:  0.9\n",
      "938 : loss:  0.48826718 \t acc:  0.8\n",
      "940 : loss:  0.5278114 \t acc:  0.85\n",
      "942 : loss:  0.5797476 \t acc:  0.77\n",
      "944 : loss:  0.79099685 \t acc:  0.65\n",
      "946 : loss:  0.555225 \t acc:  0.81\n",
      "948 : loss:  0.35771093 \t acc:  0.89\n",
      "950 : loss:  0.54027987 \t acc:  0.82\n",
      "952 : loss:  0.49905816 \t acc:  0.81\n",
      "954 : loss:  0.48005185 \t acc:  0.81\n",
      "956 : loss:  0.7391777 \t acc:  0.65\n",
      "958 : loss:  0.53154176 \t acc:  0.81\n",
      "960 : loss:  0.78001493 \t acc:  0.62\n",
      "962 : loss:  0.5127014 \t acc:  0.82\n",
      "964 : loss:  0.54778826 \t acc:  0.77\n",
      "966 : loss:  0.9354329 \t acc:  0.62\n",
      "968 : loss:  0.2947007 \t acc:  0.91\n",
      "970 : loss:  0.8944043 \t acc:  0.61\n",
      "972 : loss:  0.42253104 \t acc:  0.84\n",
      "974 : loss:  0.47057402 \t acc:  0.78\n",
      "976 : loss:  0.66369194 \t acc:  0.68\n",
      "978 : loss:  0.3774218 \t acc:  0.86\n",
      "980 : loss:  0.46423355 \t acc:  0.82\n",
      "982 : loss:  0.4253765 \t acc:  0.86\n",
      "984 : loss:  0.31772983 \t acc:  0.91\n",
      "986 : loss:  0.73815376 \t acc:  0.67\n",
      "988 : loss:  0.7765531 \t acc:  0.68\n",
      "990 : loss:  0.4899746 \t acc:  0.8\n",
      "992 : loss:  0.5084801 \t acc:  0.76\n",
      "994 : loss:  0.23885018 \t acc:  0.9\n",
      "996 : loss:  0.37609315 \t acc:  0.88\n",
      "998 : loss:  0.6881498 \t acc:  0.71\n",
      "1000 : loss:  0.37868938 \t acc:  0.85\n",
      "1002 : loss:  0.7823629 \t acc:  0.63\n",
      "1004 : loss:  0.65807235 \t acc:  0.76\n",
      "\n",
      "Saving...\n",
      "saved to models/CNN/pretrained_model.ckpt-1005\n",
      "\n",
      "70 \t [23.59489121  0.         63.87297538]\n",
      "4 \tval accuracy:  0.83428574 \t f_! score:  [0.33706987 0.         0.91247108]\n",
      "\n",
      "1006 : loss:  0.5888959 \t acc:  0.73\n",
      "1008 : loss:  0.8152458 \t acc:  0.69\n",
      "1010 : loss:  0.6923044 \t acc:  0.72\n",
      "1012 : loss:  0.9857601 \t acc:  0.53\n",
      "1014 : loss:  0.28101256 \t acc:  0.93\n",
      "1016 : loss:  0.8133199 \t acc:  0.61\n",
      "1018 : loss:  0.40690586 \t acc:  0.81\n",
      "1020 : loss:  0.5276968 \t acc:  0.77\n",
      "1022 : loss:  0.19978264 \t acc:  0.95\n",
      "1024 : loss:  0.48932037 \t acc:  0.82\n",
      "1026 : loss:  0.3045964 \t acc:  0.9\n",
      "1028 : loss:  0.4264626 \t acc:  0.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1030 : loss:  0.99037886 \t acc:  0.49\n",
      "1032 : loss:  0.6803627 \t acc:  0.67\n",
      "1034 : loss:  0.5522858 \t acc:  0.79\n",
      "1036 : loss:  0.58328766 \t acc:  0.79\n",
      "1038 : loss:  0.33579117 \t acc:  0.91\n",
      "1040 : loss:  0.588143 \t acc:  0.73\n",
      "1042 : loss:  0.81202734 \t acc:  0.63\n",
      "1044 : loss:  0.72708356 \t acc:  0.77\n",
      "1046 : loss:  0.26984808 \t acc:  0.92\n",
      "1048 : loss:  0.8179004 \t acc:  0.62\n",
      "1050 : loss:  0.4049388 \t acc:  0.88\n",
      "1052 : loss:  0.6282904 \t acc:  0.74\n",
      "1054 : loss:  0.9138385 \t acc:  0.58\n",
      "1056 : loss:  0.34511143 \t acc:  0.87\n",
      "1058 : loss:  0.36662105 \t acc:  0.88\n",
      "1060 : loss:  0.4720883 \t acc:  0.82\n",
      "1062 : loss:  0.47896767 \t acc:  0.85\n",
      "1064 : loss:  0.3701091 \t acc:  0.91\n",
      "1066 : loss:  0.509281 \t acc:  0.8\n",
      "1068 : loss:  0.43812057 \t acc:  0.86\n",
      "1070 : loss:  0.58482325 \t acc:  0.78\n",
      "1072 : loss:  0.32301524 \t acc:  0.91\n",
      "1074 : loss:  0.66075635 \t acc:  0.71\n",
      "1076 : loss:  0.5136482 \t acc:  0.83\n",
      "1078 : loss:  0.48463142 \t acc:  0.85\n",
      "1080 : loss:  0.41556808 \t acc:  0.86\n",
      "1082 : loss:  0.5536012 \t acc:  0.81\n",
      "1084 : loss:  0.5747723 \t acc:  0.78\n",
      "1086 : loss:  0.43849662 \t acc:  0.82\n",
      "1088 : loss:  0.5030554 \t acc:  0.8\n",
      "1090 : loss:  0.73114794 \t acc:  0.69\n",
      "1092 : loss:  0.59730357 \t acc:  0.79\n",
      "1094 : loss:  0.7575637 \t acc:  0.63\n",
      "1096 : loss:  0.3923384 \t acc:  0.83\n",
      "1098 : loss:  0.12551779 \t acc:  0.95\n",
      "1100 : loss:  0.84331536 \t acc:  0.67\n",
      "1102 : loss:  0.65860987 \t acc:  0.71\n",
      "1104 : loss:  0.47027162 \t acc:  0.84\n",
      "1106 : loss:  0.8077003 \t acc:  0.61\n",
      "1108 : loss:  0.5536309 \t acc:  0.9\n",
      "1110 : loss:  0.6755491 \t acc:  0.73\n",
      "1112 : loss:  0.553224 \t acc:  0.8\n",
      "1114 : loss:  0.40552983 \t acc:  0.83\n",
      "1116 : loss:  0.5689073 \t acc:  0.8\n",
      "1118 : loss:  0.56985694 \t acc:  0.79\n",
      "1120 : loss:  0.44807184 \t acc:  0.85\n",
      "1122 : loss:  0.7682405 \t acc:  0.7\n",
      "1124 : loss:  0.3638428 \t acc:  0.89\n",
      "1126 : loss:  0.52008826 \t acc:  0.83\n",
      "1128 : loss:  0.7043965 \t acc:  0.67\n",
      "1130 : loss:  0.30028188 \t acc:  0.9\n",
      "1132 : loss:  0.44840455 \t acc:  0.81\n",
      "1134 : loss:  0.7264351 \t acc:  0.65384614\n",
      "1136 : loss:  0.37588143 \t acc:  0.86\n",
      "1138 : loss:  0.4787759 \t acc:  0.8\n",
      "1140 : loss:  0.6571576 \t acc:  0.7\n",
      "1142 : loss:  0.35646346 \t acc:  0.88\n",
      "1144 : loss:  0.80686355 \t acc:  0.66\n",
      "1146 : loss:  0.39349088 \t acc:  0.87\n",
      "1148 : loss:  0.48477224 \t acc:  0.84\n",
      "1150 : loss:  0.5204926 \t acc:  0.83\n",
      "1152 : loss:  0.40538147 \t acc:  0.82\n",
      "1154 : loss:  0.5055878 \t acc:  0.82\n",
      "1156 : loss:  0.39612475 \t acc:  0.84\n",
      "1158 : loss:  0.7492594 \t acc:  0.67\n",
      "1160 : loss:  0.45791358 \t acc:  0.81\n",
      "1162 : loss:  0.5064218 \t acc:  0.84\n",
      "1164 : loss:  0.41629776 \t acc:  0.82\n",
      "1166 : loss:  0.42994052 \t acc:  0.83\n",
      "1168 : loss:  0.479561 \t acc:  0.84\n",
      "1170 : loss:  0.5061554 \t acc:  0.8\n",
      "1172 : loss:  0.37978122 \t acc:  0.86\n",
      "1174 : loss:  0.38108802 \t acc:  0.85\n",
      "1176 : loss:  0.5572331 \t acc:  0.78\n",
      "1178 : loss:  0.5680085 \t acc:  0.75\n",
      "1180 : loss:  0.4348849 \t acc:  0.85\n",
      "1182 : loss:  0.45215598 \t acc:  0.81\n",
      "1184 : loss:  0.47956258 \t acc:  0.8\n",
      "1186 : loss:  0.47725987 \t acc:  0.82\n",
      "1188 : loss:  0.7104456 \t acc:  0.74\n",
      "1190 : loss:  0.45142213 \t acc:  0.85\n",
      "1192 : loss:  0.5691084 \t acc:  0.75\n",
      "1194 : loss:  0.9819229 \t acc:  0.62\n",
      "1196 : loss:  0.4856375 \t acc:  0.84\n",
      "1198 : loss:  0.53608 \t acc:  0.81\n",
      "1200 : loss:  0.7508961 \t acc:  0.7\n",
      "1202 : loss:  0.33527753 \t acc:  0.91\n",
      "1204 : loss:  0.45569205 \t acc:  0.84\n",
      "\n",
      "Saving...\n",
      "saved to models/CNN/pretrained_model.ckpt-1206\n",
      "\n",
      "70 \t [24.64507441  0.         63.80849314]\n",
      "5 \tval accuracy:  0.828 \t f_! score:  [0.35207249 0.         0.9115499 ]\n",
      "\n",
      "1206 : loss:  0.32732993 \t acc:  0.89\n",
      "1208 : loss:  0.2889346 \t acc:  0.94\n",
      "1210 : loss:  0.1620243 \t acc:  0.96\n",
      "1212 : loss:  0.9274721 \t acc:  0.65384614\n",
      "1214 : loss:  0.4372286 \t acc:  0.83\n",
      "1216 : loss:  0.46546704 \t acc:  0.81\n",
      "1218 : loss:  0.3631067 \t acc:  0.84\n",
      "1220 : loss:  0.32363194 \t acc:  0.92\n",
      "1222 : loss:  0.1550052 \t acc:  0.96\n",
      "1224 : loss:  0.287774 \t acc:  0.89\n",
      "1226 : loss:  0.696297 \t acc:  0.67\n",
      "1228 : loss:  0.5993801 \t acc:  0.74\n",
      "1230 : loss:  0.32256767 \t acc:  0.91\n",
      "1232 : loss:  0.645782 \t acc:  0.68\n",
      "1234 : loss:  0.65804774 \t acc:  0.71\n",
      "1236 : loss:  0.62313247 \t acc:  0.74\n",
      "1238 : loss:  0.45836198 \t acc:  0.8\n",
      "1240 : loss:  0.42005962 \t acc:  0.81\n",
      "1242 : loss:  0.49959657 \t acc:  0.81\n",
      "1244 : loss:  0.72197825 \t acc:  0.64\n",
      "1246 : loss:  0.54394096 \t acc:  0.78\n",
      "1248 : loss:  0.40744987 \t acc:  0.79\n",
      "1250 : loss:  0.7374829 \t acc:  0.65\n",
      "1252 : loss:  0.18504068 \t acc:  0.95\n",
      "1254 : loss:  0.2803338 \t acc:  0.89\n",
      "1256 : loss:  0.41439423 \t acc:  0.86\n",
      "1258 : loss:  0.332674 \t acc:  0.89\n",
      "1260 : loss:  0.43528077 \t acc:  0.83\n",
      "1262 : loss:  0.45433974 \t acc:  0.83\n",
      "1264 : loss:  0.8025249 \t acc:  0.67\n",
      "1266 : loss:  0.56817734 \t acc:  0.72\n",
      "1268 : loss:  0.5236554 \t acc:  0.75\n",
      "1270 : loss:  0.483577 \t acc:  0.79\n",
      "1272 : loss:  0.3995393 \t acc:  0.88\n",
      "1274 : loss:  0.6873946 \t acc:  0.69\n",
      "1276 : loss:  0.4075756 \t acc:  0.84\n",
      "1278 : loss:  0.6991808 \t acc:  0.76\n",
      "1280 : loss:  0.91476226 \t acc:  0.63\n",
      "1282 : loss:  0.75146466 \t acc:  0.63\n",
      "1284 : loss:  0.5414111 \t acc:  0.79\n",
      "1286 : loss:  0.7495627 \t acc:  0.67\n",
      "1288 : loss:  0.26164857 \t acc:  0.91\n",
      "1290 : loss:  0.54190105 \t acc:  0.79\n",
      "1292 : loss:  0.49212882 \t acc:  0.8\n",
      "1294 : loss:  0.6034333 \t acc:  0.77\n",
      "1296 : loss:  0.3327393 \t acc:  0.85\n",
      "1298 : loss:  0.3161946 \t acc:  0.9\n",
      "1300 : loss:  0.34381604 \t acc:  0.9\n",
      "1302 : loss:  0.44719487 \t acc:  0.85\n",
      "1304 : loss:  0.5514746 \t acc:  0.77\n",
      "1306 : loss:  0.5669021 \t acc:  0.76\n",
      "1308 : loss:  0.5410532 \t acc:  0.76\n",
      "1310 : loss:  0.39432284 \t acc:  0.81\n",
      "1312 : loss:  0.41325256 \t acc:  0.8\n",
      "1314 : loss:  0.46731558 \t acc:  0.78\n",
      "1316 : loss:  0.7007836 \t acc:  0.67\n",
      "1318 : loss:  0.41046417 \t acc:  0.82\n",
      "1320 : loss:  0.5747138 \t acc:  0.79\n",
      "1322 : loss:  0.65682334 \t acc:  0.68\n",
      "1324 : loss:  0.7410704 \t acc:  0.69\n",
      "1326 : loss:  0.3262494 \t acc:  0.9\n",
      "1328 : loss:  0.24021019 \t acc:  0.9\n",
      "1330 : loss:  0.82169306 \t acc:  0.68\n",
      "1332 : loss:  0.45453507 \t acc:  0.82\n",
      "1334 : loss:  0.459006 \t acc:  0.83\n",
      "1336 : loss:  0.8355276 \t acc:  0.71\n",
      "1338 : loss:  0.37749463 \t acc:  0.85\n",
      "1340 : loss:  0.41527638 \t acc:  0.84\n",
      "1342 : loss:  0.48850268 \t acc:  0.8\n",
      "1344 : loss:  0.48746672 \t acc:  0.86\n",
      "1346 : loss:  0.5479297 \t acc:  0.8\n",
      "1348 : loss:  0.75031435 \t acc:  0.68\n",
      "1350 : loss:  0.63271874 \t acc:  0.77\n",
      "1352 : loss:  0.50823927 \t acc:  0.78\n",
      "1354 : loss:  0.56826603 \t acc:  0.73\n",
      "1356 : loss:  0.41300717 \t acc:  0.87\n",
      "1358 : loss:  0.6395139 \t acc:  0.69\n",
      "1360 : loss:  0.3491181 \t acc:  0.9\n",
      "1362 : loss:  0.54229707 \t acc:  0.8\n",
      "1364 : loss:  0.6968604 \t acc:  0.72\n",
      "1366 : loss:  0.52359277 \t acc:  0.78\n",
      "1368 : loss:  0.29261574 \t acc:  0.91\n",
      "1370 : loss:  0.6757747 \t acc:  0.72\n",
      "1372 : loss:  0.5809978 \t acc:  0.74\n",
      "1374 : loss:  0.5281718 \t acc:  0.81\n",
      "1376 : loss:  0.5443831 \t acc:  0.86\n",
      "1378 : loss:  0.51472354 \t acc:  0.82\n",
      "1380 : loss:  0.3246645 \t acc:  0.9\n",
      "1382 : loss:  0.6503402 \t acc:  0.71\n",
      "1384 : loss:  0.20443764 \t acc:  0.96\n",
      "1386 : loss:  0.48737794 \t acc:  0.77\n",
      "1388 : loss:  0.34730476 \t acc:  0.91\n",
      "1390 : loss:  0.5299173 \t acc:  0.79\n",
      "1392 : loss:  0.40131268 \t acc:  0.88\n",
      "1394 : loss:  0.53102773 \t acc:  0.81\n",
      "1396 : loss:  0.43175262 \t acc:  0.88\n",
      "1398 : loss:  0.48757115 \t acc:  0.82\n",
      "1400 : loss:  0.51447994 \t acc:  0.8\n",
      "1402 : loss:  0.5675203 \t acc:  0.76\n",
      "1404 : loss:  0.16973735 \t acc:  0.95\n",
      "1406 : loss:  0.58061683 \t acc:  0.81\n",
      "\n",
      "Saving...\n",
      "saved to models/CNN/pretrained_model.ckpt-1407\n",
      "\n",
      "70 \t [24.57121861  0.         64.16660918]\n",
      "6 \tval accuracy:  0.8435715 \t f_! score:  [0.35101741 0.         0.91666585]\n",
      "\n",
      "70 \t [24.57121861  0.         64.16660918]\n",
      "70 \t [30.47081089  0.         60.43180338]\n",
      "70 \t [22.4076504   0.         68.62041306]\n",
      "70 \t [ 501.  664. 5835.]\n",
      "--- Test   Prime ---\n",
      "0.8435715\n",
      "f1:  [0.35101741 0.         0.91666585]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yannik/ba/TestHelper.py:120: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  input_data = self.decide_which_input(input_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509411\n",
      "Vorsicht! es muss vorher ein  CNN  Netz abgespeichert worden sein\n",
      "\n",
      "Restoring...\n",
      "models/CNN/pretrained_model.ckpt-1407\n",
      "INFO:tensorflow:Restoring parameters from models/CNN/pretrained_model.ckpt-1407\n",
      "acc:  0.28\n",
      "acc:  0.41\n",
      "acc:  0.37\n",
      "acc:  0.36\n",
      "acc:  0.35\n",
      "acc:  0.26\n",
      "acc:  0.36\n",
      "acc:  0.32\n",
      "acc:  0.35\n",
      "acc:  0.3\n",
      "acc:  0.33\n",
      "acc:  0.38\n",
      "acc:  0.35\n",
      "acc:  0.38\n",
      "acc:  0.33\n",
      "acc:  0.37\n",
      "acc:  0.31\n",
      "acc:  0.38\n",
      "acc:  0.35\n",
      "acc:  0.3\n",
      "acc:  0.37\n",
      "acc:  0.44\n",
      "acc:  0.39\n",
      "acc:  0.41\n",
      "acc:  0.43\n",
      "acc:  0.38\n",
      "acc:  0.4\n",
      "acc:  0.4\n",
      "acc:  0.28\n",
      "acc:  0.33\n",
      "acc:  0.37\n",
      "acc:  0.39\n",
      "acc:  0.39\n",
      "acc:  0.37\n",
      "acc:  0.38\n",
      "acc:  0.39\n",
      "acc:  0.4\n",
      "acc:  0.43\n",
      "acc:  0.35\n",
      "acc:  0.35\n",
      "acc:  0.37\n",
      "acc:  0.38\n",
      "acc:  0.36\n",
      "acc:  0.44\n",
      "acc:  0.43\n",
      "acc:  0.39\n",
      "acc:  0.38\n",
      "acc:  0.36\n",
      "acc:  0.34\n",
      "acc:  0.35\n",
      "acc:  0.38\n",
      "acc:  0.41\n",
      "acc:  0.31\n",
      "acc:  0.43\n",
      "acc:  0.35\n",
      "acc:  0.42\n",
      "acc:  0.43\n",
      "acc:  0.32\n",
      "acc:  0.51\n",
      "acc:  0.43\n",
      "acc:  0.33\n",
      "acc:  0.27\n",
      "acc:  0.33\n",
      "acc:  0.45\n",
      "acc:  0.33\n",
      "acc:  0.41\n",
      "acc:  0.37\n",
      "acc:  0.39\n",
      "acc:  0.35\n",
      "acc:  0.39\n",
      "acc:  0.36\n",
      "acc:  0.42\n",
      "acc:  0.38\n",
      "acc:  0.31\n",
      "acc:  0.35\n",
      "acc:  0.29\n",
      "acc:  0.44\n",
      "acc:  0.37\n",
      "acc:  0.29\n",
      "acc:  0.36\n",
      "acc:  0.35\n",
      "acc:  0.38\n",
      "acc:  0.36\n",
      "acc:  0.37\n",
      "acc:  0.38\n",
      "acc:  0.42\n",
      "acc:  0.42\n",
      "acc:  0.38\n",
      "acc:  0.35\n",
      "acc:  0.44\n",
      "acc:  0.32\n",
      "acc:  0.42\n",
      "acc:  0.43\n",
      "acc:  0.37\n",
      "acc:  0.34\n",
      "acc:  0.42\n",
      "acc:  0.42\n",
      "acc:  0.33\n",
      "acc:  0.44\n",
      "acc:  0.45\n",
      "acc:  0.31\n",
      "acc:  0.48\n",
      "acc:  0.39\n",
      "acc:  0.39\n",
      "acc:  0.44\n",
      "acc:  0.4\n",
      "acc:  0.45\n",
      "acc:  0.43\n",
      "acc:  0.39\n",
      "acc:  0.39\n",
      "acc:  0.4\n",
      "acc:  0.4\n",
      "acc:  0.36\n",
      "acc:  0.35\n",
      "acc:  0.4\n",
      "acc:  0.41\n",
      "acc:  0.44\n",
      "acc:  0.48\n",
      "acc:  0.37\n",
      "acc:  0.39\n",
      "acc:  0.44\n",
      "acc:  0.4\n",
      "acc:  0.33\n",
      "acc:  0.4\n",
      "acc:  0.42\n",
      "acc:  0.36\n",
      "acc:  0.47\n",
      "acc:  0.34\n",
      "acc:  0.4\n",
      "acc:  0.4\n",
      "acc:  0.41\n",
      "acc:  0.45\n",
      "acc:  0.39\n",
      "acc:  0.4\n",
      "acc:  0.43\n",
      "acc:  0.32\n",
      "acc:  0.34\n",
      "acc:  0.43\n",
      "acc:  0.32\n",
      "acc:  0.4\n",
      "acc:  0.31\n",
      "acc:  0.42\n",
      "acc:  0.3\n",
      "acc:  0.41\n",
      "acc:  0.34\n",
      "acc:  0.4\n",
      "acc:  0.32\n",
      "acc:  0.39\n",
      "acc:  0.3\n",
      "acc:  0.35\n",
      "acc:  0.36\n",
      "acc:  0.27\n",
      "acc:  0.32\n",
      "acc:  0.42\n",
      "acc:  0.35\n",
      "acc:  0.34\n",
      "acc:  0.38\n",
      "acc:  0.38\n",
      "acc:  0.35\n",
      "acc:  0.35\n",
      "acc:  0.41\n",
      "acc:  0.37\n",
      "acc:  0.36\n",
      "acc:  0.35\n",
      "acc:  0.35\n",
      "acc:  0.34\n",
      "acc:  0.45\n",
      "acc:  0.41\n",
      "acc:  0.35\n",
      "acc:  0.28\n",
      "acc:  0.38\n",
      "acc:  0.34\n",
      "acc:  0.45\n",
      "acc:  0.34\n",
      "acc:  0.4\n",
      "acc:  0.36\n",
      "acc:  0.4\n",
      "acc:  0.35\n",
      "acc:  0.35\n",
      "acc:  0.34\n",
      "acc:  0.36\n",
      "acc:  0.38\n",
      "acc:  0.43\n",
      "acc:  0.32\n",
      "acc:  0.39\n",
      "acc:  0.34\n",
      "acc:  0.38\n",
      "acc:  0.36\n",
      "acc:  0.36\n",
      "acc:  0.43\n",
      "acc:  0.31\n",
      "acc:  0.41\n",
      "acc:  0.38\n",
      "acc:  0.43\n",
      "acc:  0.41\n",
      "acc:  0.36\n",
      "acc:  0.38\n",
      "acc:  0.37\n",
      "acc:  0.39\n",
      "acc:  0.34\n",
      "acc:  0.42\n",
      "acc:  0.34\n",
      "acc:  0.41\n",
      "acc:  0.39\n",
      "acc:  0.37\n",
      "acc:  0.38\n",
      "acc:  0.4\n",
      "acc:  0.4\n",
      "acc:  0.33\n",
      "acc:  0.3\n",
      "acc:  0.39\n",
      "acc:  0.43\n",
      "acc:  0.37\n",
      "acc:  0.44\n",
      "acc:  0.39\n",
      "acc:  0.35\n",
      "acc:  0.38\n",
      "acc:  0.4\n",
      "acc:  0.35\n",
      "acc:  0.24\n",
      "acc:  0.4\n",
      "acc:  0.35\n",
      "acc:  0.39\n",
      "acc:  0.45\n",
      "acc:  0.32\n",
      "acc:  0.43\n",
      "acc:  0.47\n",
      "acc:  0.33\n",
      "acc:  0.34\n",
      "acc:  0.42\n",
      "acc:  0.37\n",
      "acc:  0.4\n",
      "acc:  0.38\n",
      "acc:  0.31\n",
      "acc:  0.32\n",
      "acc:  0.36\n",
      "acc:  0.29\n",
      "acc:  0.4\n",
      "acc:  0.41\n",
      "acc:  0.36\n",
      "acc:  0.37\n",
      "acc:  0.41\n",
      "acc:  0.4\n",
      "acc:  0.41\n",
      "acc:  0.39\n",
      "acc:  0.39\n",
      "acc:  0.35\n",
      "acc:  0.32\n",
      "acc:  0.35\n",
      "acc:  0.4\n",
      "acc:  0.47\n",
      "acc:  0.4\n",
      "acc:  0.39\n",
      "acc:  0.35\n",
      "acc:  0.35\n",
      "acc:  0.34\n",
      "acc:  0.31\n",
      "acc:  0.49\n",
      "acc:  0.37\n",
      "acc:  0.41\n",
      "acc:  0.33\n",
      "acc:  0.3\n",
      "acc:  0.31\n",
      "acc:  0.37\n",
      "acc:  0.34\n",
      "acc:  0.39\n",
      "acc:  0.38\n",
      "acc:  0.34\n",
      "acc:  0.49\n",
      "acc:  0.38\n",
      "acc:  0.39\n",
      "acc:  0.44\n",
      "acc:  0.33\n",
      "acc:  0.41\n",
      "acc:  0.37\n",
      "acc:  0.42\n",
      "acc:  0.39\n",
      "acc:  0.28\n",
      "acc:  0.28\n",
      "acc:  0.33\n",
      "acc:  0.37\n",
      "acc:  0.25\n",
      "acc:  0.31\n",
      "acc:  0.42\n",
      "acc:  0.36\n",
      "acc:  0.46\n",
      "acc:  0.35\n",
      "acc:  0.33\n",
      "acc:  0.44\n",
      "acc:  0.44\n",
      "acc:  0.29\n",
      "acc:  0.39\n",
      "acc:  0.3\n",
      "acc:  0.34\n",
      "acc:  0.4\n",
      "acc:  0.33\n",
      "acc:  0.44\n",
      "acc:  0.4\n",
      "acc:  0.36\n",
      "acc:  0.46\n",
      "acc:  0.37\n",
      "acc:  0.24\n",
      "acc:  0.28\n",
      "acc:  0.33\n",
      "acc:  0.47\n",
      "acc:  0.45\n",
      "acc:  0.4\n",
      "acc:  0.42\n",
      "acc:  0.34\n",
      "acc:  0.42\n",
      "acc:  0.38\n",
      "acc:  0.37\n",
      "acc:  0.37\n",
      "acc:  0.35\n",
      "acc:  0.37\n",
      "acc:  0.4\n",
      "acc:  0.38\n",
      "acc:  0.4\n",
      "acc:  0.36\n",
      "acc:  0.45\n",
      "acc:  0.38\n",
      "acc:  0.42\n",
      "acc:  0.37\n",
      "acc:  0.41\n",
      "acc:  0.47\n",
      "acc:  0.44\n",
      "acc:  0.35\n",
      "acc:  0.44\n",
      "acc:  0.34\n",
      "acc:  0.34\n",
      "acc:  0.43\n",
      "acc:  0.29\n",
      "acc:  0.34\n",
      "acc:  0.27\n",
      "acc:  0.37\n",
      "acc:  0.4\n",
      "acc:  0.33\n",
      "acc:  0.37\n",
      "acc:  0.32\n",
      "acc:  0.36\n",
      "acc:  0.37\n",
      "acc:  0.36\n",
      "acc:  0.41\n",
      "acc:  0.38\n",
      "acc:  0.25\n",
      "acc:  0.37\n",
      "acc:  0.42\n",
      "acc:  0.34\n",
      "acc:  0.41\n",
      "acc:  0.43\n",
      "acc:  0.5\n",
      "acc:  0.41\n",
      "acc:  0.33\n",
      "acc:  0.35\n",
      "acc:  0.38\n",
      "acc:  0.35\n",
      "acc:  0.36\n",
      "acc:  0.45\n",
      "acc:  0.42\n",
      "acc:  0.33\n",
      "acc:  0.38\n",
      "acc:  0.41\n",
      "acc:  0.41\n",
      "acc:  0.39\n",
      "acc:  0.37\n",
      "acc:  0.33\n",
      "acc:  0.41\n",
      "acc:  0.41\n",
      "acc:  0.37\n",
      "acc:  0.32\n",
      "acc:  0.36\n",
      "acc:  0.32\n",
      "acc:  0.53\n",
      "acc:  0.33\n",
      "acc:  0.37\n",
      "acc:  0.36\n",
      "acc:  0.33\n",
      "acc:  0.32\n",
      "acc:  0.31\n",
      "acc:  0.35\n",
      "acc:  0.4\n",
      "acc:  0.38\n",
      "acc:  0.42\n",
      "acc:  0.39\n",
      "acc:  0.35\n",
      "acc:  0.45\n",
      "acc:  0.4\n",
      "acc:  0.31\n",
      "acc:  0.41\n",
      "acc:  0.37\n",
      "acc:  0.31\n",
      "acc:  0.42\n",
      "acc:  0.38\n",
      "acc:  0.32\n",
      "acc:  0.45\n",
      "acc:  0.36\n",
      "acc:  0.44\n",
      "acc:  0.41\n",
      "acc:  0.42\n",
      "acc:  0.24\n",
      "acc:  0.32\n",
      "acc:  0.32\n",
      "acc:  0.45\n",
      "acc:  0.35\n",
      "acc:  0.34\n",
      "acc:  0.38\n",
      "acc:  0.36\n",
      "acc:  0.29\n",
      "acc:  0.39\n",
      "acc:  0.46\n",
      "acc:  0.41\n",
      "acc:  0.35\n",
      "acc:  0.37\n",
      "acc:  0.39\n",
      "acc:  0.42\n",
      "acc:  0.31\n",
      "acc:  0.35\n",
      "acc:  0.29\n",
      "acc:  0.31\n",
      "acc:  0.37\n",
      "acc:  0.38\n",
      "acc:  0.33\n",
      "acc:  0.33\n",
      "acc:  0.41\n",
      "acc:  0.29\n",
      "acc:  0.36\n",
      "acc:  0.35\n",
      "acc:  0.42\n",
      "acc:  0.4\n",
      "acc:  0.36\n",
      "acc:  0.43\n",
      "acc:  0.39\n",
      "acc:  0.37\n",
      "acc:  0.37\n",
      "acc:  0.41\n",
      "acc:  0.32\n",
      "acc:  0.39\n",
      "acc:  0.42\n",
      "acc:  0.47\n",
      "acc:  0.35\n",
      "acc:  0.34\n",
      "acc:  0.4\n",
      "acc:  0.37\n",
      "acc:  0.39\n",
      "acc:  0.38\n",
      "acc:  0.28\n",
      "acc:  0.37\n",
      "acc:  0.44\n",
      "acc:  0.41\n",
      "acc:  0.43\n",
      "acc:  0.34\n",
      "acc:  0.47\n",
      "acc:  0.4\n",
      "acc:  0.4\n",
      "acc:  0.29\n",
      "acc:  0.39\n",
      "acc:  0.35\n",
      "acc:  0.34\n",
      "acc:  0.45\n",
      "acc:  0.33\n",
      "acc:  0.42\n",
      "acc:  0.36\n",
      "acc:  0.36\n",
      "acc:  0.44\n",
      "acc:  0.42\n",
      "acc:  0.36\n",
      "acc:  0.41\n",
      "acc:  0.36\n",
      "acc:  0.36\n",
      "acc:  0.39\n",
      "acc:  0.42\n",
      "acc:  0.39\n",
      "acc:  0.37\n",
      "acc:  0.38\n",
      "acc:  0.44\n",
      "acc:  0.33\n",
      "acc:  0.44\n",
      "acc:  0.41\n",
      "acc:  0.32\n",
      "acc:  0.37\n",
      "acc:  0.4\n",
      "acc:  0.35\n",
      "acc:  0.37\n",
      "acc:  0.43\n",
      "acc:  0.36\n",
      "acc:  0.43\n",
      "acc:  0.37\n",
      "acc:  0.43\n",
      "acc:  0.37\n",
      "acc:  0.41\n",
      "acc:  0.39\n",
      "acc:  0.43\n",
      "acc:  0.43\n",
      "acc:  0.38\n",
      "acc:  0.35\n",
      "acc:  0.32\n",
      "acc:  0.32\n",
      "acc:  0.43\n",
      "acc:  0.43\n",
      "acc:  0.31\n",
      "\n",
      "500 \t [272.32631982   0.           0.        ]\n",
      "val accuracy:  0.37588 \t f_! score:  [0.54465264 0.         0.        ]\n",
      "\n",
      "500 \t [272.32631982   0.           0.        ]\n",
      "500 \t [187.94   0.     0.  ]\n",
      "500 \t [500.   0.   0.]\n",
      "500 \t [18794. 13823. 17383.]\n",
      "---just Test  Review ---\n",
      "0.37588\n",
      "f1:  [0.54465264 0.         0.        ]\n",
      "65730\n",
      "Vorsicht! es muss vorher ein  CNN  Netz abgespeichert worden sein\n",
      "\n",
      "Restoring...\n",
      "models/CNN/pretrained_model.ckpt-1407\n",
      "INFO:tensorflow:Restoring parameters from models/CNN/pretrained_model.ckpt-1407\n",
      "acc:  0.28\n",
      "acc:  0.35\n",
      "acc:  0.35\n",
      "acc:  0.3\n",
      "acc:  0.34\n",
      "acc:  0.34\n",
      "acc:  0.26\n",
      "acc:  0.37\n",
      "acc:  0.39\n",
      "acc:  0.36\n",
      "acc:  0.38\n",
      "acc:  0.36\n",
      "acc:  0.33\n",
      "acc:  0.3\n",
      "acc:  0.29\n",
      "acc:  0.26\n",
      "acc:  0.37\n",
      "acc:  0.35\n",
      "acc:  0.34\n",
      "acc:  0.37\n",
      "acc:  0.36\n",
      "acc:  0.24\n",
      "acc:  0.36\n",
      "acc:  0.39\n",
      "acc:  0.29\n",
      "acc:  0.38\n",
      "acc:  0.33\n",
      "acc:  0.31\n",
      "acc:  0.34\n",
      "acc:  0.35\n",
      "acc:  0.32\n",
      "acc:  0.37\n",
      "acc:  0.4\n",
      "acc:  0.41\n",
      "acc:  0.41\n",
      "acc:  0.38\n",
      "acc:  0.25\n",
      "acc:  0.31\n",
      "acc:  0.34\n",
      "acc:  0.25\n",
      "acc:  0.37\n",
      "acc:  0.42\n",
      "acc:  0.34\n",
      "acc:  0.4\n",
      "acc:  0.44\n",
      "acc:  0.33\n",
      "acc:  0.36\n",
      "acc:  0.27\n",
      "acc:  0.36\n",
      "acc:  0.29\n",
      "acc:  0.32\n",
      "acc:  0.34\n",
      "acc:  0.4\n",
      "acc:  0.28\n",
      "acc:  0.37\n",
      "acc:  0.36\n",
      "acc:  0.38\n",
      "acc:  0.38\n",
      "acc:  0.33\n",
      "acc:  0.37\n",
      "acc:  0.32\n",
      "acc:  0.33\n",
      "acc:  0.35\n",
      "acc:  0.33\n",
      "acc:  0.33\n",
      "acc:  0.35\n",
      "acc:  0.23\n",
      "acc:  0.28\n",
      "acc:  0.37\n",
      "acc:  0.34\n",
      "acc:  0.33\n",
      "acc:  0.35\n",
      "acc:  0.34\n",
      "acc:  0.36\n",
      "acc:  0.36\n",
      "acc:  0.33\n",
      "acc:  0.29\n",
      "acc:  0.33\n",
      "acc:  0.37\n",
      "acc:  0.35\n",
      "acc:  0.32\n",
      "acc:  0.3\n",
      "acc:  0.29\n",
      "acc:  0.36\n",
      "acc:  0.36\n",
      "acc:  0.44\n",
      "acc:  0.32\n",
      "acc:  0.31\n",
      "acc:  0.38\n",
      "acc:  0.38\n",
      "acc:  0.37\n",
      "acc:  0.28\n",
      "acc:  0.28\n",
      "acc:  0.35\n",
      "acc:  0.3\n",
      "acc:  0.37\n",
      "acc:  0.41\n",
      "acc:  0.41\n",
      "acc:  0.47\n",
      "acc:  0.25\n",
      "\n",
      "100 \t [50.23917216  0.          8.39737648]\n",
      "val accuracy:  0.3418 \t f_! score:  [0.50239172 0.         0.08397376]\n",
      "\n",
      "100 \t [50.23917216  0.          8.39737648]\n",
      "100 \t [34.17090123  0.         35.5794733 ]\n",
      "100 \t [96.12961755  0.          4.88571302]\n",
      "100 \t [3387. 3290. 3323.]\n",
      "---just Test  Twitter ---\n",
      "0.3418\n",
      "f1:  [0.50239172 0.         0.08397376]\n",
      "(7733, 3)\n",
      "7733\n",
      "Vorsicht! es muss vorher ein  CNN  Netz abgespeichert worden sein\n",
      "\n",
      "Restoring...\n",
      "models/CNN/pretrained_model.ckpt-1407\n",
      "INFO:tensorflow:Restoring parameters from models/CNN/pretrained_model.ckpt-1407\n",
      "acc:  0.1\n",
      "acc:  0.01\n",
      "acc:  0.03\n",
      "acc:  0.06\n",
      "acc:  0.13\n",
      "acc:  0.15\n",
      "acc:  0.15\n",
      "acc:  0.06\n",
      "acc:  0.01\n",
      "acc:  0.03\n",
      "acc:  0.05\n",
      "acc:  0.07\n",
      "acc:  0.05\n",
      "acc:  0.15\n",
      "\n",
      "14 \t [1.89835659 0.         0.        ]\n",
      "val accuracy:  0.074999996 \t f_! score:  [0.1355969 0.        0.       ]\n",
      "\n",
      "14 \t [1.89835659 0.         0.        ]\n",
      "14 \t [1.05 0.   0.  ]\n",
      "14 \t [14.  0.  0.]\n",
      "14 \t [ 105.  116. 1179.]\n",
      "---just Test  Medical ---\n",
      "0.074999996\n",
      "f1:  [0.1355969 0.        0.       ]\n"
     ]
    }
   ],
   "source": [
    "prime_loss, prime_acc = testhelper.train_input(\"Prime\", net_name)\n",
    "\n",
    "testhelper.just_test(\"Review\", net_name)\n",
    "testhelper.just_test(\"Twitter\", net_name)\n",
    "testhelper.just_test(\"Medical\", net_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_result(a, b, c, d):\n",
    "    f, axarr = plt.subplots(2, 2)\n",
    "    axarr[0, 0].plot(a)\n",
    "    axarr[0, 0].set_title('Twitter')\n",
    "    axarr[0, 1].plot(b)\n",
    "    axarr[0, 1].set_title('Prime Video')\n",
    "    axarr[1, 0].plot(c)\n",
    "    axarr[1, 0].set_title('Amazon Review')\n",
    "    axarr[1, 1].plot(d)\n",
    "    axarr[1, 1].set_title('Medikamente')\n",
    "    plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.5,\n",
    "                        wspace=0.35)\n",
    "    plt.show()\n",
    "\n",
    "print(\"Loss: \\n\")\n",
    "plot_result(tw_loss, prime_loss, review_loss, med_loss)\n",
    "print(\"Acc: \\n\")\n",
    "plot_result(tw_acc, prime_acc, review_acc, med_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
